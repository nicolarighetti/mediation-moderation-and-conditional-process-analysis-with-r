---
title: "Exploratory Data Analysis"
author: Dr. Nicola Righetti, PhD
---

# Learning Goals

By the end of this section, you will gain an understanding of the following concepts, be familiar with the R functions required to calculate and create statistical measures and plots, and be able to interpret them:

-   Sample size and data set preview

-   Frequency distributions of a variable

-   Histograms, boxplots, and bar plots

-   Measures of central tendency and measures of spread

-   Standardization and z-scores

# Sample size and data preview

Sample size and data preview are crucial first steps when working with a new data set. This involves describing the general characteristics of the data set, such as the number of cases (sample size), examining the types of variables, and calculating descriptive statistics like the mean and standard deviation. Including information on the data set and variables in a research report is standard practice.

To perform these operations, we can use the following functions in R:

-   **`dim()`** to find the number of rows in the data set, which corresponds to the number of cases (sample size) in a common and well-structured data set.

-   **`str()`** or **`glimpse()`** (in the **`tidyverse`** package) to examine the types of variables included in the data set.

-   **`head()`** to preview the data set.

-   **`View()`** to view the data set in a separate window (note: this function is not recommended for huge data sets, as it can freeze your computer).

For example, let's use the global warming study data set (**`glbwarm`**).

```{r}
# load the glbwarm data set
glbwarm <- read.csv("data/glbwarm.csv")
```

The dataset is well-organized with each row representing a case, making it easy to calculate the sample size by counting the number of rows, which in this case is 815 (n = 815). The dataset contains 7 variables and their type can be examined.

```{r}
dim(glbwarm)
```

```{r}
library(tidyverse)
glimpse(glbwarm)
```

While the `glimpse` function provides a preview of the dataset, a clearer preview is displayed using the `head` function.

```{r}
head(glbwarm)
```

# Measure of central tendency

[Descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics) are used to describe and summarize variables of a data set.

Typically, there are two general types of statistics that are used to describe data: **Measures of central tendency** and **Measures of spread**.

**Measures of central tendency** include: - **Mode**: the most frequent score in our data set - **Median**: the middle score for a set of data that has been arranged in order of magnitude - **Mean**: the sum of all the values in the data set divided by the number of values in the data set.

A clear description of the different measures can be found [here](https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php).

[Descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics) are used to summarize and describe the variables of a dataset. There are two main types of statistics used for this purpose: **measures of central tendency and measures of spread**. Measures of central tendency include the mode, median, and mean, which is the most commonly used measure and is particularly useful in linear regression modeling. The **mean** is essentially a model of the dataset and has the property of minimizing error in the prediction of any value in the dataset. In other words, the mean is the best predictor in the absence of better predictors. The **mode** is the most frequent score in our data set and the **median** is the middle score for a set of data that has been arranged in order of magnitude.

## The Mean

In R, the mean of a variable can be calculated using the function `mean`, and to access a variable in the dataset, which is a column of a data.frame, the dollar sign is used. For instance, the mean age of people in the sample can be calculated using `mean(glbwarm$age)`, which returns 49.5362.

```{r}
mean(glbwarm$age)
```

If you wish to round off the number of decimals, the `round` function can be used, which takes two arguments - the number or function to be rounded off and the integer specifying the number of decimals. For example, `round(mean(glbwarm$age), 2)` returns 49.54, which can also be represented by creating an object (e.g., mean_age) to store the output of the function `mean` and then rounding it off.

```{r}
round(mean(glbwarm$age), 2)
```

This is equivalent to creating an object *mean_age* representing the output of the function `mean` (i.e., the mean age), and then rounding it off.

```{r}
mean_age <- mean(glbwarm$age)
round(mean_age, 2)
```

## Frequency distributions

The frequency distribution of a variable provides an overview of its values and how often they occur, also known as its frequency.

Variables can have either continuous or discrete distributions, depending on their type. The distribution of a variable is typically summarized using the [five-number summary](https://en.wikipedia.org/wiki/Five-number_summary) and visually represented using histograms and box plots (for continuous or discrete variables) or bar charts (for categorical variables).

-   Discrete variables only take on specific values and are therefore characterized by discrete distributions. Examples of such variables include count data, which consist of non-negative integers, such as the number of friends on Facebook. In the "glbwarm" dataset, the variable "ideology" is measured on a discrete scale ranging from 1 to 7.

-   Discrete variables can also represent categorical data, which express the qualitative dimensions of a phenomenon. For instance, city names (Vienna, London, Verona) or ordered categorical values (low, medium, high) can be used as categorical data. Although categorical data can be represented by numbers, they should not be treated as mathematical objects, but as "names" or ordered categories. Categorical variables that can take on only two values are called dichotomous variables. In the "glbwarm" dataset, the variable "sex" is represented as a dichotomous variable.

-   Continuous variables have infinite (uncountable) possible values and are therefore characterized by continuous distributions. Examples of continuous variables include temperature or velocity. In the "glbwarm" dataset, the variable "govact" is represented as a continuous variable.

## Five-Number Summary

A numerical description of the distribution is provided by the function `summary`, which gives the **five-number summary**.

```{r}
summary(glbwarm$ideology)
```

The five-number summary includes the mean, the median, minimum and maximum values, and the first and third quartiles. [Quartiles](https://en.wikipedia.org/wiki/Quartile) are cut points that divide the range of values of the variable into four parts, or quarters, of more-or-less equal size (25%, 25%, 25%, 25%). The [median](https://en.wikipedia.org/wiki/Median) is the value separating the lower 50% from the upper 50% of the distribution.

## Boxplot

The same information as the five-number summary can be graphically displayed by a [boxplot](https://en.wikipedia.org/wiki/Box_plot),, also called a box-and-whisker plot. The "box" represents the 1st and 3rd quartiles, the central line is the median, and the "whiskers" represent the maximum and minimum values.

The points above and below the whiskers are [outliers](https://en.wikipedia.org/wiki/Outlier), which are data points that differ significantly from other observations. Since they are so different, they can be difficult to predict and can bias estimates of linear regression models.

```{r}
# use the argument "main" to add a title
boxplot(glbwarm$govact, main = "Ideology")
```

## Histogram

Another way to display the distribution of a variable is by using a [histogram](https://en.wikipedia.org/wiki/Histogram), which can be created using the `hist` function.

The function can be used to show the frequency distribution of values and their probability distributions.

```{r}
# use "main" tp change the title, and "xlab" to change the name of the x axis
hist(glbwarm$govact, 
     main = "GOVACT (GLBWARM data set)", xlab = "govact")
```

```{r}
# you can use the argument "col" to change the color
# to see the available colors, run the following command in the console: colors() 
hist(glbwarm$govact, probability = TRUE,
     main = "GOVACT (GLBWARM data set)", xlab = "govact",
     col = "slategrey")
lines(density(glbwarm$govact, adjust = 2))
```

When dealing with continuous variables, may can add a line to overlay a [density curve](https://www.statisticshowto.com/density-curve-examples/) on the histogram, to approximate the shape of the distribution.

```{r}
hist(glbwarm$govact, probability = TRUE,
     main = "GOVACT (GLBWARM data set)", xlab = "govact",
     col = "slategrey")

# this is to add a line of the "density"
# the argument "lwd" sets the width of the line
lines(density(glbwarm$govact), 
      col = "steelblue1", 
      lwd = 2)

# this is to add a more smoothed (simpler and more aproximated) density curve 
# the argument "lty" sets the type of line
lines(density(glbwarm$govact, adjust = 2), 
      col = "lightblue", 
      lwd = 3, 
      lty = "dotted")
```

## Bar plot

Histograms and box plots are used to represent the distribution of numerical variables, such as continuous and discrete variables. For categorical variables, a bar graph is used instead.

For instance, a histogram of the dichotomous categorical variable "sex" is not appropriate. It only takes on the values 0 and 1, but they are not numbers (0 = male, 1 = female) and there is nothing in between them (e.g., 0, 0.1, 0.2, ..., 1).

```{r}
hist(glbwarm$sex, main = "SEX (categorical dichotomous variable)", xlab = "sex")
```

The appropriate way to represent the frequency distribution of categorical variables is with a bar chart (or bar plot). The corresponding R function is `barplot`.

It takes as an argument a table with the frequency for each modality of the category (i.e., a table with the number of males and females in the sample). The function `table` returns the frequency of the variable.

```{r}
table(glbwarm$sex)
```

This function can be used as the main argument of the function `barplot`:

```{r}
barplot(table(glbwarm$sex),
        main = "Observations by SEX",
        xlab = "0 = Male 1 = Female")
```

# Measures of Spread

Measures of spread describe how similar or varied the set of observed values are for a particular variable. Measures of spread include the range, quartiles, and the interquartile range, as well as the variance and standard deviation.

For example, the interquartile range is the difference between the third and first quartiles and is represented by the size of the "box" in a boxplot.

```{r}
boxplot(glbwarm$age)
```

Even more relevant are the variance and standard deviation.

## Variance

**Variance** (represented as $s^2$ when computed on a sample, as in this case) is a measure of dispersion that quantifies how spread the values of a variable are from the mean (the average value of the variable). In other words, variance measures how much the values of a variable vary.

The formula for variance is:

$$s^2 =  \frac{\sum_{i=1}^{n} (x_i-\bar x)^2}{n-1}$$

Capital sigma $\sum$ is the summation symbol, $(x_i-\bar x)$ is the deviation (i.e., the difference) of each value $i$ of $X$ from the *mean* of $X$ ($\bar x$, the mean or average of the variables), and $n$ is the sample size. In R:

```{r}
var(glbwarm$age)
```

## Standard Deviation

The **standard deviation** (represented as $s$ when computed on a sample) is the square root of the variance and has the same units as the variable itself. It is a related, and more commonly used, measure of dispersion. A low standard deviation indicates that the values of a variable tend to be close to its average, while a high standard deviation indicates that the values are spread out over a wider range.

$$s = \sqrt \frac{\sum (x_i-\bar x)^2}{n-1}$$

In R:

```{r}
sd(glbwarm$age)
```

# Standardization and z-scores

Besides measuring the spread of a variable, standard deviation is used for standardizing variables.

Standardization is a procedure that transforms the original data ("raw data") in the so-called **z-scores**. Z-scores are are values expressed in standardized units by using the average and the standard deviation:

$$Z = \frac{x_i - \bar x}{s}$$

A way to think about standardization, is considering it as the interpretation of values relatively to their own distribution.

The average of a standardized variable is always zero, below-average values are negative and above-average values are positive.

In R, standardization is provided by the function `scale`.

```{r}
zscores_age <- scale(glbwarm$age)
```

```{r}
hist(zscores_age)
```

A classic example of the utility of z-scores typically goes like this. Suppose two sections of a statistics course are being taught. John is a student in section A and Mary is a student in section B. On the final exam for the course, John receives a raw score of 80 out of 100 (i.e., 80%). Mary, on the other hand, earns a score of 70 out of 100 (i.e., 70%).

At first glance, it may appear that John was more successful on his final exam. However, scores, considered absolutely, do not allow us a comparison of each student's score relative to their class distributions. For instance, if the mean in John's class was equal to 85% with a standard deviation of 2, John's z-score is:

$$Z = \frac{x_i - \bar x}{\sigma} =  \frac{80 - 85}{2} = -2.5$$

Suppose that in Mary's class, the mean was equal to 65% also with a standard deviation of 2. Mary's zscore is thus:

$$Z = \frac{x_i - \bar x}{\sigma} =  \frac{70 - 65}{2} = 2.5$$

As we can see, relative to their particular distributions, Mary greatly outperformed John.

Z-scores are often used with reference to the **standard normal distribution**. In this case, 68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean, respectively. It is considered statistically unlikely that a value is 2 standard deviations either side of the mean. We'll encounter again the normal distribution and the 2 standard deviation threshold when we'll take statistical inference into consideration.

# Correlation and prediction

In the first section we used statistical measures and charts to describe variables. In this section we'll learn to assess the relationships between two variables.

At the end of this unit you will understand the Pearson's correlation coefficient and know the R functions to calculate and create the related statistical plots (scatter plot).

## Correlation

Correlation is a **statistical relationship**, whether causal or not, between two variables.

Two variables $X$ and $Y$ are said to be correlated when they show a systematic association, such that knowing the values of one variable enables the **prediction** of the values of the other.

Correlation analysis permits the identification of this relationship and the quantification of its strength.

## Pearson's correlation coefficient (Pearson's R)

The Pearson product-moment correlation, also known as Pearson's r, is an important measure of correlation. It is also the foundation of most of the linear regression methods discussed in this course. Indeed, it can be used to quantify the linear association (or relationship) between two quantitative variables, a quantitative and a dichotomous variable, as well as between two dichotomous variables.

In a linear association, the direction and rate of change in one variable are constant with respect to changes in the other variable. For example, in a perfect linear association between two variables $X$ and \$Y\$, an increase of 1 point in $X$ corresponds to an increase of 1 point in $Y$. A linear relationship describes a straight-line relationship between two variables.

A linear relationship describes a straight-line relationship between two variables.

```{r echo=FALSE}
knitr::include_graphics("img/correlation_examples.png")
```

### Pearson's R and covariance

Pearson's r is based on the formalization of the idea of calculating the *ratio* between the degree to which $X$ and $Y$ vary *together*, and the degree to which $X$ and $Y$ vary *separately*, in order to find how much "co-variation" they share.

$$r = \frac{degree-to-which-X-and-Y-vary-together}{degree-to-which-X-and-Y-vary-separately}$$

The idea of co-variation is mathematically expressed by **covariance** ($cov$), which is at the heart of virtually all statistical methods. The Pearson's $r$ is a **standardized measure of covariance**.

Covariance is based on **variance** ($s^2$):

$$s^2 =  \frac{\sum_{i=1}^{n} (x_i-\bar x)^2}{n-1}$$

More precisely, covariance is a *measure of the joint variability of two variables*, and is calculated as the product of the **variance** of the variables:

$$ cov = \frac{\sum_{i=1}^{n} (x_i-\bar x) (y_i-\bar y)}{n-1}$$

The covariance can be positive, negative, or equal to zero, and gives a measure of the linear correlation between variables.

So, why do we need Pearson's *r*? Because covariance is a measure that depends on the unit of measurement of the variables, while it is preferable to have a **standard** measure, meaningful independently of each specific data set.

Standardization of covariance is obtained by dividing the covariance $cov$ by the **product of the standard deviations** of the variables $X$ and $Y$. This product represents the **total amount of variation possible**.

Thanks to the denominator, which represents the total amount of variation possible for the specific couple of variables, $r$ **is always constrained between 1 and -1** for *any* variable expressed in any unit of measure. The extent to which the covariation $cov_{XY}$ accounts for all of the possible variation ($SD_XSD_Y$) is the extent to which $r$ approximates $|1|$ (negative -1, or positive +1).

$$ r = \frac{cov_{XY}}{SD_XSD_Y}$$

In R, the correlation coefficient is computed with the function `cor`. For example:

```{r}
cor(glbwarm$govact, glbwarm$negemot)
```

### Pearson's R interpretation

Pearson's correlation coefficient can be interpreted as follows:

-   The **sign (+/-)** corresponds to the *direction* of the association between X and Y:

    -   **positive linear association** (+): the higher X, the higher Y;
    -   **negative linear association** (-): the higher X, the lower Y.

-   $r$ is close to **zero** when the association is null or nonlinear

-   The closer $r$ is to $|1|$ (+1 or -1), the **stronger** the linear association.

There are different rule of thumbs for the interpretation of the Pearson's correlation. For example, Mike Allen (2017), *The SAGE Encyclopedia of Communication Research Methods* (p. 269) suggests the following:

-   0.00-0.19 Negligible correlation
-   0.20-0.39 Weak correlation
-   0.40-0.59 Fair correlation
-   0.60-0.79 Moderate correlation
-   0.80-1.00 Strong correlation

### Scatter plots

A scatter plot is a common method for visually exploring and representing the correlation between two variables.

"[A scatter plot](https://en.wikipedia.org/wiki/Scatter_plot) is a type of plot or mathematical diagram that uses Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), an additional variable can be displayed. The data are displayed as a collection of points, with the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis".

To create a scatter plot in R, you can use the function `plot`. It takes two arguments: x (the first variable) and y (the second variable). The argument `pch` changes the shape of the points; see more details [here](https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/points.html).

```{r echo=TRUE}
plot(glbwarm$govact, glbwarm$posemot,
     pch = 20,
     xlab = "govact", 
     ylab = "posemot")
```

The scatter plot shows the direction of the association.

```{r}
X <- rnorm(100) # simulating data
Yp <- X + rnorm(100, sd=0.5)
Yn <- -Yp
```

```{r fig.height=4, fig.width=9, paged.print=FALSE}
par(mfrow=c(1,2))
plot(X, Yp, main = "Positive Association")
plot(X, Yn, main = "Negative Association")
```

## Predictions

When there is a correlation between $X$ and $Y$, using $X_j$ to estimate $Y_j$ produces estimates that are more accurate than if one were to merely estimate $Y_j = \bar Y$ (the average value of $Y$) for every case in the data.

> Given a variable $X$ and $Y$ of a data set of size n, the subscripts "j" in $X_j$ and $Y_j$, indicates the j case. It can be any one of the n cases, but the same for both the $X$ and $Y$ variables. In other words, it indicates the measures of the case j on the variables $X$ and $Y$.

Pearson's *r* provides an estimate as to how many *standard deviations (Z) from the sample mean* on $Y$ a case is, given how many standard deviations from the sample mean the case is on $X$. More formally:

$$\hat Z_{Y_j} = r_{XY}Z_{X_j}$$

where $\hat Z_{Y_j}$ is the estimated value of $Z_{Y_j}$.

-   For instance, a person who is one-half of a standard deviation above the mean ($Z_X = 0.5$) in negative emotions, is estimated to be $\hat{Z} = 0.578(0.5) = 0.289$ standard deviations from the mean in his or her support for government action.
-   The sign of $\hat{Z_Y}$ is positive, meaning that this person is estimated to be above the sample mean (i.e., more supportive than average).
-   Similarly, someone who is two standard deviations below the mean ($Z_X = −2$) in negative emotions is estimated to be $Z_Y = 0.578(−2) = −1.156$ standard deviations from the mean in support for government action.
-   In this case, $\hat{Z_Y}$ is negative, meaning that such a person is estimated to be below the sample mean in support for government action (i.e., less supportive than average).

**Estimates** of Y from X are **expectations** extracted from what is known about the association between X and Y.

In statistics, expectations are never perfectly met, but we have a numerical means of gauging how close are to reality.

In our case, that gauge is the size of Pearson's *r*. The closer it is to +/- 1, the more consistent those expectations are with the reality of the data.

```{r echo=FALSE}
  par(mfrow=c(2,2))

for (i in c(0.2, 1, 3, 7)) {
    X <- rnorm(100)
    Y <- X + rnorm(100, sd=i)
    r <- round(cor.test(X, Y)$estimate, 2)
    plot(X, Y, main = paste("r =", r))
}
```

# Readings and exercises

-   Review the content of the class
-   Read chapter 2, pp. 29-34

## Notes on the GLBWARM data set and exercises

GLBWARM is a data set collected from 815 residents of the United States (417 female, 398 male) who expressed a willingness to participate in online surveys in exchange for various incentives. The sampling procedure was designed such that the respondents roughly represent the U.S. population.

The data set contains a variable constructed from how each participant responded to five questions about the extent to which he or she supports various policies or actions by the U.S. government to mitigate the threat of global climate change.

Examples include:

-   "How much do you support or oppose increasing government investment for developing alternative energy like biofuels, wind, or solar by 25%?"
-   "How much do you support or oppose creating a 'Cap and Trade' policy that limits greenhouse gases said to cause global warming?"

Response options were scaled from "Strongly opposed" (coded 1) or "Strongly support" (7), with intermediate labels to denote intermediate levels of support. An index of support for government action to reduce climate change was constructed for each person by averaging responses to the five questions (GOVACT in the data file).

The data set also contains a variable quantifying participants' negative emotional responses to the prospect of climate change. This variable was constructed using participants' responses to a question that asked them to indicate how frequently they feel each of three emotions when thinking about global warming: "worried," "alarmed," and "concerned." Response options included "not at all," "slightly," "a little bit," "some," "a fair amount," and "a great deal." These responses were numerically coded 1 to 6, respectively, and each participant's responses were averaged across all three emotions to produce a measure of negative emotions about climate change (NEGEMOT in the data file). This variable is scaled such that higher scores reflect feeling stronger negative emotions.

### Exercise 1

Display the correlation between the variables *negemot* and *govact* using a scatter plot.

Based on the plot, can you tell if there is a correlation between the two variables?

In your opinion, is there a strong correlation between these variables?

### Exercise 2

Use the function `cor` to calculate the correlation between the variables *negemot* and *govact* in the `glbwarm` data set.

Based on the Pearson's r coefficient, is there a strong correlation between these variables?

### Exercise 3

Consider the variables *tnews* and *age* in the `glbwarm` data set:

-   Visualize the association using a scatterplot
-   Calculate the Pearson's r correlation
-   Write a comment to interpret the results
