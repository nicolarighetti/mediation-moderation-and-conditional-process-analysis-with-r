[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mediation, Moderation, and Conditional Process Analysis with R",
    "section": "",
    "text": "This data analysis seminar focuses on linear regression analysis to explore questions about mediated and moderated effects.\nComputer applications will focus on R statistical language, the Rstudio environment (https://www.rstudio.com),,/) and the PROCESS software by Andrew F. Hayes (http://processmacro.org)../)\nThe course is subdivided into the following learning units:\n\nThe introductory part of the course is dedicated to a brief review of the basic principles of linear regression and the setup of R and PROCESS for statistical analysis. We discuss multiple regression analysis and its principles to understand how to fit, visualize, interpret, and evaluate multiple regression models in R. We also get a first overview of mediation, moderation, and conditional process models.\nWe then focus on mediation analysis, seeing how to fit, visualize, interpret, and evaluate mediation models using PROCESS in the R environment.\nThe following learning unit is dedicated to moderation analysis and explaining how to fit, visualize, interpret, and evaluate moderation models using PROCESS in the R environment.\nThe final learning unit is an overview of conditional process analysis and aims at explaining how to fit, visualize, interpret, and evaluate conditional process models using PROCESS in the R environment.\n\nThe last part of the course is dedicated to the project work.\nBy the end of this course, participants are expected to know how:\n\nRun and interpret the results of linear regression, moderation, mediation, and conditional process models.\nTest competing theories of mechanisms statistically through the comparison of indirect effects in models with multiple mediators.\nVisualize and probe interactions in regression models to interpret interaction effects appropriately.\nEstimate the contingencies of mechanisms through the computation and inference about conditional indirect effects.\nUse the R language and PROCESS to run, visualize, and understand linear regression, moderation, mediation, and conditional process models.\n\n\n\n\nThe official handbook of the course is:\nAndrew F. Hayes. Introduction to Mediation, Moderation, and Conditional Process Analysis. A Regression-Based Approach. 2018. SECOND EDITION. THE GUILFORD PRESS, New York, London."
  },
  {
    "objectID": "mediation.html#learning-outcomes",
    "href": "mediation.html#learning-outcomes",
    "title": "Mediation",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nAt the end of UNIT 3A you should know:\n\nWhat is the Simple Mediation Model\nHow to estimate a Simple Mediation Model using the PROGRESS software (in R)\nHow to interpret the output of the estimation of the Simple Mediation Model using the PROGRESS software\n\nIn this part of the course, also due to time costrains, we are going to follow a practical/applied approach. We’ll start from concrete research problems and questions, and we’ll learn to use mediation models to answer them. However, we’ll also touch upon the theoretical concepts needed to an appropriate interpretation of the models."
  },
  {
    "objectID": "mediation.html#download-and-load-the-process-software",
    "href": "mediation.html#download-and-load-the-process-software",
    "title": "Mediation",
    "section": "Download and load the PROCESS software",
    "text": "Download and load the PROCESS software\n\nDownload from Moodle (folder “Scripts”) the PROCESS software for R. It is a software written by the author of the book we are using during this course, Andrew F. Hayes. The software is also available on https://www.processmacro.org/download.html, where you can find the version for R and other statistical software. In this course we’ll use the R version (several examples in the book are from the SPSS version).\nCreate a folder “PROCESS_R” in your R project directory, and save the “process.R” there\nRun the code below to load the PROCESS functions\n\n\nsource(\"PROCESS_R/process.R\")\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n \nPROCESS is now ready for use.\nCopyright 2021 by Andrew F. Hayes ALL RIGHTS RESERVED"
  },
  {
    "objectID": "mediation.html#download-and-load-the-data-set",
    "href": "mediation.html#download-and-load-the-data-set",
    "title": "Mediation",
    "section": "Download and load the data set",
    "text": "Download and load the data set\n\nDownload the pmi dataset from Moodle, and save it in your “data” folder, in the R project directory.\nLoad the data by using the read.csv function.\n\n\npmi_data <- read.csv(\"data/pmi.csv\")"
  },
  {
    "objectID": "mediation.html#case-study-description",
    "href": "mediation.html#case-study-description",
    "title": "Mediation",
    "section": "Case study description",
    "text": "Case study description\nParticipants read a news story describing global economic conditions leading to a possible sugar shortage and increase in price.\nResearchers told them that the story is about to be published:\n\non the front page of a newspaper (= front page condition (cond = 0))\nor in the inside of an economic supplement (= back page condition (cond = 1)).\n\nThis is the independent variable (\\(X\\)), and is a dichotomous variable (in this case it expresses a experimentally manipulated condition, but a variable can be used in a mediation model even when in nonexperimenal studies).\nParticipants then answered a series of questions:\n\nabout their intention to buy sugar (DV = reported reaction (reaction))\nand aimed to assess (1) their perception of media’s influence on general public’s intention to buy sugar (= presumed media influence (pmi)), and (2) how important the topic was to the global economic crisis (=perceived importance of the topic (import)).\n\nResearch question: does the location of the article affect behavioral intentions indirectly through presumed media influence?\nThe question is about a mechanism at work that leads to different behavioral intentions after being told about the different location of the article. The appropriate statistical model is a mediated model. This is the kind of model we use to answer questions about mechanisms."
  },
  {
    "objectID": "mediation.html#estimation-with-process",
    "href": "mediation.html#estimation-with-process",
    "title": "Mediation",
    "section": "Estimation with PROCESS",
    "text": "Estimation with PROCESS\nUsing OLS regression, we use the process function to estimate equations 1 and 2 (in the previous slide) and thereby to get a, b, and c′ along with standard regression statistics such as R2 for each of the equations. It also creates a section of output containing the direct and indirect effects of \\(X\\).\nThe function requires you to specificy:\n\nthe dataset\nthe dependent variable \\(y\\)\nthe independent variable \\(x\\)\nand the mediator variable \\(m\\)\n\nMoreover, we specify:\n\nthe total parameter, which we set to 1 (total = 1). It is used to tell the software to generate the total effect of X on Y (which we called c in the previous slides).\nmodel=4, a parameter setting used for mediation models\nWe also use a seed with an abribrary number. This is used to ensure replicability of the results, since the model estimation involves a procedure using random number generation (the seed ensures that the random generated sequences are the same if you repeat the estimation).\n\n\n\n\n\n\n\nprocess(pmi_data, y = \"reaction\", x = \"cond\", m = \"pmi\", \n        total = 1, normal = 1, model = 4, progress=0,\n        seed=31216)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                \nModel : 4       \n    Y : reaction\n    X : cond    \n    M : pmi     \n\nSample size: 123\n\nCustom seed: 31216\n\n\n*********************************************************************** \nOutcome Variable: pmi\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.1808    0.0327    1.7026    4.0878    1.0000  121.0000    0.0454\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    5.3769    0.1618   33.2222    0.0000    5.0565    5.6973\ncond        0.4765    0.2357    2.0218    0.0454    0.0099    0.9431\n\n*********************************************************************** \nOutcome Variable: reaction\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.4538    0.2059    1.9404   15.5571    2.0000  120.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    0.5269    0.5497    0.9585    0.3397   -0.5615    1.6152\ncond        0.2544    0.2558    0.9943    0.3221   -0.2522    0.7609\npmi         0.5064    0.0970    5.2185    0.0000    0.3143    0.6986\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: reaction\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.1603    0.0257    2.3610    3.1897    1.0000  121.0000    0.0766\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    3.2500    0.1906   17.0525    0.0000    2.8727    3.6273\ncond        0.4957    0.2775    1.7860    0.0766   -0.0538    1.0452\n\n*********************************************************************** \nBootstrapping in progress. Please wait.\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nTotal effect of X on Y:\n     effect        se         t         p      LLCI      ULCI\n     0.4957    0.2775    1.7860    0.0766   -0.0538    1.0452\n\nDirect effect of X on Y:\n     effect        se         t         p      LLCI      ULCI\n     0.2544    0.2558    0.9943    0.3221   -0.2522    0.7609\n\nIndirect effect(s) of X on Y:\n       Effect    BootSE  BootLLCI  BootULCI\npmi    0.2413    0.1322    0.0143    0.5254\n\nNormal theory test for indirect effect(s):\n       Effect        se         Z         p\npmi    0.2413    0.1300    1.8559    0.0635\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000"
  },
  {
    "objectID": "mediation.html#interpretation-of-coefficients",
    "href": "mediation.html#interpretation-of-coefficients",
    "title": "Mediation",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\nTwo people who differ by one unit in X (front page condition vs. back-page condition) are estimated to be differ by 0.4957 units (95% CI [0.0538, 1.0452]) on average in their intention to buy sugar (total effect).\nThey differ by 0.2413 units (95% CI [0.0143, 0.5254]) on average as a results of the indirect effect of page manipulation on buying intention through presumed media influence. The rest of the difference, 0.2544 units (95% CI [-0.2522, 0.7609]), is the direct effect of page manipulation on reported reactions, which is independent of the effects of page manipulation on reaction through PMI."
  },
  {
    "objectID": "mediation.html#inference",
    "href": "mediation.html#inference",
    "title": "Mediation",
    "section": "Inference",
    "text": "Inference\nWhen coming to statistical inference, and thus to the “statistical significance” of the coefficients, the output reports p-values and confidence intervals.\nInferential methods have different characteristics for the direct effect and the indirect effect: Inference for the direct effect (and the total effect) is simple and non-controversial, while the case of indirect effect is more complex.\nRegarding the direct effect, the inference can be framed in terms of a p-value (through hypothesis testing) or confidence interval.\nIn the example above, the direct effect of the experiental condition, independent from the mediator, amount to 0.2544 units (95% CI [-0.2522, 0.7609], p-value: 0.3221).\nThe p-value and the confidence interval show that the effect is not statistically significant: the p-value is above the standard threshold of 0.05, while the confidence interval includes zero (when it does so, it means that it is possible that it is equal zero).\nRegarding the inderect effect, there are two possibilities:\n\ncalculating the p-value through an hypothesis test, relying the assumption of normality of the sampling distribution\nUsing a particular tecniques called bootstrapping\n\nIn the bottom part of the output of process, are both the “indirect effect(s) of X on Y”, in the confidence interval form, and the “normal theory test for indirect effect(s)”, in the p-value form.\nBased on stastical research, the recommendation of the PROCESS software is, in general, to avoid the “normal theory test for indirect effect(s)” approach, and to interpret the confidence interval. This approach would be better in several empirical circumtances, and more powerful in detecting stastical significance.\nWe can avoid calculating the normal theory test by removing normal = 1 from the function progress.\nFor instance, in the example, the p-value of the indirect effect is not statistically significant (p-value = 0.0635, > 0.05), while the bootstrapping confindence interval is statistically significant (it does not include zero: 95% CI [0.0143, 0.5254]).\nWe already saw that confidence intervals can be calculated through standard errors based on the Central Limit Theorem, and thus based on theoretical assumptions regarding the sampling distribution.\nIn this case, instead, they are calculated through bootstrapping. It is a procedure based on resampling with replacement. Also in this case, it is of fundamental importance that the sample is a miniature representation of the original population.\nResampling with replacement is a resample method that create new samples by choosing, in a random way, the cases that compose an initial sample. The particularity is that the same case can be choosen more than once: “Suppose case 1 in the original sample is”Joe.” Joe happened to be contacted for participation in the study and provided data to the effort. In the resampling process, Joe functions as a stand-in for himself and anyone else like him in the pool of potential research participants, as defined by Joe’s measurements on the variables in the model. The original sampling could have sampled several Joes or none, depending in part on the luck of the draw”.\nIn bootstrapping, a large number of samples are drawn from the sample (e.g. 5,000 is the default in PROCESS. Also 10,000 is commonly used), using the just descripted random sampling with replacement.\nNext, the indirect effect (the statistic of interest in this case) is calculated based on each sample.\nFinally, the confidence intervals are found by dividing the distribution in 100 parts (percentiles), and taking the value corresponding to the 2.5 percentile (lower boundary of the 95% confidence interval), and the 97.5 percentile (upper boundary of the 95% confidence interval).\nA difference between confidence intervals calculated using standard errors (relying on the “theoretical” sampling distribution), and those calculated through bootstrapping, is that the former is always symmetrical (the upper and lower bound of the confidence interval are equidistant from the point estimate), while the latter can be asymmetrical, depending on the shape of the distribution.\nBootstrapping is particularly useful relative to the normal theory approach in smaller samples, because it is in smaller samples that the non-normality of the sampling distribution of ab is likely to be most severe, the large sample asymptotics of the normal theory approach are harder to trust, and the power advantages of bootstrapping are more pronounced."
  },
  {
    "objectID": "mediation.html#example-with-continuous-x",
    "href": "mediation.html#example-with-continuous-x",
    "title": "Mediation",
    "section": "Example with continuous X",
    "text": "Example with continuous X\nIn the previous example the independent variable (\\(X\\)), was a dichotomous variable. In this case it expressed a experimentally manipulated condition, but a variable can be used in a mediation model even when in nonexperimenal studies.\nIn other case, the independent variable can be continuous. No modifications are necessary to the mathematics or procedures described to estimate these effects, and the interpretation of these effects otherwise remains unchanged.\nDownload the estress data from Moodle, save the file into your “data” folder and load it.\n\nestress <- read.csv(\"data/estress.csv\")\n\nThe dataset includes information on 262 entrepreneurs who responded to an online survey about recent performance of their business as well as their emotional and cognitive reactions to the economic climate.\nResearchers proposed that economic stress (\\(X\\), variable estress) leads to a desire to disen gage from entrepreneurial activities (\\(Y\\), variable withdraw) as a result of the depressed affect (\\(M\\), variable affect) such stress produces, which in turns leads to a desire to disengage from entrepreneurship.\nMore specifically, the experience of stress results in feelings of despondency and hopelessness, and the more such feelings of depressed affect result, the greater the desire to withdraw from one’s role as a small-business owner to pursue other vocational activities.\nSo depressed affect was hypothesized as a mediator of the effect of economic stress on withdrawal intentions.\n\n\n\n\n\n\nprocess(estress, y = \"withdraw\", x = \"estress\", m = \"affect\", \n        total = 1, normal = 1, model = 4, progress=0,\n        seed=100770)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                \nModel : 4       \n    Y : withdraw\n    X : estress \n    M : affect  \n\nSample size: 262\n\nCustom seed: 100770\n\n\n*********************************************************************** \nOutcome Variable: affect\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.3401    0.1156    0.4650   33.9988    1.0000  260.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    0.7994    0.1433    5.5777    0.0000    0.5172    1.0816\nestress     0.1729    0.0296    5.8308    0.0000    0.1145    0.2313\n\n*********************************************************************** \nOutcome Variable: withdraw\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.4247    0.1804    1.2841   28.4946    2.0000  259.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    1.4471    0.2520    5.7420    0.0000    0.9508    1.9433\nestress    -0.0768    0.0524   -1.4667    0.1437   -0.1800    0.0263\naffect      0.7691    0.1031    7.4627    0.0000    0.5662    0.9721\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: withdraw\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.0641    0.0041    1.5543    1.0718    1.0000  260.0000    0.3015\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    2.0619    0.2620    7.8691    0.0000    1.5459    2.5778\nestress     0.0561    0.0542    1.0353    0.3015   -0.0506    0.1629\n\n*********************************************************************** \nBootstrapping in progress. Please wait.\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nTotal effect of X on Y:\n     effect        se         t         p      LLCI      ULCI\n     0.0561    0.0542    1.0353    0.3015   -0.0506    0.1629\n\nDirect effect of X on Y:\n     effect        se         t         p      LLCI      ULCI\n    -0.0768    0.0524   -1.4667    0.1437   -0.1800    0.0263\n\nIndirect effect(s) of X on Y:\n          Effect    BootSE  BootLLCI  BootULCI\naffect    0.1330    0.0333    0.0713    0.2033\n\nNormal theory test for indirect effect(s):\n          Effect        se         Z         p\naffect    0.1330    0.0291    4.5693    0.0000\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000\n\n\nReporting the estimated coefficients:\n\n\n\n\n\nTwo entrepreneurs who differ by one unit in their economic stress are estimated to differ by 0.133 (95% CI [0.0713, 0.2033]) units in their reported intentions to withdraw from their business as a result of the tendency for those under relatively more economic stress to feel more depressed affect (because a is positive), which in turn translates into greater withdrawal intentions (because b is positive).\nThis indirect effect is statistically different from zero, as revealed by a 95% bootstrap confidence interval that is entirely above zero (95% CI [0.0713, 0.2033])"
  },
  {
    "objectID": "mediation.html#learning-outcomes-1",
    "href": "mediation.html#learning-outcomes-1",
    "title": "Mediation",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nLearn to rule out alternative explanation in mediation models with reference to\n\nconfunding variables and\nalternative casual explanations\n\nLearn the meaning of “effect size” and learn how to measure it\nLearn how to work with multiple \\(X\\) and \\(Y\\) variables, and the potential pitfalls thereof"
  },
  {
    "objectID": "mediation.html#confounding-variables",
    "href": "mediation.html#confounding-variables",
    "title": "Mediation",
    "section": "Confounding variables",
    "text": "Confounding variables\nAn apparent causal association between \\(X\\) and \\(M\\) (\\(X -> M\\)) may be actually due to some other variable that \\(X\\) is actually affecting, so there is the risk one will go away with the mistaken conclusion that \\(X\\) affects \\(Y\\) indirectly through \\(M\\) when in fact the other variable is the mechanism variable through which X exerts its effect indirectly. We can call this phenomenon as epiphenomenal association.\nThat is, the association between \\(M\\) and \\(Y\\) may be an epiphenomenon of the fact that \\(X\\) affects some other variable not in the model, and that other variable affects \\(Y\\), but because \\(M\\) is correlated with that other variable, it appears that \\(M\\) is the variable through with \\(X\\)’s effect on \\(Y\\) is carried.\n\n\n\n\n\nOther times the association between variables is simply spurious.\nFor example, the fact that children who watch relatively more television are more likely to be overweight does not imply with certainty that excessive television viewing causes weight problems. Perhaps parents who don’t encourage a healthy lifestyle are more likely to purchase and feed their children less healthy food that is high in fat and calories and are also less likely to encourage their children to play sports, exercise, or engage in other behaviors that are better for their bodies than just watching television. So it isn’t necessarily the excessive television viewing causing the weight problems.\nWhen X is not experimentally manipulated, then things get even worse. Absent random assignment to values of \\(X\\), all of the associations in a mediation model are susceptible to confounding and epiphenomenal association, not just the association between \\(M\\) and \\(Y\\)."
  },
  {
    "objectID": "mediation.html#statistical-controls",
    "href": "mediation.html#statistical-controls",
    "title": "Mediation",
    "section": "Statistical controls",
    "text": "Statistical controls\nEpiphenomenal association and confounding as threats to the validity of a causal claim can be managed at least in part through statistical control.\nIf two variables \\(M\\) (e.g. how much children watch television) and \\(Y\\) (e.g. being outweighted) are epiphemenonally associated or confounded due to their association with some variable \\(C\\) (e.g. how much parents encourage a healthy lifestyle in their children), then the association between \\(M\\) and \\(Y\\) should not exist among people who are equal on \\(C\\) (e.g. have parents equally encouraging a healthy lifestyle in their children), no matter what they are different on \\(M\\) (e.g. how much they watch television).\nIn regression, if we add a variable \\(C\\), we can get the partial coefficient of \\(M\\), we interpret as the weight of the \\(M\\) variable, holding all the other variables in the model constant (including \\(C\\)):\n\nConfounding and epiphenomenal association due to C can be ruled out by including C as a predictor\nAdding \\(C\\) to the models of \\(M\\) and \\(Y\\) will also remove \\(C\\) as an epiphenomenal or confounding threat to a causal claim about the association between \\(X\\) and \\(M\\) and \\(X\\) and \\(Y\\) as well as between \\(M\\) and \\(Y\\).\n\nMore than one confounding variable can be included in the model.\nWe can make an example using the previous estress dataset and research problem: In this research project nothing was manipulated, and potential confounds abound.\nFor example, the indirect effect may be a manifestation of nothing other than individual differences such as perceptions of one’s own confidence and skill in managing a business: People who feel relatively more confident in their abilities may tend to feel relatively less stress in general, perhaps are less prone to feeling negative and down about their business under any circumstances.\nIf so, then statistically controlling for such an individual difference when assessing the indirect effect of economic stress should weaken or eliminate it.\nThat is, among people equal in their confidence, there should be no evidence of an indirect effect of economic stress on withdrawal intentions through depressed affect, because this variable has been taken out of the process that, by this reasoning, induces spurious association between \\(X\\) and \\(M\\) and between \\(M\\) and \\(Y\\). But if the indirect effect persists even when holding confidence constant, a causal claim remains viable.\nIn the dataset is included a measure of entrepreneurial self-efficacy (“ese”)\nThose relatively high in entrepreneurial self-efficacy did report feeling relatively less economic stress (\\(r = −0:158; p = 0.010\\)), relatively less business-related depressed affect (\\(r = −0.246; p < 0.001\\)), and reported relatively weaker intentions to withdraw from entrepreneurship (\\(r = −0:243; p < 0.001\\)).\nSo spurious or epiphenomenal association are plausible alternative explanations for at least some of the relationship observed between economic stress, depressed affect, and withdrawal intentions.\nTo illustrate that more than a single variable can be used as a statistical control, we also include other statistical controls, namely sex of the participant (“sex” in the data, 0 = female, 1 = male) and length of time in the business, in years (“tenure” in the data) as predictors.\n\n\n\n\n\nThe only difference between the code to fit a mediation model with covariates and the code we already saw, is the addition of the covariates listed in a vector (using the c function) after the parameter cov.\nBy default, any variable in the covariate list will be included as additional antecedent variables in the model of each of the consequents.\n\n\n\n\n\n\nprocess(estress, y = \"withdraw\", x = \"estress\", m = \"affect\", \n        cov = c(\"ese\", \"sex\", \"tenure\"),\n        total = 1, model = 4, progress=0,\n        seed=100770)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                \nModel : 4       \n    Y : withdraw\n    X : estress \n    M : affect  \n\nCovariates: \n       ese sex tenure\n\nSample size: 262\n\nCustom seed: 100770\n\n\n*********************************************************************** \nOutcome Variable: affect\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.4039    0.1631    0.4452   12.5231    4.0000  257.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    1.7855    0.3077    5.8033    0.0000    1.1796    2.3914\nestress     0.1593    0.0297    5.3612    0.0000    0.1008    0.2179\nese        -0.1549    0.0444   -3.4892    0.0006   -0.2423   -0.0675\nsex         0.0148    0.0857    0.1726    0.8631   -0.1540    0.1836\ntenure     -0.0108    0.0063   -1.7227    0.0861   -0.0232    0.0016\n\n*********************************************************************** \nOutcome Variable: withdraw\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.4539    0.2060    1.2586   13.2824    5.0000  256.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    2.7461    0.5502    4.9913    0.0000    1.6626    3.8295\nestress    -0.0935    0.0527   -1.7751    0.0771   -0.1973    0.0102\naffect      0.7071    0.1049    6.7420    0.0000    0.5006    0.9137\nese        -0.2121    0.0764   -2.7769    0.0059   -0.3625   -0.0617\nsex         0.1274    0.1441    0.8838    0.3776   -0.1565    0.4112\ntenure     -0.0021    0.0106   -0.1940    0.8463   -0.0230    0.0189\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: withdraw\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.2550    0.0650    1.4763    4.4667    4.0000  257.0000    0.0017\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    4.0087    0.5603    7.1548    0.0000    2.9053    5.1120\nestress     0.0191    0.0541    0.3535    0.7240   -0.0874    0.1257\nese        -0.3216    0.0808   -3.9789    0.0001   -0.4808   -0.1624\nsex         0.1379    0.1561    0.8831    0.3780   -0.1695    0.4453\ntenure     -0.0097    0.0115   -0.8491    0.3966   -0.0323    0.0128\n\n*********************************************************************** \nBootstrapping in progress. Please wait.\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nTotal effect of X on Y:\n     effect        se         t         p      LLCI      ULCI\n     0.0191    0.0541    0.3535    0.7240   -0.0874    0.1257\n\nDirect effect of X on Y:\n     effect        se         t         p      LLCI      ULCI\n    -0.0935    0.0527   -1.7751    0.0771   -0.1973    0.0102\n\nIndirect effect(s) of X on Y:\n          Effect    BootSE  BootLLCI  BootULCI\naffect    0.1127    0.0292    0.0594    0.1734\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000\n\n\n\n\n\n\n\nComparing the output for the model controlling for sex, tenure, and entrepreneurial self-efficacy to the output excluding these controls, it can be seen that substantively, nothing has really changed.\nHowever, this kind of analysis may be done in order to see how sensitive or susceptible the results from a comparable analysis without such controls is to alternative explanations involving those variables being controlled, or it may be done because it is known a priori or based on preliminary analyses that certain variables may be producing spurious association between key variables in the causal system.\nRuling out epiphenomenality or spurious association as alternative explanations is an important part of any causal argument that includes associations that are only correlational in nature."
  },
  {
    "objectID": "mediation.html#approach",
    "href": "mediation.html#approach",
    "title": "Mediation",
    "section": "Approach",
    "text": "Approach\nIn an attempt to entertain alternative direction of causal flow, one procedure some investigators employ is to estimate a mediation model corresponding to the alternative explanation to see whether the direct and indirect effects are consistent with what that alternative order predicts.\nFor instance, when economic stress was specified as the mediator of the effect of withdrawal intentions on depressed affect, there was no evidence of such a process at work, as a bootstrap confidence interval for the indirect effect contained zero.\nSimilarly, when this procedure was applied to the presumed media influence study by treating presumed media influence as the final outcome and intentions to buy sugar as the mediator, the results were not consistent with this alternative direction of causal flow."
  },
  {
    "objectID": "mediation.html#partially-standardized-effect",
    "href": "mediation.html#partially-standardized-effect",
    "title": "Mediation",
    "section": "Partially standardized effect",
    "text": "Partially standardized effect\nThe Partially Standardized Effect is calculated by dividing the direct (c’) and indirect (ab) effect by the standard deviation of the \\(Y\\) variable:\n\\[c'_{ps} = \\frac{c'}{SD_Y}\\]\n\\[ab_{ps} = \\frac{ab}{SD_Y}\\]\nFor instance, in the estress study, the partially standardized indirect effect is 0.107, meaning that two entrepreneurs who differ by one unit on X (economic stress) differ by about one-tenth (0.107) of a standard deviation in their intentions to withdraw from entrepreneurship as a result of the effect of stress on depressed affect."
  },
  {
    "objectID": "mediation.html#completely-standardized-effect",
    "href": "mediation.html#completely-standardized-effect",
    "title": "Mediation",
    "section": "Completely standardized effect",
    "text": "Completely standardized effect\nThe The Partially Standardized Effect is still expressed in the original \\(X\\) scale (“two entrepreneurs who differ by one unit on X… differ by about one-tenth (0.107) of a standard deviation…”).\nThe Completely Standardized Effect standardizes both the \\(X\\) and the \\(Y\\):\n\\[c'_{cs} = \\frac{SD_X(c')}{SD_Y} = SD_X(c'_ps)\\]\n\\[ab_{cs} = \\frac{SD_X(ab)}{SD_Y} = SD_X(ab_ps)\\]\nThese two measures are identical to the direct and indirect effects when those effects are calculated using standardized regression coefficients (or standardized X, M, and Y are used in the model rather than X, M, and Y in their original metric).\nIn the estress study, one standard deviation change in economic stress leads to a 0.152 standard deviations change in withdrawal intentions, as a result of the effect of stress on affect which in turn influences withdrawal intentions.\nThe completely standardized effect is generally not meaningful if X is a dichotomous variable, thus, in this case, it is not reccommended."
  },
  {
    "objectID": "mediation.html#effect-size-in-process",
    "href": "mediation.html#effect-size-in-process",
    "title": "Mediation",
    "section": "Effect size in PROCESS",
    "text": "Effect size in PROCESS\nTo calculate the partial and complete standardized effect size, just add effsize=1. PROCESS also generates confidence intervals for the partially and completely standardized indirect effects using bootstrapping (or the Monte Carlo method, another way to calculate CIs).\n\nprocess(estress, y = \"withdraw\", x = \"estress\", m = \"affect\", \n        cov = c(\"ese\", \"sex\", \"tenure\"),\n        total = 1, model = 4, progress = 0,\n        effsize = 1,\n        seed=100770)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                \nModel : 4       \n    Y : withdraw\n    X : estress \n    M : affect  \n\nCovariates: \n       ese sex tenure\n\nSample size: 262\n\nCustom seed: 100770\n\n\n*********************************************************************** \nOutcome Variable: affect\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.4039    0.1631    0.4452   12.5231    4.0000  257.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    1.7855    0.3077    5.8033    0.0000    1.1796    2.3914\nestress     0.1593    0.0297    5.3612    0.0000    0.1008    0.2179\nese        -0.1549    0.0444   -3.4892    0.0006   -0.2423   -0.0675\nsex         0.0148    0.0857    0.1726    0.8631   -0.1540    0.1836\ntenure     -0.0108    0.0063   -1.7227    0.0861   -0.0232    0.0016\n\n*********************************************************************** \nOutcome Variable: withdraw\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.4539    0.2060    1.2586   13.2824    5.0000  256.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    2.7461    0.5502    4.9913    0.0000    1.6626    3.8295\nestress    -0.0935    0.0527   -1.7751    0.0771   -0.1973    0.0102\naffect      0.7071    0.1049    6.7420    0.0000    0.5006    0.9137\nese        -0.2121    0.0764   -2.7769    0.0059   -0.3625   -0.0617\nsex         0.1274    0.1441    0.8838    0.3776   -0.1565    0.4112\ntenure     -0.0021    0.0106   -0.1940    0.8463   -0.0230    0.0189\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: withdraw\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.2550    0.0650    1.4763    4.4667    4.0000  257.0000    0.0017\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    4.0087    0.5603    7.1548    0.0000    2.9053    5.1120\nestress     0.0191    0.0541    0.3535    0.7240   -0.0874    0.1257\nese        -0.3216    0.0808   -3.9789    0.0001   -0.4808   -0.1624\nsex         0.1379    0.1561    0.8831    0.3780   -0.1695    0.4453\ntenure     -0.0097    0.0115   -0.8491    0.3966   -0.0323    0.0128\n\n*********************************************************************** \nBootstrapping in progress. Please wait.\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nTotal effect of X on Y:\n     effect        se         t         p      LLCI      ULCI      c_cs\n     0.0191    0.0541    0.3535    0.7240   -0.0874    0.1257    0.0218\n\nDirect effect of X on Y:\n     effect        se         t         p      LLCI      ULCI     c'_cs\n    -0.0935    0.0527   -1.7751    0.0771   -0.1973    0.0102   -0.1068\n\nIndirect effect(s) of X on Y:\n          Effect    BootSE  BootLLCI  BootULCI\naffect    0.1127    0.0292    0.0594    0.1734\n\nCompletely standardized indirect effect(s) of X on Y:\n          Effect    BootSE  BootLLCI  BootULCI\naffect    0.1286    0.0330    0.0668    0.1968\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000"
  },
  {
    "objectID": "mediation.html#learning-outcomes-2",
    "href": "mediation.html#learning-outcomes-2",
    "title": "Mediation",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nUnderstand the difference between parallel and serial multiple mediation models\nLearn to use progress to fit parallel and serial multiple mediation models\nLearn to read the output of the model"
  },
  {
    "objectID": "mediation.html#comparing-theories",
    "href": "mediation.html#comparing-theories",
    "title": "Mediation",
    "section": "Comparing theories",
    "text": "Comparing theories\nFor example, some researchers simultaneously examined three potential mediators of the effectiveness of a 30-session, 1-year experimental weight loss intervention among middle-aged women:\n\nEmotional eating (e.g., eating to placate a negative mood)\nRestrained eating (e.g., not eating after feeling full)\nPerceived barriers to exercise\n\n\n\n\n\n\nThey found that relative to women randomly assigned to a control weight-loss program, those who experienced the experimental method did lose more weight over the year.\nThe mediation analysis suggested that:\n\nThe intervention reduced frequency of emotional eating and increased restraint while eating, which in turn resulted in greater weight loss\nIndependent of these two mechanisms, there was no evidence that the intervention influenced weight loss by changing perceived barriers to exercise"
  },
  {
    "objectID": "mediation.html#epiphenomenality",
    "href": "mediation.html#epiphenomenality",
    "title": "Mediation",
    "section": "Epiphenomenality",
    "text": "Epiphenomenality\nEstablishing an indirect effect of \\(X\\) on \\(Y\\) through \\(M\\) through a simple mediation analysis does not imply that \\(M\\) is the only mechanism at work linking \\(X\\) to \\(Y\\).\nThe indirect effect could also be due to an epiphenomenal association between the \\(M\\) in a simple mediation model and the “true” mediator or mediators causally between \\(X\\) and \\(Y\\).\nFor instance, in the Presumed Media Influence Study (“pmi” dataset) any variable correlated with the mediator presumed media influence (M) and also affected by the experimental manipulation of article location (X) could be the actual mediator transmitting the effect of location on intentions to buy sugar.\n\n\n\n\n\nThe authors recognized this and so had the foresight to measure a variable related to another possible mechanism: perceived issue importance (“import” variable).\nPerhaps people infer, from where an article is published in the newspaper, the extent to which the issue is something worthy of attention, and thereby potentially something one should think about and perhaps act upon.\nSo they measured people’s beliefs about how important the potential sugar shortage was (“import”) using two questions that were aggregated to form a perceived importance measure.\nIssue importance (import) is actually correlated (r = 0.282; p < 0.01) with presumed media influence (pmi), so the epiphenomenal explanation for the pmi mediator is plausible.\nTo clarify the role of the two possible mediators, we can fit a parallel multiple mediator model with two mediators, by using the same code used for the simple mediation model, just adding more than one variable following the parameter m.\nAnother difference is that we add the parameter contrast = 1 to conduct a test of differences between specific indirect effects with bootstrapping.\nAlso, the results include the total indirect effect (the indirect effect summed across all mediators), and the indirect effect of each mediator.\n\nprocess(pmi_data, y = \"reaction\", x = \"cond\", m = c(\"import\", \"pmi\"), \n        model = 4, total = 1, contrast = 1, effsize = 1, progress = 0,\n        seed = 10)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                \nModel : 4       \n    Y : reaction\n    X : cond    \n   M1 : import  \n   M2 : pmi     \n\nSample size: 123\n\nCustom seed: 10\n\n\n*********************************************************************** \nOutcome Variable: import\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.1809    0.0327    2.9411    4.0942    1.0000  121.0000    0.0452\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    3.9077    0.2127   18.3704    0.0000    3.4866    4.3288\ncond        0.6268    0.3098    2.0234    0.0452    0.0135    1.2401\n\n*********************************************************************** \nOutcome Variable: pmi\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.1808    0.0327    1.7026    4.0878    1.0000  121.0000    0.0454\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    5.3769    0.1618   33.2222    0.0000    5.0565    5.6973\ncond        0.4765    0.2357    2.0218    0.0454    0.0099    0.9431\n\n*********************************************************************** \nOutcome Variable: reaction\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.5702    0.3251    1.6628   19.1118    3.0000  119.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant   -0.1498    0.5298   -0.2828    0.7778   -1.1989    0.8993\ncond        0.1034    0.2391    0.4324    0.6662   -0.3701    0.5768\nimport      0.3244    0.0707    4.5857    0.0000    0.1843    0.4645\npmi         0.3965    0.0930    4.2645    0.0000    0.2124    0.5806\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: reaction\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.1603    0.0257    2.3610    3.1897    1.0000  121.0000    0.0766\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    3.2500    0.1906   17.0525    0.0000    2.8727    3.6273\ncond        0.4957    0.2775    1.7860    0.0766   -0.0538    1.0452\n\n*********************************************************************** \nBootstrapping in progress. Please wait.\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nTotal effect of X on Y:\n     effect        se         t         p      LLCI      ULCI      c_ps\n     0.4957    0.2775    1.7860    0.0766   -0.0538    1.0452    0.3197\n\nDirect effect of X on Y:\n     effect        se         t         p      LLCI      ULCI     c'_ps\n     0.1034    0.2391    0.4324    0.6662   -0.3701    0.5768    0.0667\n\nIndirect effect(s) of X on Y:\n          Effect    BootSE  BootLLCI  BootULCI\nTOTAL     0.3923    0.1636    0.0896    0.7329\nimport    0.2033    0.1163    0.0024    0.4684\npmi       0.1890    0.1042    0.0066    0.4175\n(C1)      0.0144    0.1483   -0.2681    0.3141\n\nPartially standardized indirect effect(s) of X on Y:\n          Effect    BootSE  BootLLCI  BootULCI\nTOTAL     0.2530    0.1032    0.0581    0.4652\nimport    0.1312    0.0740    0.0016    0.2975\npmi       0.1219    0.0667    0.0045    0.2646\n(C1)      0.0093    0.0960   -0.1758    0.2001\n\nSpecific indirect effect contrast definition(s):\n(C1)     import   minus    pmi\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000"
  },
  {
    "objectID": "mediation.html#interpretation",
    "href": "mediation.html#interpretation",
    "title": "Mediation",
    "section": "Interpretation",
    "text": "Interpretation\nThe interpretation of the total and direct effect do not change, while indirect effect includes the indirect effects of each mediator.\nAlso, if we use contrast = 1, there is a test of statistically significance of the difference between the indirect effects. In this case the output includes also the description of variables tested.\nIn this case, the tested difference is import - pmi. The difference is positive (0.0144) meaning that, with reference to the internal page condition, those assigned to the front page condition have stronger intentions to buy sugar (by 0.0144 units) as result of the mediation of “import” compared to the effect of the mediation of “pmi”. The difference is not statistically significant, though.\nIn the output you can also read the standardized effects, if you require it. Notice that in this case only the partial standardized effect is calculated for the indirect effect.\nIndeed, the \\(X\\) of this dataset is dichotomous, and completely standardized effects are not easy to interpret in this case. Instead, PROCESS can produce partially and completely standardized measures of total, direct, and indirect effects in mediation models when X is a numerical continuum."
  },
  {
    "objectID": "mediation.html#example",
    "href": "mediation.html#example",
    "title": "Mediation",
    "section": "Example",
    "text": "Example\nSome researchers compared the anxiety of residents living in a low-income housing development located in a middleclass neighborhood to a matched group who applied to live in the housing development but remained on the waiting list.\nThey argued that life in a middle-class housing development (\\(X\\)) would reduce (\\(M_1\\)) exposure to neighborhood disorder (e.g., crime, homeless people, drugs and drug use, violence) relative to those living elsewhere, which would in turn reduce (\\(M_2\\)) the number of stressful life experiences, which in turn would translate into (\\(Y\\)) fewer anxiety symptoms.\nConsidering the Presumed Media Influence study dataset, it is plausible that the perceived issue importance (\\(M_1\\)) influences people’s beliefs about how others are going to be influenced by the media \\(M_2\\): “This article will be published on the front page, so it must be important, and people will take notice of such an important matter and act by buying sugar to stock up. Therefore, I should go out and buy sugar before supplies are all gone!”"
  },
  {
    "objectID": "mediation.html#estimation-with-process-1",
    "href": "mediation.html#estimation-with-process-1",
    "title": "Mediation",
    "section": "Estimation with PROCESS",
    "text": "Estimation with PROCESS\nTo fit a serial multiple mediator model:\n\nuse model = 6 instead of model = 4\npay attention to the order of the variables listed following the parameter m: unlike in model 4 where order is ignored, when model 6 is specified, the order matters!. The order of the variables in the list of mediators is taken literally as the causal sequence, with the first mediator variable in the list causally prior to the second in the list, and so forth\nyou can add contrast = 1 if you also want to test differences between specific indirect effects\n\n\nprocess(pmi_data, y = \"reaction\", x = \"cond\", m = c(\"import\", \"pmi\"), \n        model = 6, total = 1, contrast = 1, boot = 10000, progress = 0,\n        seed = 031216)"
  },
  {
    "objectID": "mediation.html#learning-outcomes-3",
    "href": "mediation.html#learning-outcomes-3",
    "title": "Mediation",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the characteristic of a multicategorical variable (and the difference with dichotomous and continuous variables)\nLearn to use progress to fit a mediation model with multicategorical X variable\nLearn to interpret the output of the model"
  },
  {
    "objectID": "mediation.html#mediation-with-a-multicategorical-antecedent-1",
    "href": "mediation.html#mediation-with-a-multicategorical-antecedent-1",
    "title": "Mediation",
    "section": "Mediation with a multicategorical antecedent",
    "text": "Mediation with a multicategorical antecedent\nAs far as now we have discussed models with a dichotomous or continuous \\(X\\). The interpretation of these models is, on the most basic level, based on a 1-unit change in the \\(X\\).\nHowever, there are also cases where \\(X\\) is a multicategorical variable. Multicategorical variables are variables that can take on three or more categorical values (the values of a categorical variable are also called modalities, or categories).\nFor instance, we may have a multicategorical \\(X\\) variable with three modalities corresponging to three experimental conditions: information in mainstream newspaper (COND=0), information in alternative newspaper (COND=1), and information on social media (COND=2).\nThe interpretation, in this case, is a bit different from that of dichotomous or continuous variables.\nBefore considering multicategorical variables, let’s recap the interpretation of models with dichotomous and continuous \\(X\\)."
  },
  {
    "objectID": "mediation.html#recap",
    "href": "mediation.html#recap",
    "title": "Mediation",
    "section": "Recap",
    "text": "Recap\n\nInterpreting models with a continuous X\nContinuous variables are quantitative variables, resulting from a process of measurement, and can take on several values.\nConsidering, for example, the estress dataset, we found that the indirect effect of the economic stress condition (\\(X\\)) on the intention to withdraw (\\(Y\\)) through the mediator of depressed affect (\\(M\\)), was 0.1330.\nHow can we interpret this coefficient?\nTwo businessman who differ by one unit in their economic stress are estimated to differ by 0.133 unit in their reported intentions to withdraw from their business (…as a result of the tendency for those under relatively more economic stress to feel more depressed affect, which in turn translates into greater withdrawal intentions).\nOr similarly: 1-unit increase in economic stress is associated with an increase of 0.133 units, on average, in withdraw intention (…).\n\n\nInterpreting models with a dichotomous X\nA dichotomous \\(X\\) variable can take on just two values, which are usually coded as 0 and 1.\nFor instance, in the PMI dataset, there are only two (experimental) condition (“COND”): people were said the article was going to be published in the middle of an economic supplement of the newspaper (COND = 0), or on the frontpage (COND = 1).\nUsing the variable “perceived media influence” (pmi) as a mediator, we found that the indirect effect of X on Y was equal to 0.2413 (the indirect effect quantifies the effect of \\(X\\) on \\(Y\\) through a mediator \\(M\\)). How do you interpret this coefficient?\nRelative to those assigned to the interior page condition (COND=0), those who read an article they were told was to be published in the front page of the newspaper (COND=1) were, on average, 0.241 units higher in their likelihood of buying sugar (…as a result of the effect of the location of the article on presumed media influence which, in turn, putatively affected people’s intentions to buy sugar).\nNotice that with dichotomous variables, we don’t interpret the results with reference to 1-unit increase in \\(X\\), but relative to those assigned to the condition COND=0.\nThe group coded with 0 is the reference group and the coefficients in the model are intepreted with reference to this group. The coefficients express differences with the reference group.\nWe said: Relative to those assigned to the interior page condition (COND=0), those who read an article they were told was to be published in the front page of the newspaper (COND=1) were, on average, 0.241 units higher in their likelihood of buying sugar.\nThat is to say: \\(Y_{COND_1} = Y_{COND_0} + 0.241\\)\nLet’s make another example, with a dichotomous variable where MALE=0 and FEMALE=1, and a simple regression model as follows:\n\\[Y = 0.5X + e\\]\nLet’s say Y is “intuitiveness”.\nHow do we interpret (how do we “read” in plain English) this equation?\nIn abstract terms, we can interpret/read \\(Y = 0.5X + e\\) by saying that for 1-unit increase in X, there is half a unit (0.5) increase, on average, in Y (plus some error \\(e\\)).\nBut since X is dichotomous, 1 unit increase in a dichotomous variable means switching from one modality to the other, in this case from MALES (0) to FEMALES (1).\nIn other terms, we can interpret the coefficients of the regression model with reference to MALE=0, and say: compared to MALES, we would expect, on average, FEMALE to be half a point more intuitive."
  },
  {
    "objectID": "mediation.html#mediation-with-a-multicategorical-antecedent-2",
    "href": "mediation.html#mediation-with-a-multicategorical-antecedent-2",
    "title": "Mediation",
    "section": "Mediation with a multicategorical antecedent",
    "text": "Mediation with a multicategorical antecedent\nSimilarly to dichotomous variables, we interpret a multicategorial variable in relation to a reference group.\n\n\n\n\n\nSimilarly to how we interpret the results from a dichotomous \\(X\\) relative to the category coded as \\(0\\) (e.g.: “relative to those assigned to the interior page condition (COND=0), those who read an article they were told was to be published in the front page of the newspaper (COND=1)…”), we interpret a multicategorical \\(X\\) relatively to one of its modality used as a reference group.\nThus, when working with a multicategorical variable, we compare two or more group of cases, to the one we use as reference.\nWe can select the group we want as the reference group. Often, we choose the reference group coherently with our research focus, so as to obtain meaningful results.\n\nExample\nTo make an example of model with multicategorical \\(X\\) we use the PROTEST dataset.\nIn this study, 129 participants, all of whom were female, received a written account of the fate of a female attorney (Catherine) who lost a promotion to a less qualified male as a result of discriminatory actions of the senior partners.\nAfter reading this story, which was the same in all conditions, the participants were given a description of how Catherine responded to this sexual discrimination. Those randomly assigned to three conditions\n\nno protest condition (coded PROTEST=0) learned that though very disappointed by the decision, Catherine decided not to take any action against this discrimination and continued working at the firm.\nThose assigned to the individual protest condition (coded PROTEST=1) were told that Catherine approached the partners to protest the decision, while giving various explanations as to why the decision was unfair that revolved around her, such as that she was more qualified for the job, and that it would hurt her career.\nBut those randomly assigned to a collective protest condition (PROTEST=2) were told Catherine protested and framed her argument around how the firm has treated women in the past, that women are just as qualified as men, and that they should be treated equally.\n\nFollowing this manipulation of Catherine’s response to the discrimination, the participants responded to a set of questions measuring how appropriate they perceived her response was for this situation. Higher scores on this variable (RESPAPPR in the data file) reflect a stronger perception of appropriateness of the response.\nFinally, the participants were asked to respond to six questions evaluating Catherine. Their responses were aggregated into a measure of liking, such that participants with higher scores liked her relatively more (LIKING in the data file).\n\nprotest <- read.csv(\"data/protest.csv\")\n\nTo tell PROCESS X is multicategorical (the \\(X\\) variable can contain up to nine modalities), use the mcx option (for multicategorical X), with an argument following an equals sign telling PROCESS how to code the groups. To perform our analysis with a standard coding procedure, we use mcx=1 (“indicator coding”).\n\nprocess(protest, y = \"liking\", x = \"protest\", m = \"respappr\",\n        mcx = 1, total = 1, model = 4, progress = 0,\n        seed = 30217)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                \nModel : 4       \n    Y : liking  \n    X : protest \n    M : respappr\n\nSample size: 129\n\nCustom seed: 30217\n\nCoding of categorical X variable for analysis: \n    protest        X1        X2\n     0.0000    0.0000    0.0000\n     1.0000    1.0000    0.0000\n     2.0000    0.0000    1.0000\n\n*********************************************************************** \nOutcome Variable: respappr\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.5106    0.2607    1.3649   22.2190    2.0000  126.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    3.8841    0.1825   21.2881    0.0000    3.5231    4.2452\nX1          1.2612    0.2550    4.9456    0.0000    0.7565    1.7659\nX2          1.6103    0.2522    6.3842    0.0000    1.1111    2.1095\n\n*********************************************************************** \nOutcome Variable: liking\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.5031    0.2531    0.8427   14.1225    3.0000  125.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    3.7103    0.3074   12.0711    0.0000    3.1020    4.3187\nX1         -0.0037    0.2190   -0.0169    0.9865   -0.4371    0.4297\nX2         -0.2202    0.2280   -0.9658    0.3360   -0.6715    0.2310\nrespappr    0.4119    0.0700    5.8844    0.0000    0.2734    0.5504\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: liking\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.2151    0.0463    1.0676    3.0552    2.0000  126.0000    0.0506\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    5.3102    0.1614   32.9083    0.0000    4.9909    5.6296\nX1          0.5158    0.2255    2.2870    0.0239    0.0695    0.9621\nX2          0.4431    0.2231    1.9863    0.0492    0.0016    0.8845\n\n*********************************************************************** \nBootstrapping in progress. Please wait.\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nRelative total effects of X on Y:\n      effect        se         t         p      LLCI      ULCI\nX1    0.5158    0.2255    2.2870    0.0239    0.0695    0.9621\nX2    0.4431    0.2231    1.9863    0.0492    0.0016    0.8845\n\nOmnibus test of total effect of X on Y:\n    R2-chng         F       df1       df2         p\n     0.0463    3.0552    2.0000  126.0000    0.0506\n----------\n\nRelative direct effects of X on Y:\n      effect        se         t         p      LLCI      ULCI\nX1   -0.0037    0.2190   -0.0169    0.9865   -0.4371    0.4297\nX2   -0.2202    0.2280   -0.9658    0.3360   -0.6715    0.2310\n\nOmnibus test of direct effect of X on Y:\n    R2-chng         F       df1       df2         p\n     0.0087    0.7286    2.0000  125.0000    0.4846\n\n----------\n\nRelative indirect effects of X on Y:\n\nprotest    ->    respappr    ->    liking\n\n      Effect    BootSE  BootLLCI  BootULCI\nX1    0.5195    0.1518    0.2471    0.8442\nX2    0.6633    0.1677    0.3680    1.0141\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000\n\n\nThe group coded with PROTEST=0 (no protest group), is specified as the reference group. With indicator coding (mcx=1), PROCESS uses the group with the numerically smallest code in the variable specified as X as the reference group.\nIf you would rather have a different group as the reference group, you have to recode X so that the desired reference group is coded with the numerically smallest value.\nProcess authomatically transforms the multicategorical variable with three modalities, in two variables X1 (PROTEST = 1, individual protest) and X2 (PROTEST = 2, collective protest), which coefficients can be interpreted with reference to the reference group (PROTEST = 0, no protest).\nThe output reports a legend with the coding of the multicategorical X variable used for the analysis.\n\n\n\n\n\n\n\n\n\n\n\nprocess(protest, y = \"liking\", x = \"protest\", m = \"respappr\",\n        mcx = 1, total = 1, model = 4, progress = 0,\n        seed = 30217)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                \nModel : 4       \n    Y : liking  \n    X : protest \n    M : respappr\n\nSample size: 129\n\nCustom seed: 30217\n\nCoding of categorical X variable for analysis: \n    protest        X1        X2\n     0.0000    0.0000    0.0000\n     1.0000    1.0000    0.0000\n     2.0000    0.0000    1.0000\n\n*********************************************************************** \nOutcome Variable: respappr\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.5106    0.2607    1.3649   22.2190    2.0000  126.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    3.8841    0.1825   21.2881    0.0000    3.5231    4.2452\nX1          1.2612    0.2550    4.9456    0.0000    0.7565    1.7659\nX2          1.6103    0.2522    6.3842    0.0000    1.1111    2.1095\n\n*********************************************************************** \nOutcome Variable: liking\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.5031    0.2531    0.8427   14.1225    3.0000  125.0000    0.0000\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    3.7103    0.3074   12.0711    0.0000    3.1020    4.3187\nX1         -0.0037    0.2190   -0.0169    0.9865   -0.4371    0.4297\nX2         -0.2202    0.2280   -0.9658    0.3360   -0.6715    0.2310\nrespappr    0.4119    0.0700    5.8844    0.0000    0.2734    0.5504\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: liking\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.2151    0.0463    1.0676    3.0552    2.0000  126.0000    0.0506\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant    5.3102    0.1614   32.9083    0.0000    4.9909    5.6296\nX1          0.5158    0.2255    2.2870    0.0239    0.0695    0.9621\nX2          0.4431    0.2231    1.9863    0.0492    0.0016    0.8845\n\n*********************************************************************** \nBootstrapping in progress. Please wait.\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nRelative total effects of X on Y:\n      effect        se         t         p      LLCI      ULCI\nX1    0.5158    0.2255    2.2870    0.0239    0.0695    0.9621\nX2    0.4431    0.2231    1.9863    0.0492    0.0016    0.8845\n\nOmnibus test of total effect of X on Y:\n    R2-chng         F       df1       df2         p\n     0.0463    3.0552    2.0000  126.0000    0.0506\n----------\n\nRelative direct effects of X on Y:\n      effect        se         t         p      LLCI      ULCI\nX1   -0.0037    0.2190   -0.0169    0.9865   -0.4371    0.4297\nX2   -0.2202    0.2280   -0.9658    0.3360   -0.6715    0.2310\n\nOmnibus test of direct effect of X on Y:\n    R2-chng         F       df1       df2         p\n     0.0087    0.7286    2.0000  125.0000    0.4846\n\n----------\n\nRelative indirect effects of X on Y:\n\nprotest    ->    respappr    ->    liking\n\n      Effect    BootSE  BootLLCI  BootULCI\nX1    0.5195    0.1518    0.2471    0.8442\nX2    0.6633    0.1677    0.3680    1.0141\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000\n\n\n\n\nIntepretation of the Total Effect\nLet’s start focusing on the TOTAL EFFECT MODEL, and let’s write the equation based on the coefficients.\n\\[Y_{liking} = 5.3102 + 0.5158(X_1) + 0.4431(X_2)\\]\nThe TOTAL EFFECT MODEL is the total effect of \\(X\\) on \\(Y\\) without taking into account any other varaible. With a multicategorical \\(X\\) it results in the average value of \\(Y\\) for each modality of the multicategorical \\(X\\).\n\ntapply(protest$liking, protest$protest, mean)\n\n       0        1        2 \n5.310244 5.826047 5.753333 \n\n\nIn particular, the intercept (5.3102) is the average value fo \\(Y\\) when all the \\(Xs\\) are equal zero. But when all the \\(Xs\\) are equal zero we have the average Y of the reference group (check the legend with the coding).\n\n\n\n\n\nWe also said we interpret the other coefficients with reference to the reference group, and indeed the other coefficients are express with reference to the intercept. Remember that when a modality of \\(X\\) is “on” it is equal 1, while the others are equal to 0 (check the legend with the coding again).\nTotal model: \\[Y_{liking} = 5.3102 + 0.5158(X_1) + 0.4431(X_2)\\]\n\\[X_0 = 5.3102 + 0.5158*0 + 0.4431*0 = 5.3102\\] \\[X_1 = 5.3102 + 0.5158*1 + 0.4431*0 = 5.8260\\] \\[X_1= 5.3102 + 0.5158*0 + 0.4431*1 = 5.7533\\]\nFrom the Omnibus test of total effect of X on Y we can deduce that the total effect of X on Y (thus irrespective of whether that effect is direct or indirect) is (almost) significant:\n\\[R^2 = 0.046; F(2; 126) = 3.055; p = 0.051\\]\nThe results suggest that her response to the discrimination (\\(X\\)) did influence how she was perceived (\\(Y\\)) (strictly speaking, the p-value is just above the 0.05 threshold. I am following the Hayes interpretation here).\n\n\nInterpretation of the Direct Effect\nThe direct effect is the effect of \\(X\\) on \\(Y\\) holding the mediator (and possible other covariates) constant, where “constant” means that the mediator is held at its average value (\\(M_respappr = 4.866279\\)).\n\nmean(protest$respappr)\n\n[1] 4.866279\n\n\nThe resulting estimates of \\(Y\\) are also called adjusted means, that is estimates of \\(Y\\) for each group when the covariates are set to their sample mean. In a mediation model, the mediator mathematically functions like a covariate, with \\(X\\)’s effect on \\(Y\\) estimated after controlling for \\(M\\).\nThe equation coefficients for the direct effect model can be found the model for the \\(Y\\) outcome variable, in this case liking: \\[3.7103 − 0.0047(X_1) − 0.2202(X_2) + 0.4119(M)\\]\nPluggin the average value of \\(M\\) in the equation: \\[3.7103 − 0.0047(X_1) − 0.2202(X_2) + 0.4119(4.866279)\\]\n\n\n\n\n\nTo derive the direct effect of each modality of the \\(X\\) variable, we proceed in the same way as before, sobsituting the variables with 0 and 1 (NP = no protest, X=0; IP = individual protest, X=1, CP = collective protest, X=2):\n\\[Y_{NP} = 3.71 − 0.005(0) - 0.22(0) + 0.412(4.866) = 5.715\\] \\[Y_{IP} = 3.71 − 0.005(1) − 0.22(0) + 0.412(4.866) = 5.710\\] \\[Y_{CP} = 3.71 − 0.005(0) − 0.22(1) + 0.412(4.866) = 5.495\\]\nThere is no need to calculate these values by hand if you just want the Relative direct effects of X on Y (bottom part of the the output). They express the direct effect in terms of differences with the reference group (DI = direct effect), also reporting the statistical significance of the difference (not significant, in this case):\n\\(X1_{DI} = X1_{DI} - X0_{DI} = 5.71002 - 5.71472 = -0.0047\\) \\(X2_{DI} = X1_{DI} - X0_{DI} = 5.49452 - 5.71472 = -0.2202\\)\nThe Omnibus test of direct effect of X on Y calculates the statistical significance of the overall direct effect of \\(X\\) on \\(Y\\): different types of protest do not directly impact the liking of Catherine, when we control for out the effect of the mediator (by keeping it constant).\nThis equation also provides an estimate of respappr (b), the effect of the mediator (perceived response appropriateness) on liking of Catherine among participants told the same thing about her behavior. In other words, this coefficient is the effect of \\(M\\) on \\(Y\\) holding constant \\(X\\).\n\n\n\n\n\nAmong two people told the same thing about Catherine’s response, the person who perceived her behavior as one unit higher in appropriateness liked her 0.4119 units more. The more appropriate Catherine’s behavior was perceived as being for the situation, the more she was liked.\n\n\nInterpretation of the Indirect Effect\nFinally, in the last part of the output there is the estimated effect of \\(X\\) on \\(Y\\) through \\(M\\) (“Relative indirect effects of X on Y”).\n\n\n\n\n\nThe coefficient of \\(X1\\) (individual protest) and \\(X2\\) (collective protest) can be interpreted in relation to the reference group (no protest):\n\\[X1 = X0 + 0.5195\\] \\[X1 = X0 + 0.6633\\]\nIt means that relative to not protesting at all, protesting with an individualistic focus enhanced the likeability of Catherine by 0.520 units, because individually protesting was seen as more appropriate than not protesting, and this translated into a more positive evaluation.\nLikewise, collectively protesting enhanced the likeability of Catherine by 0.663 units relative to not protesting, as collectively protesting was seen as more appropriate than not protesting, and this translated into a more positive evaluation of her.\n\n\nReporting results\nYou can report the results of the model by using a table like the following (D1 and D2 are the variables we called X1 and X2):\n\n\n\n\n\n\n\nAlternative coding method\nProcess implement an alterantive coding method to those just seen.\nUsing the same example, it may be worthwhile to examine the effect of protesting, whether collectively or individually, relative to not protesting at all, as well the effect of protesting individually relative to collectively.\nThere is a way of representing the three groups with a coding system that provides precisely this information, it also goes by the name Helmert coding. In the Hayes book, chapter 2 on Multicategorical Antecendents, you can find more information about that.\n\n\nFinal observations\n\nCollinearity\nIn a multiple mediator model, a specific indirect effect quantifies the influence of X on Y through a particular mediator while holding constant other mediators.\nThis is useful when the mediators are somewhat correlated. But when the intercorrelation between mediators becomes too large, the usual problems with collinearity in regression models begin to take hold and muddle the results.\nCollinearity between predictors increases sampling variance in estimates of their partial relationships with an outcome, and such sampling variance will propagate throughout the estimates of indirect effects and increase the width of confidence intervals (or the p-values).\nIncluding correlated mediators in the model allows you to disentangle spurious and epiphenomenal association from potential causal association, but this comes at the cost of greater sampling variance and reduced power for tests of indirect effects especially when the sample size is small\n(Reduced power = harder to detect significant effects, i.e. results can wrongly show nonsignificant effects).\n\n\nTotal indirect effect\nIn a parallel multiple mediator model with several mediators, it is possible that the total indirect effect is nonsignificant, even if one or more of the partial indirect effects are significant, for instance:\n\nIn a model with several mediators, only one is actually transmitting X’s effect on Y. The inclusion of a bunch of potential mediators in the model reduces power for tests of indirect effects (see previous slides)\nSince the total indirect effect is a sum of all specific indirect effects, if those indirect effects differ in sign but are of similar magnitude, their sum very well may be zero or nearly so\n\nEscapes from these apparently paradoxical inconsistencies:\n\nAcknowledge the uncertainty inherent in our estimates as communicated through confidence intervals. The fact that a confidence interval for an effect contains zero does not mean the effect is zero. It means that zero is in the realm of possibility, or that one cannot say with certainty what the direction of the effect is.\nDiscount the relevance of the total indirect effect when interpreting the results. In some situations, the total indirect effect will have little substantive or theoretical value."
  },
  {
    "objectID": "moderation.html#output",
    "href": "moderation.html#output",
    "title": "Moderation",
    "section": "Output",
    "text": "Output\nThe output can be divided in three main section:\n\nA first section with the usual r-squared and coefficients of the model (which this time includes also the coefficients for the interaction term), as well as a global statistical test of interaction (i.e.: moderation) (test(s) of highest order unconditional interaction(s)) reporting the p-values and the r2 attributed to the interaction term;\nThree tables reporting data to “probe” the interaction (conditional effects of the focal predictor at values of the moderator(s); moderator value(s) defining johnson-neyman significance region(s); conditional effect of focal predictor at values of the moderator);\ndata to visualize the conditional effect of the focal predictor.\n\n\nprocess(y = \"justify\", x = \"frame\", w = \"skeptic\",\n        model = 1, \n        jn = 1, \n        plot = 1,\n        decimals = 10.2,\n        data = disaster)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : justify\n    X : frame  \n    W : skeptic\n\nSample size: 211\n\n\n*********************************************************************** \nOutcome Variable: justify\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n        0.50       0.25       0.66      22.54       3.00     207.00       0.00\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant       2.45       0.15      16.45       0.00       2.16       2.75\nframe         -0.56       0.22      -2.58       0.01      -0.99      -0.13\nskeptic        0.11       0.04       2.76       0.01       0.03       0.18\nInt_1          0.20       0.06       3.64       0.00       0.09       0.31\n\nProduct terms key:\nInt_1  :  frame  x  skeptic      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W       0.05      13.25       1.00     207.00       0.00\n----------\nFocal predictor: frame (X)\n      Moderator: skeptic (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n     skeptic     effect         se          t          p       LLCI       ULCI\n        1.59      -0.24       0.15      -1.62       0.11      -0.54       0.05\n        2.80       0.00       0.12       0.01       0.99      -0.23       0.23\n        5.20       0.48       0.15       3.21       0.00       0.19       0.78\n\nModerator value(s) defining Johnson-Neyman significance region(s):\n       Value    % below    % above\n        1.17       6.64      93.36\n        3.93      67.77      32.23\n\nConditional effect of focal predictor at values of the moderator:\n     skeptic     effect         se          t          p       LLCI       ULCI\n        1.00      -0.36       0.17      -2.09       0.04      -0.70      -0.02\n        1.17      -0.33       0.17      -1.97       0.05      -0.65       0.00\n        1.42      -0.28       0.16      -1.77       0.08      -0.58       0.03\n        1.84      -0.19       0.14      -1.36       0.17      -0.47       0.09\n        2.26      -0.11       0.13      -0.84       0.40      -0.36       0.15\n        2.68      -0.02       0.12      -0.19       0.85      -0.26       0.21\n        3.11       0.06       0.11       0.55       0.58      -0.16       0.29\n        3.53       0.15       0.11       1.31       0.19      -0.07       0.37\n        3.93       0.23       0.12       1.97       0.05       0.00       0.46\n        3.95       0.23       0.12       1.99       0.05       0.00       0.46\n        4.37       0.32       0.12       2.54       0.01       0.07       0.56\n        4.79       0.40       0.14       2.94       0.00       0.13       0.67\n        5.21       0.49       0.15       3.22       0.00       0.19       0.78\n        5.63       0.57       0.17       3.41       0.00       0.24       0.90\n        6.05       0.66       0.19       3.54       0.00       0.29       1.02\n        6.47       0.74       0.20       3.62       0.00       0.34       1.14\n        6.89       0.82       0.22       3.68       0.00       0.38       1.27\n        7.32       0.91       0.24       3.72       0.00       0.43       1.39\n        7.74       0.99       0.27       3.74       0.00       0.47       1.52\n        8.16       1.08       0.29       3.76       0.00       0.51       1.64\n        8.58       1.16       0.31       3.77       0.00       0.56       1.77\n        9.00       1.25       0.33       3.78       0.00       0.60       1.90\n\nData for visualizing the conditional effect of the focal predictor:\n       frame    skeptic    justify\n        0.00       1.59       2.62\n        1.00       1.59       2.38\n        0.00       2.80       2.75\n        1.00       2.80       2.75\n        0.00       5.20       3.00\n        1.00       5.20       3.48\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nW values in conditional tables are the 16th, 50th, and 84th percentiles.\n\n\n–"
  },
  {
    "objectID": "moderation.html#interpretation-model-summary-and-coefficients",
    "href": "moderation.html#interpretation-model-summary-and-coefficients",
    "title": "Moderation",
    "section": "Interpretation: model summary and coefficients",
    "text": "Interpretation: model summary and coefficients\nModel summary. the first section contains the summary and coefficients of the model. In the model summary (before the coefficients) we see that: - the model is statistically significant (p = 0.000). It means that the model is useful to explain the variation of y. - we also see that the r-squared of the model is 0.246, which means that the model explains about 25% (24.6%) of the variability in y.\nmodel coefficients. next, we have the coefficients of the model. Here we have both the interaction (moderation) term, and the other coefficients. Let’s start with the interaction term (int_1), which is of particular interest: - in this case the coefficient is statistically significant (p < 0.001, 95% ci: 0.092 - 0.310). This information tells us that the the effect of x (framing) on y (strength of justifications for withholding aid) is moderated by w (participants’ climate change skepticism); - we also see that the coefficient is 0.201. The coefficient of the interaction term quantifies how the effect of x on y changes as w changes by one unit. In this case, as w (climate change skepticism) increases by one unit, the strength of justifications (y) between those told climate change was the cause and those not so told (x) “increases” by 0.201 units.\nIn the section test(s) of highest order unconditional interaction(s) we can read that the interaction between x and w explain 4.8% of the total variability explained by the model (r2-chng = 0.048). That is to say, by adding the interaction to the model, we account for an additional 4.8% of the variance in y (“chng” after “r2” in “r2-chng” means that the r2 changes of 0.048 if we include the moderator).\nother coefficients. the other coefficients (frame and skeptic), which do not represent interaction but are the coefficients of variables which are involved in an interaction (in this case x and w), have a different interpretation from that we are used to when working in a multiple regression framework. In a multiple regression model that does not include the interaction term xw, these coefficients would respresent partial effects that quantify how much two cases that differ by one unit on a variable, are estimated to differ on y holding the other varaibles constant. Instead, in a (simple) moderation model, they represent conditional effects, which quantify how much two cases that differ by one unit on a variable, are estimated to differ on y when the other variable (x or w respectively) are zero (indeed, you can observe from the equation that in this case, i.e. When x or w is zero, the interaction term is zero as well: y = i + b1x + b2w + b3xw). In other terms: - b1 is the conditional effect of x on y when w = 0 - b2 is the conditional effect of w on y when x = 0\nWhat should be noticed is that these coefficients don’t always have a substantive interpretation. It mainly depends whether the zero has a or not a meaning in the variables’ measurement scale.\nIn this case the coefficient for the variable w (skeptic = 0.11) is substantively meaningful, because 0 is meaningful in the measurement scale of x (frame). Indeed, frame = 0 is the condition where people were not told about climate change (natural condition). In this case, skeptic = 0.11 means that, given frame = 0, that is to say, among people who read a story that did not attribute the cause of the drought to climate change, people who are one-unit higher in skepticism, are estimated to be 0.11 unit higher in the strength of justifications for withholding aid.\nInstead, the coefficient of frame (-0.562), as well as the statistical significant tests (p-value and confidence intervals), have no substantive meaning, since skepticism in this study is measured on a scale ranging from 1 to 9, therefore no cases measuring 0 on this variable could even exist. Such a meaning is not necessary to estimate the model and the interaction. but if you are interested in interpreting also these coefficient you can transform the data to make the zero meaningful. The procedure is called mean-centering.\n\nprocess(y = \"justify\", x = \"frame\", w = \"skeptic\",\n        model = 1, \n        jn = 1, \n        plot = 1,\n        decimals = 10.2,\n        data = disaster)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : justify\n    X : frame  \n    W : skeptic\n\nSample size: 211\n\n\n*********************************************************************** \nOutcome Variable: justify\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n        0.50       0.25       0.66      22.54       3.00     207.00       0.00\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant       2.45       0.15      16.45       0.00       2.16       2.75\nframe         -0.56       0.22      -2.58       0.01      -0.99      -0.13\nskeptic        0.11       0.04       2.76       0.01       0.03       0.18\nInt_1          0.20       0.06       3.64       0.00       0.09       0.31\n\nProduct terms key:\nInt_1  :  frame  x  skeptic      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W       0.05      13.25       1.00     207.00       0.00\n----------\nFocal predictor: frame (X)\n      Moderator: skeptic (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n     skeptic     effect         se          t          p       LLCI       ULCI\n        1.59      -0.24       0.15      -1.62       0.11      -0.54       0.05\n        2.80       0.00       0.12       0.01       0.99      -0.23       0.23\n        5.20       0.48       0.15       3.21       0.00       0.19       0.78\n\nModerator value(s) defining Johnson-Neyman significance region(s):\n       Value    % below    % above\n        1.17       6.64      93.36\n        3.93      67.77      32.23\n\nConditional effect of focal predictor at values of the moderator:\n     skeptic     effect         se          t          p       LLCI       ULCI\n        1.00      -0.36       0.17      -2.09       0.04      -0.70      -0.02\n        1.17      -0.33       0.17      -1.97       0.05      -0.65       0.00\n        1.42      -0.28       0.16      -1.77       0.08      -0.58       0.03\n        1.84      -0.19       0.14      -1.36       0.17      -0.47       0.09\n        2.26      -0.11       0.13      -0.84       0.40      -0.36       0.15\n        2.68      -0.02       0.12      -0.19       0.85      -0.26       0.21\n        3.11       0.06       0.11       0.55       0.58      -0.16       0.29\n        3.53       0.15       0.11       1.31       0.19      -0.07       0.37\n        3.93       0.23       0.12       1.97       0.05       0.00       0.46\n        3.95       0.23       0.12       1.99       0.05       0.00       0.46\n        4.37       0.32       0.12       2.54       0.01       0.07       0.56\n        4.79       0.40       0.14       2.94       0.00       0.13       0.67\n        5.21       0.49       0.15       3.22       0.00       0.19       0.78\n        5.63       0.57       0.17       3.41       0.00       0.24       0.90\n        6.05       0.66       0.19       3.54       0.00       0.29       1.02\n        6.47       0.74       0.20       3.62       0.00       0.34       1.14\n        6.89       0.82       0.22       3.68       0.00       0.38       1.27\n        7.32       0.91       0.24       3.72       0.00       0.43       1.39\n        7.74       0.99       0.27       3.74       0.00       0.47       1.52\n        8.16       1.08       0.29       3.76       0.00       0.51       1.64\n        8.58       1.16       0.31       3.77       0.00       0.56       1.77\n        9.00       1.25       0.33       3.78       0.00       0.60       1.90\n\nData for visualizing the conditional effect of the focal predictor:\n       frame    skeptic    justify\n        0.00       1.59       2.62\n        1.00       1.59       2.38\n        0.00       2.80       2.75\n        1.00       2.80       2.75\n        0.00       5.20       3.00\n        1.00       5.20       3.48\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nW values in conditional tables are the 16th, 50th, and 84th percentiles."
  },
  {
    "objectID": "moderation.html#mean-centering-to-enable-interpretation-of-coefficients",
    "href": "moderation.html#mean-centering-to-enable-interpretation-of-coefficients",
    "title": "Moderation",
    "section": "Mean centering to enable interpretation of coefficients",
    "text": "Mean centering to enable interpretation of coefficients\nTo make a zero meaningful, it is possible to center the data. Mean-centering is obtained by subtracting the average from each data points of a variable. It is easy to see that this makes the variable average equal to zero. This solve the previous interpretation problem.\nTo use mean-centering in process, you can add center=1 to the process function. This will mean-center all the variables.\nBut sometimes we may want to mean-center just one or a few variables. In our case we can just center the skeptic variable (since this is the one that causes interpretation issues). We can do it by creating another mean-centered variable (skeptic_mc) by subtracting the average of the variable from each data point of the variable itself, and by fitting another model with the new variable.\nHowvere, before manually mean-centering a variable, it is advisable to prepare a dataset without missing data. Indeed, statistical softwares (including process) fit regression models on complete cases only, that is to say, by using only those cases that have no missing data on any varaibles. Unfourtunately, procedures like mean-centering (and standardization, which is similar), depend on the cases included (the average of a variable is obtained by dividing its sum by the number of cases). Therefore, if you mean center the variable using 20 cases, but the final dataset is actually composed of only 15 cases, you are doing something wrong, since you are using an average calculated on the wrong dataset! For instance:\n\nexample <- data.frame(x = c(1, 2, 3, 4, 5, 3, 5, NA, 7),\n                      y = c(1, 2, 3, 4, 5, NA, NA, 4, 5))\n\n# averages calculated on the \"raw\" dataset\nmean(example$x, NA.rm=t) # 3.75\n\n[1] NA\n\nmean(example$y, NA.rm=t) # 3.428571\n\n[1] NA\n\n# averages calculated considering only complete cases\nexample_final <- example[complete.cases(example),]\n\nmean(example_final$x, NA.rm=t) # 3.666667\n\n[1] 3.666667\n\nmean(example_final$y, NA.rm=t) # 3.333333\n\n[1] 3.333333\n\n\nTo create such a dataset (in this case there are no missing values) and mean-centering the variable:\n\n# let's create a dataset without missing data\ndisaster <- disaster[complete.cases(disaster),]\n\n# let's create a mean-centered \"skeptic_mc\" variable\ndisaster$skeptic_mc <- disaster$skeptic - mean(disaster$skeptic)\n\nNext, fit the model with the mean-centered variable.\nThe coefficients for the variable skepticism and for the interaction term (int_1) are unchanged, what changes is the coefficient for frame, which is now interpretable. We can see that its coefficients is 0.12: among people average on skepticism (now skeptic = 0 means skeptic = average), told climate change caused the drought (frame = 1) are 0.12 points higher in their strength of justifications for withholding aid, compared to those not so told (frame = 0). However, this coefficient is not statistically significant (p = 0.30).\n\nprocess(y = \"justify\", x = \"frame\", w = \"skeptic_mc\",\n        model = 1, jn = 1, plot = 1,\n        decimals = 10.2,\n        data = disaster)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n                  \nModel : 1         \n    Y : justify   \n    X : frame     \n    W : skeptic_mc\n\nSample size: 211\n\n\n*********************************************************************** \nOutcome Variable: justify\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n        0.50       0.25       0.66      22.54       3.00     207.00       0.00\n\nModel: \n                coeff         se          t          p       LLCI       ULCI\nconstant         2.81       0.08      36.20       0.00       2.65       2.96\nframe            0.12       0.11       1.05       0.30      -0.10       0.34\nskeptic_mc       0.11       0.04       2.76       0.01       0.03       0.18\nInt_1            0.20       0.06       3.64       0.00       0.09       0.31\n\nProduct terms key:\nInt_1  :  frame  x  skeptic_mc      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W       0.05      13.25       1.00     207.00       0.00\n----------\nFocal predictor: frame (X)\n      Moderator: skeptic_mc (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n  skeptic_mc     effect         se          t          p       LLCI       ULCI\n       -1.79      -0.24       0.15      -1.62       0.11      -0.54       0.05\n       -0.58       0.00       0.12       0.01       0.99      -0.23       0.23\n        1.82       0.48       0.15       3.21       0.00       0.19       0.78\n\nModerator value(s) defining Johnson-Neyman significance region(s):\n       Value    % below    % above\n       -2.21       6.64      93.36\n        0.56      67.77      32.23\n\nConditional effect of focal predictor at values of the moderator:\n  skeptic_mc     effect         se          t          p       LLCI       ULCI\n       -2.38      -0.36       0.17      -2.09       0.04      -0.70      -0.02\n       -2.21      -0.33       0.17      -1.97       0.05      -0.65       0.00\n       -1.96      -0.28       0.16      -1.77       0.08      -0.58       0.03\n       -1.54      -0.19       0.14      -1.36       0.17      -0.47       0.09\n       -1.11      -0.11       0.13      -0.84       0.40      -0.36       0.15\n       -0.69      -0.02       0.12      -0.19       0.85      -0.26       0.21\n       -0.27       0.06       0.11       0.55       0.58      -0.16       0.29\n        0.15       0.15       0.11       1.31       0.19      -0.07       0.37\n        0.56       0.23       0.12       1.97       0.05       0.00       0.46\n        0.57       0.23       0.12       1.99       0.05       0.00       0.46\n        0.99       0.32       0.12       2.54       0.01       0.07       0.56\n        1.41       0.40       0.14       2.94       0.00       0.13       0.67\n        1.83       0.49       0.15       3.22       0.00       0.19       0.78\n        2.25       0.57       0.17       3.41       0.00       0.24       0.90\n        2.67       0.66       0.19       3.54       0.00       0.29       1.02\n        3.10       0.74       0.20       3.62       0.00       0.34       1.14\n        3.52       0.82       0.22       3.68       0.00       0.38       1.27\n        3.94       0.91       0.24       3.72       0.00       0.43       1.39\n        4.36       0.99       0.27       3.74       0.00       0.47       1.52\n        4.78       1.08       0.29       3.76       0.00       0.51       1.64\n        5.20       1.16       0.31       3.77       0.00       0.56       1.77\n        5.62       1.25       0.33       3.78       0.00       0.60       1.90\n\nData for visualizing the conditional effect of the focal predictor:\n       frame skeptic_mc    justify\n        0.00      -1.79       2.62\n        1.00      -1.79       2.38\n        0.00      -0.58       2.75\n        1.00      -0.58       2.75\n        0.00       1.82       3.00\n        1.00       1.82       3.48\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nW values in conditional tables are the 16th, 50th, and 84th percentiles."
  },
  {
    "objectID": "moderation.html#visualizing-moderation",
    "href": "moderation.html#visualizing-moderation",
    "title": "Moderation",
    "section": "Visualizing moderation",
    "text": "Visualizing moderation\nA picture of the model is a useful aid to interpret and report a mediation model.\nThe parameter plot = 1 in the process function returns a table (data for visualizing the conditional effect of the focal predictor) with values which can be used to visualize the interaction (bottom part of the output.\n\nprocess(y = \"justify\", x = \"frame\", w = \"skeptic\",\n        model = 1, \n        jn = 1, \n        plot = 1,\n        decimals = 10.3,\n        data = disaster)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : justify\n    X : frame  \n    W : skeptic\n\nSample size: 211\n\n\n*********************************************************************** \nOutcome Variable: justify\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n       0.496      0.246      0.661     22.543      3.000    207.000      0.000\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant      2.452      0.149     16.449      0.000      2.158      2.745\nframe        -0.562      0.218     -2.581      0.011     -0.992     -0.133\nskeptic       0.105      0.038      2.756      0.006      0.030      0.180\nInt_1         0.201      0.055      3.640      0.000      0.092      0.310\n\nProduct terms key:\nInt_1  :  frame  x  skeptic      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W      0.048     13.250      1.000    207.000      0.000\n----------\nFocal predictor: frame (X)\n      Moderator: skeptic (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n     skeptic     effect         se          t          p       LLCI       ULCI\n       1.592     -0.242      0.149     -1.620      0.107     -0.537      0.052\n       2.800      0.001      0.117      0.007      0.994     -0.229      0.231\n       5.200      0.484      0.151      3.213      0.002      0.187      0.780\n\nModerator value(s) defining Johnson-Neyman significance region(s):\n       Value    % below    % above\n       1.171      6.635     93.365\n       3.934     67.773     32.227\n\nConditional effect of focal predictor at values of the moderator:\n     skeptic     effect         se          t          p       LLCI       ULCI\n       1.000     -0.361      0.173     -2.090      0.038     -0.702     -0.020\n       1.171     -0.327      0.166     -1.971      0.050     -0.654      0.000\n       1.421     -0.277      0.156     -1.774      0.077     -0.584      0.031\n       1.842     -0.192      0.141     -1.364      0.174     -0.469      0.086\n       2.263     -0.107      0.128     -0.837      0.403     -0.359      0.145\n       2.684     -0.022      0.119     -0.189      0.850     -0.256      0.211\n       3.105      0.062      0.113      0.550      0.583     -0.161      0.285\n       3.526      0.147      0.112      1.308      0.192     -0.075      0.368\n       3.934      0.229      0.116      1.971      0.050      0.000      0.458\n       3.947      0.232      0.116      1.991      0.048      0.002      0.461\n       4.368      0.316      0.125      2.539      0.012      0.071      0.562\n       4.789      0.401      0.136      2.940      0.004      0.132      0.670\n       5.211      0.486      0.151      3.219      0.001      0.188      0.783\n       5.632      0.571      0.167      3.408      0.001      0.241      0.901\n       6.053      0.655      0.185      3.535      0.001      0.290      1.021\n       6.474      0.740      0.204      3.621      0.000      0.337      1.143\n       6.895      0.825      0.224      3.679      0.000      0.383      1.267\n       7.316      0.909      0.245      3.718      0.000      0.427      1.392\n       7.737      0.994      0.266      3.744      0.000      0.471      1.518\n       8.158      1.079      0.287      3.762      0.000      0.513      1.644\n       8.579      1.163      0.308      3.773      0.000      0.556      1.771\n       9.000      1.248      0.330      3.781      0.000      0.597      1.899\n\nData for visualizing the conditional effect of the focal predictor:\n       frame    skeptic    justify\n       0.000      1.592      2.619\n       1.000      1.592      2.377\n       0.000      2.800      2.746\n       1.000      2.800      2.747\n       0.000      5.200      2.998\n       1.000      5.200      3.482\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nW values in conditional tables are the 16th, 50th, and 84th percentiles.\n\n\nSome manual work is necessary to create the plots. The code below can be reused by changing the necessary values. First of all, the values of the abovementioned table have to be saved in a x, w, and y variable (in this case respectively frame, skeptic, and justify). Also the lables have to be changed accordingly, when reusing the code with other datasets.\nThe effect of the cause framing manipulation (x) on strength of justifications for withholding aid is reflected in the gap between the two lines, which varies with climate change skepticism (the moderator). Among those lower in skepticism, the model estimates weaker justification for those in the climate change condition than among those in the natural causes condition, but among those more skeptical, the opposite is found.\n\nx <- c(0,1,0,1,0,1)\nw <- c(1.592,1.592,2.80,2.80,5.20,5.20)\ny <- c(2.619,2.377,2.746,2.747,2.998,3.482)\n\nplot(y = y, x = w, pch = 15, col = \"black\",\n     xlab = \"climate change skepticism (w)\",\n     ylab = \"negative justifications (y)\")\n\nlegend.txt <- c(\"natural causes (x=0)\", \"climate change (x=1)\")\nlegend(\"topleft\", legend = legend.txt, lty = c(3,1), lwd = c(3,2))\nlines(w[x==0], y[x==0], lwd = 3, lty = 3, col = \"black\")\nlines(w[x==1], y[x==1], lwd = 2, lty = 1, col = \"black\")"
  },
  {
    "objectID": "moderation.html#learning-objectives-1",
    "href": "moderation.html#learning-objectives-1",
    "title": "Moderation",
    "section": "Learning objectives",
    "text": "Learning objectives\nLearning objectives of unit 3b consist in learning to fit and interpret:\n\nmodels with a dichotomous moderator\nmodels with a continuous moderator and independent variable\nmodels with more than one moderator: additive multiple moderation and moderated moreation."
  },
  {
    "objectID": "moderation.html#dichotomous-moderator-and-continuous-x",
    "href": "moderation.html#dichotomous-moderator-and-continuous-x",
    "title": "Moderation",
    "section": "Dichotomous moderator and continuous x",
    "text": "Dichotomous moderator and continuous x\nWe fitted a moderation model with a dichotomous independent variable (frame) and a continuous mediator (skeptic). The identical procedure can be used with other types of variables. It is also possible to control for covariates.\nAn example of dichotomous moderator and continuous independent variable can be made with the same dataset used in the previous example, by using the dichotomous variable frame as moderator (instead of skeptic) and the variable skeptic as independent variable (instead of frame).\nThe model coefficients are the same as the previous ones. Indeed, from a mathematical perspective, nothing has changed.\nAnyway, the second part of the outputs (from conditional effects of the focal predictor at values of the moderator(s)) is different, since you are using different variables as focal antecendent (x). Also the visualization data is partly different.\n\nprocess(y = \"justify\", x = \"skeptic\", w = \"frame\",\n        model = 1, plot = 1,\n        decimals = 10.2,\n        data = disaster)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : justify\n    X : skeptic\n    W : frame  \n\nSample size: 211\n\n\n*********************************************************************** \nOutcome Variable: justify\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n        0.50       0.25       0.66      22.54       3.00     207.00       0.00\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant       2.45       0.15      16.45       0.00       2.16       2.75\nskeptic        0.11       0.04       2.76       0.01       0.03       0.18\nframe         -0.56       0.22      -2.58       0.01      -0.99      -0.13\nInt_1          0.20       0.06       3.64       0.00       0.09       0.31\n\nProduct terms key:\nInt_1  :  skeptic  x  frame      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W       0.05      13.25       1.00     207.00       0.00\n----------\nFocal predictor: skeptic (X)\n      Moderator: frame (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n       frame     effect         se          t          p       LLCI       ULCI\n        0.00       0.11       0.04       2.76       0.01       0.03       0.18\n        1.00       0.31       0.04       7.65       0.00       0.23       0.39\n\nData for visualizing the conditional effect of the focal predictor:\n     skeptic      frame    justify\n        1.59       0.00       2.62\n        2.80       0.00       2.75\n        5.20       0.00       3.00\n        1.59       1.00       2.38\n        2.80       1.00       2.75\n        5.20       1.00       3.48\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\n\nfirst, in the code we can omit jn = 1, since this is the parameter to request the johnson-neyman significance region(s), which only works with continuous moderators. Also, there is no table with the 21 data points, since it is related to the results of the johnson-neyman tecnique;\nthe table conditional effects of the focal predictor at values of the moderator(s) (pick-a-point approach) does not show the 16th, 50th, and 84th percentiles of the variable distribution, since they do not exist for categorical variables, but shows only the two possible values of the variable (frame = 0 and frame = 1).\n\nthe substantive interpretation is: between two people told nothing about the cause of the drought (frame=0), one-unit increase in climate change skepticism (the “focal predictor”), is estimated to increase the strength of justifications for withholding aid (y) 0.105 units. But among those told climate change was the cause of the drought, the person one unit higher in climate change skepticism is estimated to be 0.306 units higher in justifications for withholding aid. These two conditional effects correspond to the slopes of the lines in the plot.\n\nalso the visualization data are different, since the order of the variable is different (the same code can be used to create the plot, but the variable order should be changed. The same code below can be used for plots of models with same structure: dichotomous w and continuous x).\n\n\nx <- c(1.59, 2.80, 5.20, 1.59, 2.80, 5.20)   \nw <- c(0.00, 0.00, 0.00, 1.00, 1.00, 1.00)\ny <- c(2.62, 2.75, 3.00, 2.38, 2.75, 3.48)\n\nplot(y = y, x = x, pch = 15, col = \"black\", \n     xlab = \"climate change skepticism (x)\",\n     ylab = \"strength of justifications for withholding aid (y)\")\n\nlegend.txt <- c(\"natural causes (w=0)\", \"climate change (w=1)\")\nlegend(\"topleft\", legend = legend.txt, lty = c(3,1), lwd = c(3,2))\nlines(x[w==0], y[w==0], lwd = 3, lty = 3, col = \"black\")\nlines(x[w==1], y[w==1], lwd = 2, lty = 1, col = \"black\")\n\n\n\n\n–"
  },
  {
    "objectID": "moderation.html#continuous-moderator-and-continuous-x",
    "href": "moderation.html#continuous-moderator-and-continuous-x",
    "title": "Moderation",
    "section": "Continuous moderator and continuous x",
    "text": "Continuous moderator and continuous x\nAn example of continuous moderator and x variable can be made by using the dataset glbwarm, using age as moderator and negemot as independent variable. We also add covariates.\nIn this case, it makes sense to specify different values from the 16th, 50th, and 84th percentiles of the distribution of age to probe the interaction. We use 30, 50, 70 with the wmodval option. We also use jn = 1 since the moderator is continuous.\n\nglbwarm <- haven::read_sav(\"data/glbwarm.sav\")\n\nprocess(y = \"govact\", x = \"negemot\", w = \"age\",\n        cov = c(\"posemot\", \"ideology\", \"gender\"),\n        model = 1, \n        jn = 1, \n        plot = 1,\n        wmodval = c(30,50,70),\n        decimals = 10.3,\n        data = glbwarm)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : govact \n    X : negemot\n    W : age    \n\nCovariates: \n       posemot ideology gender\n\nSample size: 815\n\n\n*********************************************************************** \nOutcome Variable: govact\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n       0.633      0.401      1.117     90.080      6.000    808.000      0.000\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant      5.174      0.338     15.287      0.000      4.510      5.838\nnegemot       0.120      0.083      1.449      0.148     -0.042      0.282\nage          -0.024      0.006     -3.993      0.000     -0.036     -0.012\nInt_1         0.006      0.002      4.104      0.000      0.003      0.009\nposemot      -0.021      0.028     -0.768      0.443     -0.076      0.033\nideology     -0.212      0.027     -7.883      0.000     -0.264     -0.159\ngender       -0.011      0.076     -0.147      0.883     -0.160      0.138\n\nProduct terms key:\nInt_1  :  negemot  x  age      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W      0.012     16.839      1.000    808.000      0.000\n----------\nFocal predictor: negemot (X)\n      Moderator: age (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n         age     effect         se          t          p       LLCI       ULCI\n      30.000      0.310      0.041      7.488      0.000      0.228      0.391\n      50.000      0.436      0.026     16.644      0.000      0.385      0.488\n      70.000      0.563      0.040     14.209      0.000      0.485      0.640\n\nThere are no statistical significance transition points within the observed\nrange of the moderator found using the Johnson-Neyman method.\n\nConditional effect of focal predictor at values of the moderator:\n         age     effect         se          t          p       LLCI       ULCI\n      17.000      0.227      0.058      3.900      0.000      0.113      0.342\n      20.333      0.248      0.054      4.623      0.000      0.143      0.354\n      23.667      0.269      0.049      5.466      0.000      0.173      0.366\n      27.000      0.291      0.045      6.454      0.000      0.202      0.379\n      30.333      0.312      0.041      7.612      0.000      0.231      0.392\n      33.667      0.333      0.037      8.961      0.000      0.260      0.406\n      37.000      0.354      0.034     10.505      0.000      0.288      0.420\n      40.333      0.375      0.031     12.210      0.000      0.315      0.435\n      43.667      0.396      0.028     13.965      0.000      0.340      0.452\n      47.000      0.417      0.027     15.562      0.000      0.365      0.470\n      50.333      0.438      0.026     16.736      0.000      0.387      0.490\n      53.667      0.459      0.027     17.291      0.000      0.407      0.511\n      57.000      0.480      0.028     17.217      0.000      0.426      0.535\n      60.333      0.502      0.030     16.676      0.000      0.443      0.561\n      63.667      0.523      0.033     15.880      0.000      0.458      0.587\n      67.000      0.544      0.036     14.995      0.000      0.473      0.615\n      70.333      0.565      0.040     14.124      0.000      0.486      0.643\n      73.667      0.586      0.044     13.315      0.000      0.500      0.672\n      77.000      0.607      0.048     12.584      0.000      0.512      0.702\n      80.333      0.628      0.053     11.935      0.000      0.525      0.731\n      83.667      0.649      0.057     11.360      0.000      0.537      0.761\n      87.000      0.670      0.062     10.853      0.000      0.549      0.792\n\nData for visualizing the conditional effect of the focal predictor:\n     negemot        age     govact\n       1.670     30.000      4.038\n       3.670     30.000      4.657\n       5.330     30.000      5.171\n       1.670     50.000      3.772\n       3.670     50.000      4.644\n       5.330     50.000      5.368\n       1.670     70.000      3.506\n       3.670     70.000      4.631\n       5.330     70.000      5.565\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\nIn this case we have covariates we use as statistical controls. To control for these covariates they are held constant (at their average value). In this example the averages of these variables are: - posemot = 3.132, ideology = 4.083, and gender = 0.488, - and their coefficients (from the first table): posemot = −0.021, ideology = −0.212, and gender = − 0.011 - so, considering coefficients and averages, the coefficients in the table can be read as: −0.021(3.132) −0.212(4.083) − 0.011(0.488).\nAnyway, usually we are not interested in interpreting the covariates, we just want to held them constant. So what is important is to know that they are held constant at their average value. The values plugged into the model for the covariates end up merely adding or subtracting from the regression constant, depending on the signs of the regression coefficients for the covariates. This will have the effect of moving the plot up or down the vertical axis (you can use the sample mean for dichotomous covariates. If a dichotomous variable is coded zero and one, then the sample mean is the proportion of the cases in the group coded one. But using the mean works regardless of how the groups are coded, even if the mean is itself meaningless).\nIn this case the johnson-neyman analysis returns that there are no statistical significance transition points within the observed range of the moderator found using the johnson-neyman method. Indeed, by looking at the next 21 point table, we see that x has a significant effect on y over the entire range of w.\nIt is possible the data in the table data for visualizing the conditional effect of the focal predictor to plot the interaction effect (“wmarker” and “pch” are used to specify the shape of the points). (in a similar way as before, it can be visualized the statistical significance of the moderating effect (in this case is not very useful, since the effect is always significant).\n\nprocess(y = \"govact\", x = \"negemot\", w = \"gender\",\n        cov = c(\"posemot\", \"ideology\", \"age\"),\n        model = 1, \n        jn = 1, \n        plot = 1,\n        wmodval = c(30,50,70),\n        decimals = 10.3,\n        data = glbwarm)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : govact \n    X : negemot\n    W : gender \n\nCovariates: \n       posemot ideology age\n\nSample size: 815\n\n\n*********************************************************************** \nOutcome Variable: govact\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n       0.637      0.406      1.107     92.172      6.000    808.000      0.000\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant      4.507      0.221     20.365      0.000      4.072      4.941\nnegemot       0.315      0.036      8.638      0.000      0.243      0.386\ngender       -0.873      0.190     -4.595      0.000     -1.246     -0.500\nInt_1         0.242      0.049      4.951      0.000      0.146      0.337\nposemot      -0.028      0.028     -0.995      0.320     -0.082      0.027\nideology     -0.211      0.027     -7.889      0.000     -0.263     -0.158\nage          -0.001      0.002     -0.554      0.580     -0.006      0.003\n\nProduct terms key:\nInt_1  :  negemot  x  gender      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W      0.018     24.516      1.000    808.000      0.000\n----------\nFocal predictor: negemot (X)\n      Moderator: gender (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n      gender     effect         se          t          p       LLCI       ULCI\n      30.000      7.561      1.438      5.257      0.000      4.738     10.384\n      50.000     12.391      2.414      5.134      0.000      7.653     17.129\n      70.000     17.222      3.389      5.081      0.000     10.569     23.875\n\nData for visualizing the conditional effect of the focal predictor:\n     negemot     gender     govact\n       1.670     30.000    -10.077\n       3.670     30.000      5.045\n       5.330     30.000     17.595\n       1.670     50.000    -19.475\n       3.670     50.000      5.307\n       5.330     50.000     25.877\n       1.670     70.000    -28.874\n       3.670     70.000      5.570\n       5.330     70.000     34.158\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\n\nx <- c(1.67,3.67,5.33,1.67,3.67,5.33,1.67,3.67,5.33)\nw <- c(30,30,30,50,50,50,70,70,70)\ny <- c(4.038,4.657,5.171,3.772,4.644,5.362,3.506,4.631,5.565)\nwmarker <- c(15,15,15,16,16,16,17,17,17)\n\nplot(y = y, x = x, cex = 1.2, pch = wmarker,\n     xlab = \"negative emotions (w)\",\n     ylab = \"support for government action (y)\")\n\nlegend.txt <- c(\"30 years old\",\"50 years old\", \"70 years old\")\nlegend(\"topleft\", legend = legend.txt, \n       cex = 1, lty = c(1,3,6), lwd = c(2,3,2), \n       pch = c(15,16,17))\n\nlines(x[w==30], y[w==30], lwd = 2, col = \"black\")\nlines(x[w==50], y[w==50], lwd = 3, lty = 3, col = \"black\")\nlines(x[w==70], y[w==70], lwd = 2, lty = 6, col = \"black\")"
  },
  {
    "objectID": "moderation.html#additive-multiple-moderation",
    "href": "moderation.html#additive-multiple-moderation",
    "title": "Moderation",
    "section": "Additive multiple moderation",
    "text": "Additive multiple moderation\nAn additive multiple moderation model uses two moderators to moderate the same relation.\n\n\n\n\n\nTo fit an additive multiple moderation model just add two moderators “w” and “z” and specify model = 2. If you want to manually specify values to probe the interaction, use zmodval or wmodval depending on the moderator you are considering. We omit jn=1 since the johnson-neyman significance region(s) is not provided for this kind of model.\n\nprocess(y = \"govact\", x = \"negemot\", w = \"gender\", z = \"age\",\n        cov = c(\"posemot\", \"ideology\"),\n        model = 2, \n        plot = 1,\n        zmodval = c(30,50,70),\n        decimals = 10.3,\n        data = glbwarm)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 2      \n    Y : govact \n    X : negemot\n    W : gender \n    Z : age    \n\nCovariates: \n       posemot ideology\n\nSample size: 815\n\n\n*********************************************************************** \nOutcome Variable: govact\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n       0.643      0.413      1.096     81.092      7.000    807.000      0.000\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant      5.272      0.336     15.686      0.000      4.612      5.931\nnegemot       0.093      0.082      1.135      0.257     -0.068      0.254\ngender       -0.742      0.194     -3.822      0.000     -1.123     -0.361\nInt_1         0.204      0.050      4.084      0.000      0.106      0.303\nage          -0.018      0.006     -2.997      0.003     -0.030     -0.006\nInt_2         0.005      0.002      3.013      0.003      0.002      0.008\nposemot      -0.023      0.028     -0.849      0.396     -0.078      0.031\nideology     -0.207      0.027     -7.772      0.000     -0.259     -0.155\n\nProduct terms key:\nInt_1  :  negemot  x  gender      \nInt_2  :  negemot  x  age      \n\nTest(s) of highest order unconditional interaction(s):\n        R2-chng          F        df1        df2          p\nX*W       0.012     16.676      1.000    807.000      0.000\nX*Z       0.007      9.080      1.000    807.000      0.003\nBOTH      0.025     16.921      2.000    807.000      0.000\n----------\nFocal predictor: negemot (X)\n      Moderator: gender (W)\n      Moderator: age (Z)\n\nConditional effects of the focal predictor at values of the moderator(s):\n      gender        age     effect         se          t          p       LLCI\n       0.000     30.000      0.236      0.045      5.262      0.000      0.148\n       0.000     50.000      0.331      0.037      9.024      0.000      0.259\n       0.000     70.000      0.426      0.052      8.240      0.000      0.324\n       1.000     30.000      0.440      0.052      8.472      0.000      0.338\n       1.000     50.000      0.535      0.035     15.072      0.000      0.465\n       1.000     70.000      0.630      0.043     14.808      0.000      0.547\n        ULCI\n       0.323\n       0.402\n       0.527\n       0.542\n       0.605\n       0.714\n\nData for visualizing the conditional effect of the focal predictor:\n     negemot     gender        age     govact\n       1.670      0.000     30.000      4.200\n       3.670      0.000     30.000      4.671\n       5.330      0.000     30.000      5.062\n       1.670      0.000     50.000      3.994\n       3.670      0.000     50.000      4.655\n       5.330      0.000     50.000      5.204\n       1.670      0.000     70.000      3.788\n       3.670      0.000     70.000      4.639\n       5.330      0.000     70.000      5.346\n       1.670      1.000     30.000      3.800\n       3.670      1.000     30.000      4.680\n       5.330      1.000     30.000      5.411\n       1.670      1.000     50.000      3.594\n       3.670      1.000     50.000      4.664\n       5.330      1.000     50.000      5.552\n       1.670      1.000     70.000      3.388\n       3.670      1.000     70.000      4.648\n       5.330      1.000     70.000      5.694\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\nTwo moderators means that the output includes two interaction coefficients (int_1 and int_2). In the table product terms key you find the legend to interpret they meaning.\n\nint_1 is xw and quantifies how much the conditional effect of x on y changes as w changes by one unit, holding z constant;\nint_2 is xz and estimates how much the conditional effect of x on y changes as z (age) changes by one unit, holding w constant.\n\nFor instance, considering the interpretation of int_2: as age increases by one year, the conditional effect of negative emotions on support for government action increases by 0.005 points (or alternatively: among two hypothetical groups of people who differ by 1 year in age, the conditional effect of negative emotions on support for government action is 0.005 points larger in the older group).\nAs before, tests of significance for the interaction terms answer the question as to whether w moderates x’s effect and whether z moderates x’s effect, respectively. In this case both are statistically different from zero (p = .000 and p = 0.003), meaning both gender and age function as moderators of the effect of negative emotions on support for government action.\nFrom the test(s) of highest order unconditional interaction(s) we learn that the moderation of the effect of negative emotions by gender (w) uniquely accounts for 1.21% of the variance (p < 0.001), whereas the moderation by age (z) uniquely accounts for 0.66% of the variance (p = 0.003).\nFrom the table conditional effects of the focal predictor at values of the moderator(s) it can be seen that: - the effect of negative emotions on support for government action is consistently positive and statistically significant for both males and females of 30, 50, or 70 years of age. - the effect of negative emotions on support for government action is stronger for men (gender = 1) than women (gender = 0). - more exactly, the difference between males and females is 0.204 (see the coefficient int_1 = 0.204) which is also the difference beween the male and female values reported in the table “conditional effects of the focal predictor at values of the moderator(s)” (0.440 - 0.236 = 0.204; 0.535 - 0.331 = 0.204; 0.630 - 0.426 = 0.204. - notice that this observation highlights an important characteristic of this kind of model: the two moderators exert a combined effect on the x->y relation, but do not interact with each other. regardless of which age you consider, the difference between males and females is always 0.204. This may be a limitation or not, depending on your theoretical framework. This “limitation” is overcome by the mediated moderation models (described in the next section). - also, the effect of negative emotions on support for government action is stronger for older people (int_2 = 0.005). - as said above, int_2 is the coefficient of this interaction and estimates how much the conditional effect of x on y changes as z (age) changes by one unit, holding w constant. Thus, regardless gender, one-unit change in z (i.e., + 1 year) is associated with a +0.005 points in y (support for government). This means there is a linear increase in support for government that depends on age. - for instance: among two hypothetical groups of people who differ by 1 year in age, the conditional effect of negative emotions on support for government action is 0.005 larger in the Older group. For two groups 10 years apart, the difference in the effect is 10*0.0005 = 0.05, and so forth (this difference is invariant to where you start on the distribution of age, a constraint built into this model).\nThe plot of the model clearly shows that the difference between gender does not depend on age: the gap between the lines is always the same (.204) at different ages. At the same time, the visualization makes it clear the positive effect of these moderators (it may be clearer in the plot used in the book at page 328: in particular with reference to the age variable, the lines are “steeper” for older people).\n\npar(mfrow = c(3, 1))\npar(mar = c(3, 4, 0, 0), oma = c(2, 2, 2, 2))\npar(mgp = c(5, 0.5, 0))\nx <- c(1.67, 3.67, 5.33, 1.67, 3.67, 5.33)\nw <- c(0, 0, 0, 1, 1, 1)\nyage30 <- c(4.2003, 4.6714, 5.0624, 3.800, 4.6801, 5.4106)\nyage50 <- c(3.9943, 4.6554, 5.2041, 3.5941, 4.6641, 5.5523)\nyage70 <- c(3.7883, 4.6394, 5.3458, 3.3881, 4.6481, 5.6940)\nlegend.txt <- c(\"female (w=0)\", \"male (w=1)\")\nfor (i in 1:3) {\n  if (i == 1)\n  {\n    y <- yage30\n    legend2.txt <- c(\"age (z) = 30\")\n  }\n  if (i == 2)\n  {\n    y <- yage50\n    legend2.txt <- c(\"age (z) = 50\")\n  }\n  if (i == 3)\n  {\n    y <- yage70\n    legend2.txt <- c(\"age (z) = 70\")\n  }\n  plot(\n    y = y,\n    x = x,\n    col = \"white\",\n    ylim = c(3, 6),\n    cex = 1.5,\n    xlim = c(1, 6),\n    tcl = -0.5\n  )\n  lines(x[w == 0], y[w == 0], lwd = 2, lty = 2)\n  lines(x[w == 1], y[w == 1], lwd = 2, lty = 1)\n  legend(\"topleft\",\n         legend = legend.txt,\n         lwd = 2,\n         lty = c(2, 1))\n  legend(\"bottomright\", legend = legend2.txt)\n}\nmtext(\"negative emotions (x)\", side = 1, outer = TRUE)\nmtext(\"support for government action\",\n      side = 2,\n      outer = TRUE)\n\n\n\n\n–"
  },
  {
    "objectID": "moderation.html#moderated-moderation",
    "href": "moderation.html#moderated-moderation",
    "title": "Moderation",
    "section": "Moderated moderation",
    "text": "Moderated moderation\nIn moderated moderation models there are two moderators w and z that interact. It is also known as three-way interaction (it means that x, w, and z interact).\nThe characteristic of this model is that it allows the moderation of x’s effect on y by w to be dependent on z.\n\n\n\n\n\nWe fit this model using the glbwarm dataset. We measure the relation between negative emotion (x) and support for government (y), conditionally to two interacting moderators: gender (w) and age (z). In other terms, this model allows us to understand if the relation between negative emotion (x) and support for government (y) changes (in sign or strenght) between males and females (w) at different ages (z).\nModerated moderation models are fitted in process by specifying model = 3. In this case “w” is the primary moderator, and “z” the secondary moderator (the moderator that moderates w). In this model we can also calculate the johnson-neyman significance region(s) by adding jn = 1.\n\nprocess(y = \"govact\", x = \"negemot\", w = \"gender\", z = \"age\",\n        cov = c(\"posemot\", \"ideology\"),\n        model = 3, \n        jn = 1, \n        plot = 1,\n        zmodval = c(30,50,70),\n        decimals = 10.3,\n        data = glbwarm)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 3      \n    Y : govact \n    X : negemot\n    W : gender \n    Z : age    \n\nCovariates: \n       posemot ideology\n\nSample size: 815\n\n\n*********************************************************************** \nOutcome Variable: govact\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n       0.645      0.416      1.093     63.765      9.000    805.000      0.000\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant      4.559      0.485      9.401      0.000      3.607      5.512\nnegemot       0.273      0.118      2.311      0.021      0.041      0.505\ngender        0.529      0.646      0.819      0.413     -0.740      1.798\nInt_1        -0.131      0.167     -0.781      0.435     -0.460      0.198\nage          -0.003      0.009     -0.356      0.722     -0.022      0.015\nInt_2         0.001      0.002      0.381      0.704     -0.004      0.006\nInt_3        -0.025      0.012     -2.059      0.040     -0.049     -0.001\nInt_4         0.007      0.003      2.096      0.036      0.000      0.013\nposemot      -0.021      0.028     -0.745      0.456     -0.075      0.034\nideology     -0.205      0.027     -7.726      0.000     -0.258     -0.153\n\nProduct terms key:\nInt_1  :  negemot  x  gender      \nInt_2  :  negemot  x  age      \nInt_3  :  gender  x  age      \nInt_4  :  negemot  x  gender  x  age\n\nTest(s) of highest order unconditional interaction(s):\n         R2-chng          F        df1        df2          p\nX*W*Z      0.003      4.393      1.000    805.000      0.036\n----------\nFocal predictor: negemot (X)\n      Moderator: gender (W)\n      Moderator: age (Z)\n\nTest of conditional X*W interaction at value(s) of Z:\n         age     effect          F        df1        df2          p\n      30.000      0.070      0.730      1.000    805.000      0.393\n      50.000      0.203     16.505      1.000    805.000      0.000\n      70.000      0.337     17.462      1.000    805.000      0.000\n\nConditional effects of the focal predictor at values of the moderator(s):\n      gender        age     effect         se          t          p       LLCI\n       0.000     30.000      0.300      0.054      5.545      0.000      0.194\n       0.000     50.000      0.319      0.037      8.600      0.000      0.246\n       0.000     70.000      0.337      0.067      5.060      0.000      0.206\n       1.000     30.000      0.370      0.062      5.968      0.000      0.248\n       1.000     50.000      0.522      0.036     14.488      0.000      0.451\n       1.000     70.000      0.674      0.048     14.180      0.000      0.580\n        ULCI\n       0.407\n       0.391\n       0.468\n       0.492\n       0.592\n       0.767\n\nModerator value(s) defining Johnson-Neyman significance region(s):\n       Value    % below    % above\n      38.114     28.221     71.779\n\nConditional X*W interaction at values of the moderator Z:\n         age     effect         se          t          p       LLCI       ULCI\n      17.000     -0.017      0.117     -0.148      0.883     -0.247      0.212\n      20.500      0.006      0.107      0.057      0.954     -0.204      0.216\n      24.000      0.030      0.097      0.303      0.762     -0.161      0.220\n      27.500      0.053      0.088      0.602      0.547     -0.120      0.225\n      31.000      0.076      0.079      0.966      0.334     -0.079      0.231\n      34.500      0.100      0.071      1.410      0.159     -0.039      0.238\n      38.000      0.123      0.063      1.944      0.052     -0.001      0.247\n      38.114      0.124      0.063      1.963      0.050     -0.000      0.248\n      41.500      0.146      0.057      2.563      0.011      0.034      0.258\n      45.000      0.170      0.053      3.225      0.001      0.066      0.273\n      48.500      0.193      0.050      3.841      0.000      0.094      0.292\n      52.000      0.216      0.050      4.301      0.000      0.118      0.315\n      55.500      0.240      0.053      4.542      0.000      0.136      0.344\n      59.000      0.263      0.057      4.588      0.000      0.151      0.376\n      62.500      0.287      0.064      4.507      0.000      0.162      0.411\n      66.000      0.310      0.071      4.365      0.000      0.171      0.449\n      69.500      0.333      0.079      4.202      0.000      0.178      0.489\n      73.000      0.357      0.088      4.041      0.000      0.183      0.530\n      76.500      0.380      0.098      3.892      0.000      0.188      0.572\n      80.000      0.403      0.107      3.757      0.000      0.193      0.614\n      83.500      0.427      0.117      3.637      0.000      0.196      0.657\n      87.000      0.450      0.128      3.530      0.000      0.200      0.701\n\nData for visualizing the conditional effect of the focal predictor:\n     negemot     gender        age     govact\n       1.670      0.000     30.000      4.056\n       3.670      0.000     30.000      4.657\n       5.330      0.000     30.000      5.155\n       1.670      0.000     50.000      4.019\n       3.670      0.000     50.000      4.656\n       5.330      0.000     50.000      5.185\n       1.670      0.000     70.000      3.982\n       3.670      0.000     70.000      4.656\n       5.330      0.000     70.000      5.215\n       1.670      1.000     30.000      3.942\n       3.670      1.000     30.000      4.682\n       5.330      1.000     30.000      5.296\n       1.670      1.000     50.000      3.622\n       3.670      1.000     50.000      4.665\n       5.330      1.000     50.000      5.531\n       1.670      1.000     70.000      3.302\n       3.670      1.000     70.000      4.649\n       5.330      1.000     70.000      5.767\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\nThe output of the model includes single variable coefficients and four interaction coefficients that can be interpreted with references to the legend “product terms key”.\nIn the book you can find a detailed description of the equation for this model (and also for all the other models) which is just the written form of the above diagram:\n\\[\\hat y = i_y + b_1x + b_2w + b_3z + b_4xw + b_5xz + b_6wz + b_7xwz\\]\nBased on the general equation, we can plug the model coefficients into the equation, keeping in mind that our model has the following varaibles:\n\nx = “negemot”\nw = “gender”\nz = “age”\n\n\\[\\hat y = 4.559 + 0.273x + 0.529w + -0.003z -0.131xw + 0.001xz -0.025wz + 0.007xwz\\]\nAlso in this case the coefficients estimate conditional effects, that is, the effect of the variable when the other ones are set to zero (and not when they are held constant, as in a generic multiple regression model):\n\nb1 estimates the effect of x on y when both w and z are zero (indeed you can see that in this particular condition all the coefficients other than b1 and the intercept are deleted from the equation, since they become zero);\nb2 estimates the effect of w on y when both x and z are equal to zero (for the same reason above);\nand b3 estimates the effect of z on y when both x and w are equal to zero (for the same reason);\nb4 estimates the conditional interaction between x and w when z = 0 (as in the previous model);\nb5 quantifies the conditional interaction between x and z when w = 0;\nand b6 estimates the conditional interaction betweenw and z when x = 0.\n\nSince this is a model used to test moderated moderation, the significance test (table test(s) of highest order unconditional interaction(s) is about the moderated moderation term xwz (int_4). In this case it is significant (p = 0.036), and explains 0.3% of the variance in support for government action.\nThe table **test of conditional x*w interaction at value(s) of z** shows at which values of z (age, secondary moderator) the interaction between x (negative emotion) and w (gender, primary moderator) has a significant effect on y:\n\nin this case, at a “relatively low” age (age = 30), the relation between negative emotion and support for government is not moderated by gender (i.e.: when considering the relation between x and y at this age, there is no significant difference between males and females: effect: 0.070; p-value = 0.393).\namong 50- and 70-year olds, the relation between x and y is significantly moderated by gender (i.e., there are significant differences between males and females at these ages, 0.203 and 0.337 respectively, p-value = 0.000).\n\nAlso notice from the table below (conditional effects of the focal predictor at values of the moderator(s)), that these estimates of the conditional xw interaction (i.e., 0.070, 0.203, and 0.337) are just the difference between the effects in males and females (gender w, primary moderator) conditionally to the three choosen ages (age z, secondary moderator):\n\n0.070 = 0.370 - 0.300\n0.203 = 0.522 - 0.319\n0.337 = 0.674 - 0.337\n\nThis is also clear in the plot (based on the data in the output): the effect of negative emotions on support for government action is consistently positive (as negative emotions increase, the support for government also increases), but there is a larger difference between men and women among those who are older (as shown by the gap between the lines).\nNotice the difference with the previous additive multiple moderator model (and plot): in that case, the gap between males and females was always the same. Here, it changes conditionally on the age (the secondary moderator).\n\npar(mfrow = c(3, 1))\npar(mar = c(3, 4, 0, 0), oma = c(2, 2, 2, 2))\npar(mgp = c(5, 0.5, 0))\nx <- c(1.67, 3.67, 5.33, 1.67, 3.67, 5.33)\nw <- c(0, 0, 0, 1, 1, 1)\nyage30 <- c(4.0561, 4.6566, 5.1551, 3.9422, 4.6819, 5.2959)\nyage50 <- c(4.0190, 4.6562, 5.1850, 3.6220, 4.6654, 5.5314)\nyage70 <- c(3.9820, 4.6557, 5.2149, 3.3017, 4.6489, 5.7670)\nlegend.txt <- c(\"female (w=0)\", \"male (w=1)\")\nfor (i in 1:3) {\n  if (i == 1)\n  {\n    y <- yage30\n    legend2.txt <- c(\"age (z) = 30\")\n  }\n  if (i == 2)\n  {\n    y <- yage50\n    legend2.txt <- c(\"age (z) = 50\")\n  }\n  if (i == 3)\n  {\n    y <- yage70\n    legend2.txt <- c(\"age (z) = 70\")\n  }\n  \n  plot(\n    y = y,\n    x = x,\n    col = \"white\",\n    ylim = c(3, 6),\n    cex = 1.5,\n    xlim = c(1, 6),\n    tcl = -0.5\n  )\n  \n  lines(x[w == 0], y[w == 0], lwd = 2, lty = 2)\n  lines(x[w == 1], y[w == 1], lwd = 2, lty = 1)\n  legend(\"topleft\",\n         legend = legend.txt,\n         lwd = 2,\n         lty = c(2, 1))\n  legend(\"bottomright\", legend = legend2.txt)\n  \n}\nmtext(\"negative emotions (x)\", side = 1, outer = TRUE)\nmtext(\"support for government action\",\n      side = 2,\n      outer = TRUE)\n\n\n\n\nThe johnson-neyman tecnique (moderator value(s) defining johnson-neyman significance region(s)) shows that there is a statistically significant difference between men and women in the effect of negative emotions on support for goverment action among those at least 38.114 years of age. Below this age, gender does not moderate the effect of negative emotions on support for government action. As in the previous models, it is possible to plot the johnson-neyman significance region(s):\n\npar(mfrow=c(1,1))\nage <- c(17,20.5,24,27.5,31,34.5,38,38.11,41.5,45,48.5,\n         52,55.5,59,62.5,66, 69.5,73,76.5,80,83.5,87)\neffect <- c(-.017,.006,.030,.053,.076,.100,.123,.124,.146,\n          .170,.193,.217,.240,.263,.287,.310,.333,.357,\n          .380,.404,.427,.450)\nllci <- c(-.247,-.204,-.161,-.120,-.078,-.039,-.001,0,\n          .034,.066,.094,.118,.136,.151,.162,.171,.178,\n          .184,.188,.193,.197,.200)\nulci <- c(.212,.216,.220,.225,.231,.238,.247,.248,.259,\n          .273,.292,.315,.344,.376,.411,.449,.489,.530,\n          .572,.614,.657,.701)\n\nplot(age, effect, type = \"l\", pch = 19, ylim = c(-1,1.5), xlim = c(15,90), lwd = 3,\n     ylab = \"conditional negative emotions by sex interaction\",\n     xlab = \"age\")\npoints(age, llci, lwd = 2, lty = 2, type = \"l\")\npoints(age, ulci, lwd = 2, lty = 2, type = \"l\")\nabline(h = 0, untf = FALSE, lty = 3, lwd = 1)\nabline(v = 38.114, untf = FALSE, lty = 3, lwd = 1)\ntext(38.114, -1, \"38.114\", cex = 0.8)\n\n\n\n\n–"
  },
  {
    "objectID": "moderation.html#learning-objectives-2",
    "href": "moderation.html#learning-objectives-2",
    "title": "Moderation",
    "section": "Learning objectives",
    "text": "Learning objectives\nLearning objectives of unit 3b consist in learning to fit and interpret:\n\nmodels with multicategorical antecedent variable.\nmodels with multicategorical moderator variable."
  },
  {
    "objectID": "moderation.html#moderation-with-multicategorical-antecendent-x-variable",
    "href": "moderation.html#moderation-with-multicategorical-antecendent-x-variable",
    "title": "Moderation",
    "section": "Moderation with multicategorical antecendent (x) variable",
    "text": "Moderation with multicategorical antecendent (x) variable\nTo fit a moderation model with a multicategorical antecendent variable (x) we use a process function including the options model = 1 and mcx = 1, where mcx means (as in mediation models) multi-categorical x. This option is necessary to analyze x as a multicategorical variable.\nAs in other models previously considered, a multicategorical antecedent variable with g number of groups is authomatically splitted into g-1 x variables, each indicating a specific modality of the varaible, and 1 reference group. For instance, in the case of the “protest” dataset (which we are going to use to exempify this model), there are three conditions: no protest, individual protest, collective protest, respectively coded in the “protest” variable as protest=0, protest=1, and protest=2. The software authomatically subdivides this multicategorical variable (g=3) into 2 (g-1 = 3-1 = 2) groups. The missing group is used as reference group. The reference is the group with the lowest value: protest = 0.\nDifferently from a mediation model with a multicategorical x, we now consider the interaction between each modality (i.e., group) of x, and the moderator variable w.\n\n\n\n\n\nIn the model we are fitting, x is the experimental “multicategorical” condition (protest), and y is how much catherine was liked. The moderator variable w is the “perceived pervasiveness of gender discrimination in society”.\n\nprotest <- haven::read_sav(\"data/protest.sav\")\n\nprocess(y = \"liking\", x = \"protest\", w = \"sexism\", \n        mcx = 1,\n        model = 1, \n        plot = 1,\n        decimals = 10.2,\n        data = protest)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : liking \n    X : protest\n    W : sexism \n\nSample size: 129\n\nCoding of categorical X variable for analysis: \n     protest         X1         X2\n        0.00       0.00       0.00\n        1.00       1.00       0.00\n        2.00       0.00       1.00\n\n*********************************************************************** \nOutcome Variable: liking\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n        0.37       0.13       0.99       3.84       5.00     123.00       0.00\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant       7.71       1.05       7.32       0.00       5.62       9.79\nX1            -4.13       1.50      -2.76       0.01      -7.09      -1.16\nX2            -3.49       1.41      -2.48       0.01      -6.28      -0.70\nsexism        -0.47       0.21      -2.30       0.02      -0.88      -0.07\nInt_1          0.90       0.29       3.13       0.00       0.33       1.47\nInt_2          0.78       0.28       2.83       0.01       0.23       1.32\n\nProduct terms key:\nInt_1  :  X1  x  sexism      \nInt_2  :  X2  x  sexism      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W       0.08       5.85       2.00     123.00       0.00\n----------\nFocal predictor: protest (X)\n      Moderator: sexism (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n\nModerator value(s):\n                 \nsexism       4.25\n\n       effect         se          t          p       LLCI       ULCI\nX1      -0.30       0.34      -0.88       0.38      -0.97       0.37\nX2      -0.19       0.31      -0.60       0.55      -0.80       0.43\n\nTest of equality of conditional means\n           F        df1        df2          p\n        0.40       2.00     123.00       0.67\n\nEstimated conditional means being compared:\n     protest     liking\n        0.00       5.70\n        1.00       5.40\n        2.00       5.51\n----------\nModerator value(s):\n                 \nsexism       5.12\n\n       effect         se          t          p       LLCI       ULCI\nX1       0.49       0.22       2.22       0.03       0.05       0.92\nX2       0.49       0.22       2.28       0.02       0.06       0.92\n\nTest of equality of conditional means\n           F        df1        df2          p\n        3.34       2.00     123.00       0.04\n\nEstimated conditional means being compared:\n     protest     liking\n        0.00       5.29\n        1.00       5.77\n        2.00       5.78\n----------\nModerator value(s):\n                 \nsexism       5.90\n\n       effect         se          t          p       LLCI       ULCI\nX1       1.18       0.31       3.88       0.00       0.58       1.79\nX2       1.10       0.32       3.47       0.00       0.47       1.72\n\nTest of equality of conditional means\n           F        df1        df2          p\n        8.82       2.00     123.00       0.00\n\nEstimated conditional means being compared:\n     protest     liking\n        0.00       4.92\n        1.00       6.11\n        2.00       6.02\n\nData for visualizing the conditional effect of the focal predictor:\n     protest     sexism     liking\n        0.00       4.25       5.70\n        1.00       4.25       5.40\n        2.00       4.25       5.51\n        0.00       5.12       5.29\n        1.00       5.12       5.77\n        2.00       5.12       5.78\n        0.00       5.90       4.92\n        1.00       5.90       6.11\n        2.00       5.90       6.02\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nW values in conditional tables are the 16th, 50th, and 84th percentiles.\n\n\nThe output includes several sections:\n\nthe general statistics on the model\na legend with coding of categorical x variable\nthe regression coefficients including the interaction terms (int_1, int_2, etc.)\nresults from the test of interaction(s)\n\nAs always, the first part of the output includes general statistics on the model (e.g.: the overall r squared of the model is 0.1349 and statistically significant, so we can say it works, and it explains 13.5% of y variability).\nAs in other multicategorical models, there is a legend with coding of categorical x variable for analysis. It can be see that protest = 0 is the reference group, protest = 1 is indicated by x1, and protest = 2 by x2.\nNext, there are the regression coefficients. These values can also be plugged into an equation to represent the model:\n\\[\\hat y = i_y + b_1x_1 + b_2x_2 + b_3w + b_4x_1w + b_5x_2w\\] \\[\\hat y = 7.71 - 4.13x_1 - 3.49x_2 - 0.47w + 0.90x_1w + 0.78x_2w\\]\nIn this case we find the two x modalities corresponding to the groups of people assigned to the experimental conditions coded as 1 (individual protest) and 2 (collective protest). As this is a multicategorical variable, we have to interpret them with the reference to the reference group (no protest). As in other regression models with multicategorical x (e.g., mediation models), these coefficients represent differences between the indicated group (e.g., x1, x2, …) And the reference group. For instance:\n\nx1 quantifies the average (in regression models we generally discuss averages if not otherwise specified) difference in y (how catherine is liked) between those told she individually protested (protest = 1) and those told she didn’t protest (protest = 0, reference group) among people who score zero on w.\nx2 quantifies this difference in y (how catherine is liked) between those told she collectively protested (protest = 2) and those told she did not protest the discrimination (protest = 0, reference group) among people who score zero on w.\n\nThis may be clearer with reference to the above equation, if you remember from basic algebra courses that when you multiply a value by zero, the result is zero, and that:\n\nWhen a modality of the multicategorical variable is “on”, the other(s) is(are) authomatically “off”. Or in other words, you interpret just one modality at a time. For instance, when you interpret the coefficient x1, you just ignore the coefficient x2, beacuse it is zero by defintion (see again the legend in the top part of the output). Therefore, when reading the x1 coefficient you just cancel the x2 coefficient as well as the interaction between x2 and w because they are zero by definition, and the above equation becomes simplier:\n\n\\[\\hat y = 7.71 - 4.13x_1 - 0.47w + 0.90x_1w\\]\n\nAlthough simplier, the x1 coefficient cannot be directly interpreted because there still are several other terms, in particular w and the interaction between w and x. We cannot just say that the difference in the estimated y between the groups in the experimental condition protest=1 and protest=0 is 4.13 because there are this other terms. But when w=0, then both w and the interaction between x1 and w become zero, so we have the proper interpretation of the coefficient (we can ignore the intercept, since it is a constant that is always the same, so it does not influence the interpretation). When w=0, the equation becomes as follows, thus we can see that this coefficient is the difference between how much catherine is liked in the group protest=1 and protest=0 by people that measure 0 on the moderator variable w (perceived pervasiveness of gender discrimination in society):\n\n\\[\\hat y = 7.71 - 4.13x_1\\]\nAlso, a positive estimate (the coefficient) reflects a more positive evaluation of catherine among those told she individually (or collectively) protested, whereas a negative estimate means that she was liked more when she did not protest compared to when she individually (or collectively) protested.\nunfourtunately, in this case neither of these coefficients is substantively meaningful because we said that their interpretation is conditional to w=0, but the variable w is bound between 1 and 7, and even if zero were on the measurement scale, the smallest value observed in the data is 2.37. So b1 and b2 are not substantively interpretable, and their hypothesis tests and confidence intervals are meaningless.\nTo make these coefficients interpretable, we should transform w so as the zero becomes meaningful. We previously learned that we can do that by mean-centering the variable. With this type of model in process this is not necessary because, in the second part of the output, these coefficients can be interpreted with reference to relatively low, moderate, and relatively high levels of w. This will be explained in a bit, after having taken into account the interpretation of the interaction (the moderator effect).\nLet’s start with the interaction (i.e., moderation) coefficients “int_1” and “int_2”. They are the coefficients indicated in the equation as b4 and b5, and represent the average difference in the estimate of y between groups when w changes by one unit. In this case:\n\nint_1 is the interaction between x=1 (protest=1) and w and is equal 0.90. We already know that x1 measures the difference in the estimate of y (how much catherine is liked) between people in the protest=1 and protest=0 condition groups. So int_1 = 0.90 (0.90x1w in the equation), means that this difference increase by 0.90 units for each unit increase in perceived pervasiveness of gender discrimination (w).\nint_2 is the interaction between x=2 (protest=2) and w and is equal 0.78. This is how much the appreciation of catherine (y) changes for a 1-unit change in w, between people assigned to the collective protest condition (protest=2) compared to people in the no-protest condition (protest=0).\n\nIn more general terms, that means that the stronger the belief in the existence of a problem of gender discrimination in society (w), the more catherine is appreaciated when she protests (both individually or collectively) against discrimination, compared to when she does not protest.\nTo properly interprete the coefficients, it is also important to notice that these two interaction terms are both statistically significant, which support the idea that there is an interaction between the variables.\nWe can also see that the interaction between x and w is statistically significant in the table test(s) of highest order unconditional interaction(s) (p = 0.00). Also, we can see that this interaction help to explain an additional 8% of variablity in y, compared to a model where this interaction is not included. This is a general test to ascertain that the interaction between x and w is significant.\nFinally, there is the coefficient for “sexism” (-0.47, p = 0.02), which is our moderator variable w. It estimates the relationship between w and y in the reference group. We can say that among those told she did not protest, two people who differ by one unit in their perceived pervasiveness of gender discrimination (w) are estimated to differ by -0.47 units in how much they like catherine. In other terms, the more they believe there is a problem of gender discimination, the less (negative sign in the coefficient) they appreaciate the fact that catherine did not protest at all. Also this term is statistically significant.\nUsing the data in the last table, it is possible to visualize the model, which makes it immediatly clear the relation of moderation. What is interesting with respect to the interpretation of the plot is the gap between the lines and how it varies as a function of the moderator. The gap represents the difference between the conditions (x) in how much catherine is liked by people who are the same in their beliefs about sex discrimination (w).\nIn this example, the values used by process are the 16th, 50th, and 84th percentiles (i.e., the default approach) of the distribution of sexism in the sample, but these can be changed using the moments=1 or wmodval options (already discussed).\n(tip: to learn more about the plot codes, the functions and parameters, search the functions in the “help” and read the documentation, and/or google them. Also, you can experiment with the code, by changing the parameter, seeing what changes. Also, check the tutorial on ggplot2 visualization of moderation effects to learn an alternative way to create these plots).\n\nx <- c(0,1,2,0,1,2,0,1,2)\nw <- c(4.25,4.25,4.25,5.12,5.12,5.12,5.896,5.896,5.896)\ny <- c(5.698,5.400,5.513,5.287,5.773,5.779,4.920,6.105,6.016)\n\nplot(y = y, x = w, \n     cex = 1.2, pch = 16,\n     col=\"black\",\n     xlab=\"perceived pervasiveness of gender discrimination (w)\",\n     ylab=\"liking of the attorney (y)\")\n\nlegend.txt <- c(\"no protest\", \"individual protest\", \"collective protest\")\nlegend(\"topleft\", legend = legend.txt, \n       cex = 1, lty = c(1,2,1), lwd = c(4,2,1),\n       pch = c(16, 16, 16))\n\nlines(w[x==0], y[x==0], lwd=4, lty=1)\nlines(w[x==1], y[x==1], lwd=2, lty=2)\nlines(w[x==2], y[x==2], lwd=1, lty=1)\n\nabline(v = 4.25, untf = FALSE, lty = 3, lwd = 1)\nabline(v = 5.12, untf = FALSE, lty = 3, lwd = 1)\nabline(v = 5.896, untf = FALSE, lty = 3, lwd = 1)\n\ntext(4.25+0.03, 5, \"w=4.25\", cex = 0.8, srt = 90) # 0.03 is added to avoid overlap between the label and the vertical dotted line \ntext(5.12+0.03, 5, \"w=5.12\", cex = 0.8, srt = 90)\ntext(5.896+0.03, 5, \"w=5.896\", cex = 0.8, srt = 90)\n\n\n\n\nWhen it comes to probe the interaction (i.e., testing the level and statistical significance of the interaction at different values of the moderator), although the the application of johnson–neyman technique would be possible, process implements it only when x is dichotomous or continuous, but not when it is multicategorical (there is specific macro for spss and sas, but at the moment i am not aware of something similar for r). What process implements is the pick-a-point approach, based on specific values of the moderators. By default, process uses the 16th, 50th, and 84th percentiles of the distribution of w in the data available to operationalize relatively low, moderate, and relatively high values of the moderator (but it is possible to change the choosen values of w using the moments=1 or wmodval options).\nThe results from these tecnique is in the second part of the output. It can be seen that, in this case, the relatively low (16th percentile) value of the moderator w (sexism) is 4.25, the moderate (50th percentile value is 5.12, and relatively high (84th percentile) is 5.90. Each of these values has its own tests. In particular we have two kinds of test:\n\nAn omnibus test that tests, in general, if there are statistically significant differences between any group in the estimate of y, at that value of the moderator (table named test of equality of conditional means). This is a general test like that of anova, which allows you to say there are differences between groups, but without knowing specific details, like which are the groups that are different, and how much they differ.\nA pairwise test that tests, more specifically, the statistical difference between each group and the reference one (i.e., the difference in the estimate of y between group x1 and the reference group x0, at a certain level of the moderator). This can be seen in the table following the value of the moderator that is taken into consideration (e.g., sexism 4.25).\n\nFinally, the table estimated conditional means being compared reports the estimates of y for the groups at that level of the moderator.\nFor instance, we can interpret these table by saying that:\n\nat low values of the moderator (16th percentile: 4.25) there is no difference between the groups (both the omnibus and the paiwise test are not significant).\nat moderate values of the moderator (50th percentile: 5.12) the differences are statistically significant. We can also see that the size of the difference is almost the same in x1 (protest=1) and x2 (protest=2). In both cases the effect is 0.49. You can see that this is the difference between the reported estimates of y of the groups at that level of the moderator, which are reported in the table estimated conditional means being compared (the results are rounded to the second decimal place so there is some imprecision, but these differences are: 5.77 - 5.29 = 0.48 ; and 5.78 - 5.29 = 0.49).\nat relatively high values of the moderator (84th percentile: 5.12) the differences are statistically significant, and the effect is higher than at moderate values of the moderator. This is also what can be seen from the plot."
  },
  {
    "objectID": "moderation.html#moderation-with-multicategorical-moderator",
    "href": "moderation.html#moderation-with-multicategorical-moderator",
    "title": "Moderation",
    "section": "Moderation with multicategorical moderator",
    "text": "Moderation with multicategorical moderator\nIn the last part dedicated to moderation analysis, we consider the case of a model with a continuous or dichotomous focal antecedent variable x and a multicategorical moderator w.\nInstead of using the multicategorical variable protest as x, in this model we use it as a moderator, while we use sexism as x. In other words, we are going to test the relationship between perceived pervasiveness of gender discrimination (sexism = x) and how positively catherine is evaluated (liking = y) in relation to the possible influence of wether and how she protests (protest = w).\nThe code is the same as before, but instead of using mcx=1 (where mcx means multi-categorical x), we use mcw=1 (multi-categorical w), since now the multicategorical variable is w.\n\n\n\n\n\n\nprocess(y = \"liking\", x = \"sexism\", w = \"protest\", \n        mcw = 1,\n        model = 1, \n        plot = 1,\n        decimals = 10.2,\n        data = protest)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : liking \n    X : sexism \n    W : protest\n\nSample size: 129\n\nCoding of categorical W variable for analysis: \n     protest         W1         W2\n        0.00       0.00       0.00\n        1.00       1.00       0.00\n        2.00       0.00       1.00\n\n*********************************************************************** \nOutcome Variable: liking\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n        0.37       0.13       0.99       3.84       5.00     123.00       0.00\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant       7.71       1.05       7.32       0.00       5.62       9.79\nsexism        -0.47       0.21      -2.30       0.02      -0.88      -0.07\nW1            -4.13       1.50      -2.76       0.01      -7.09      -1.16\nW2            -3.49       1.41      -2.48       0.01      -6.28      -0.70\nInt_1          0.90       0.29       3.13       0.00       0.33       1.47\nInt_2          0.78       0.28       2.83       0.01       0.23       1.32\n\nProduct terms key:\nInt_1  :  sexism  x  W1      \nInt_2  :  sexism  x  W2      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W       0.08       5.85       2.00     123.00       0.00\n----------\nFocal predictor: sexism (X)\n      Moderator: protest (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n     protest     effect         se          t          p       LLCI       ULCI\n        0.00      -0.47       0.21      -2.30       0.02      -0.88      -0.07\n        1.00       0.43       0.20       2.13       0.04       0.03       0.83\n        2.00       0.31       0.18       1.67       0.10      -0.06       0.67\n\nData for visualizing the conditional effect of the focal predictor:\n      sexism    protest     liking\n        4.25       0.00       5.70\n        5.12       0.00       5.29\n        5.90       0.00       4.92\n        4.25       1.00       5.40\n        5.12       1.00       5.77\n        5.90       1.00       6.11\n        4.25       2.00       5.51\n        5.12       2.00       5.78\n        5.90       2.00       6.02\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\nThe first thing we can see is that the first section of the output is the same as before: the general statistics of the model, the coefficients, and the test of significance of the interaction are unchanged. Indeed, from the mathematical standpoint of this model, it doesn matter if you change the order of w and x (i.e., using two variables a and b as x and w or w and x).\nLet’s consider the coefficients of the model. Let’s try to write the equation. It is the same as before, but x is now w and w is now x:\n\\[\\hat y = i_y + b_1w_1 + b_2w_2 + b_3x + b_4w_1x + b_5w_2x\\] \\[\\hat y = 7.71 - 4.13w_1 - 3.49w_2 - 0.47x + 0.90x_1w + 0.78x_2w\\]\nConsider the coefficient b3 for sexism (x): it is -0.47 and statistically significant. It quantifies the relationship between x and y when w1 and w2 both equal zero (and therefore also the interaction terms, since they are the product of x by w, that is the product of a number by zero, which is zero). That is to say, the coefficient “sexism” (x) quantifies the relation between x and y in the reference group (protest(w) = 0). This can be seen in the table conditional effects of the focal predictor at values of the moderator(s).\nThe substantive interpretation is as follows: among those told the attorney (catherine) did not protest (protest = 0), the relationship between perceived pervasiveness of gender discrimination (x) and how positively catherine is evaluated (y) is negative and statistically significant. In more precise terms, the group of people told she did not protest, like catherine less and less as they perceive more gender discrimination in society (0.47 less for each one-point increase in the perception of gender issues in society).\nWe can directly refer to the table conditional effects of the focal predictor at values of the moderator(s) also to interpret the other interaction coefficients (int_1 and int_2), which are what we are mostly interested in when fitting a moderation model. Here you can see that the relation between x (perceived pervasiveness of gender bias in society) and y (appreciation for catherine) is negative in the protest=0 condition, and positive in the protest=1 and protest=2 condition.\nWe can interpret these coefficients as the slope of the linear relation between x and y: for each 1-point increase in x, the estimate of y decreases by 0.47 points in the protest=0 group, while increases by 0.43 and 0.31 points in the protest=1 and protest=2 respectively. In substantive terms, people told catherine did not protest, express a bad opinion of her, which become worse as they perceive gender bias as a important problem in society. Instead, people told she protested, either by using an individual or collective frame, like her more and more as their they perceive more and more problem related to gender bias in society. Also notice that this effect is not significant for the protest=2 group (p = 0.10).\nThe same information, in a more convolute way, is also included in the first table reporting the interaction coefficients int_1 and int_2 (b4 and b5 in the equation). It can be observed that 0.43 is equal the “sexism” coefficient -0.47 plus the int_1 coefficient 0.90 (-0.47 + 0.90 = 0.43), and that 0.31 is the “sexism” coefficient -0.47 plus the int_2 coefficient 0.78 (-0.47 + 0.78 = 0.31).\nTherefore, in a mediation model with multicategorical moderator, we can directly look at the table conditional effects of the focal predictor at values of the moderator(s) to ascertain and test the significance of the effect of the different levels of the moderator on the relation betweem x and y. However, the coefficients in the first table give us another useful information. They quantify the difference in the relation between x and y, between the reference group (w = 0) and the indicated category of the moderator (i.e., w = 1 for int_1, and w = 2 for int_2). Therefore, they inform us about the size (the value of the coefficient) and the significance (the p-value) of the difference in the estimated relation between x and y between the categories of the moderator. For instance, althoug we have seen, in the table conditional effects of the focal predictor at values of the moderator(s), that in absolute term the relation between x and y is not significantly affected by the collective frame (p = 0.10), we can also see that there is in any case a significant difference between this condition and the reference one (p = 0.01).\nThe interpretation of the significance of the difference depends on the reference group. Therefore we can test the difference between w=0, on the one hand, and w=1 or w=2, on the other hand, but not the difference between w=1 and w=2. To do that, we need to change the reference group (in this case using w=1 as reference) and fit another model. process consider as reference group the one with the lowest value (in this case zero). So the conding of the w variable should be changed accordingly (w=1 should become w=0).\nTo conlude, the table conditional effects of the focal predictor at values of the moderator(s) is dedicated to probe the interaction, because it is about the relation between x and y at different levels of the moderator. Since we have a multicategorical moderator, this means that the relation between x and y is tested in each group of the groups that consitute the categorical moderator (in this case, protest = 0, 1 and 2). Since we have a categorical moderator, the johnson-neyman technique to calculate the significance region(s) is off the table, as it applies only to continuous moderators.\nAlso the chart is about the same of the previous model, with just a few changes. First, the data should be replaced with the new values. Next, the labels have to be changed, since “perceived pervasiveness of gender discrimination” is now the x. Also, we can remove the dots we used to visualize the values choosen for probing the interaction (the paraemeter pch in the functions), as well as the dotted lines (functions abline) and the labels put in the plot (function text) we used for the same reason. By comparing the code with the previous one you can check all the differences.\nHowever, the interpretation is a bit different. In the previous case (multicategorical antecendent and continuous moderator), we used a pick-a-point approach to probe the interaction at different levels of the moderator (which was plotted on the x axis). Coherently, we looked at the gaps between lines at those levels of the moderator. Instead, in this case we cannot do that, also because on the x axis we have the x variable and not the moderator. The moderator is moreover categorical, so each line in the chart show a different category of the moderator. What we want to consider now is not the gap between the lines at different levels of the moderator, but the inlcination (the “slope”) of the three lines that indicate different category of the moderator. So we can see that as the perceived pervasiveness of gender discrimination (x) increases, catherine is liked less and less when people are told she did not protest, while she is liked more and more when people are told she protested using an individual and a collective frame.\n\nx <- c(4.25,5.12,5.90,4.25,5.12,5.90,4.25,5.12,5.90)\ny <- c(5.70,5.29,4.92,5.40,5.77,6.11,5.51,5.78,6.02)\nw <- c(0,0,0,1,1,1,2,2,2)\n\nplot(y = y, x = x, \n     col=\"white\",\n     xlab=\"perceived pervasiveness of gender discrimination (x)\",\n     ylab=\"liking of the attorney (y)\")\n\nlegend.txt <- c(\"no protest\", \"individual protest\", \"collective protest\")\nlegend(\"topleft\", legend = legend.txt, \n       cex = 1, lty = c(1,2,1), lwd = c(4,2,1))\n\nlines(x[w==0], y[w==0], lwd=4, lty=1)\nlines(x[w==1], y[w==1], lwd=2, lty=2)\nlines(x[w==2], y[w==2], lwd=1, lty=1)"
  },
  {
    "objectID": "moderation.html#conditional-effects",
    "href": "moderation.html#conditional-effects",
    "title": "Moderation",
    "section": "Conditional effects",
    "text": "Conditional effects\nMultiple regression models are characterized by partial effects, and mediation models are mainly characterized by direct and indirect effects, the distinctive feauture of moderation models are conditional effects.\nConditional effects are among the first things to consider when interpreting a moderation model. They are included in several parts of the model output, and in particular of the section dedicated to probe the interaction, like the table conditional effects of the focal predictor at values of the moderator(s), and the interaction coefficients in the first regression table.\nConditional effects refer to the relation between two variables (dependent and independent) in particular “conditions”, that is to say, at particular values of the moderator, when the moderator takes on a particular value.\nThis is exactly what moderation analysis is about: distinguishing the particular circumstances that change the sing, strength, and significance of the relation between two variables. Indeed, we run a moderation analysis when we think that the relation between two variables may change depending on the values of a third variable, the moderator.\nWhen the moderator is categorical (e.g. Dichotomous or multicategorical) it segments the dataset into subgroups based on its categories (e.g., males and females, climate change frame or natural frame, or people told catherine did not protest, protested using an individual frame, or a collective frame). In this case, the moderated relation is analyzed within these groups (e.g., does the investigated relation change in males and females?).\nThe table conditional effects of the focal predictor at values of the moderator(s) shows you these conditional effects. It shows the change in the estimate of the dependent variable for a 1-unit change in the independent variable, when the moderator takes on a specific numerical value, if continuous, or in its categories, when dichotomous or multicategorical.\n\n\n\n\n\nprobing the interaction with a dichotomous moderator\n\n\n\n\nWhen the moderator is continuous, the analysis of the moderation is continuous too, but we probe the moderation by creating groups of data based on arbitrary values of the moderator. By default, process chooses the 16th, 50th, and 84th percentile of the moderator, to operationalize low, moderate, and high values of the moderator, but it is possible to use values corresponding to the mean and 1sd above and below the mean (moments=1), or to manually choose these values (e.g., wmodval = c(30,50,70)).\n\n\n\n\n\nprobing the interaction with a continuous moderator"
  },
  {
    "objectID": "moderation.html#conditional-effect-table",
    "href": "moderation.html#conditional-effect-table",
    "title": "Moderation",
    "section": "Conditional effect table",
    "text": "Conditional effect table\nSince the most distinctive element in conditional effect is the conditional part, when reading these effects, it may be useful to start with it. A way to read the values in the table table conditional effects of the focal predictor at values of the moderator(s) is by starting with something similar to: among cases ….\nFor instance: “among people told the drought was due to climate change (frame = 1) ….\n\n\n\n\n\nprobing the interaction with a dichotomous moderator\n\n\n\n\nOr: “among relatively young people (30 years old) …”.\n\n\n\n\n\nprobing the interaction with a continuous moderator\n\n\n\n\nAfter having taking into account the condition (in the first column), you can read the column “effect”, which shows the estimated change in the dependent variable for a 1-unit change in the independent variable, which is called “focal predictor” in the legend you find just above the table. You can say something like: “among relatively young people (30 years old), one unit change in negative emotion (the focal predictor) yields to an estimated increase of 0.310 points in support for government (p = 0.000, 95% ci [0.228, 0.391]).\nSince we are interested in how the relationship changes with the moderator, what is meaningful is not the single estimate, but how the estimates change at different levels of the moderator: “among relatively young people (30 years old), one unit change in negative emotion yields to 0.310 points in support for the government of 0.310 points, while among middle-aged people (50 years old), the same one-unit change in negative emotion yields to a higher increase in support for government (0.436 points), and among older people (70 years old), this increase is even higher: one unit change in negative emotion yields to an increase in support for the government of 0.563 points”."
  },
  {
    "objectID": "moderation.html#regression-coefficients",
    "href": "moderation.html#regression-coefficients",
    "title": "Moderation",
    "section": "Regression coefficients",
    "text": "Regression coefficients\nThe interpretation of regression coefficients is also conditioned to the values assumed by other variables. This is especially true for the interaction coefficients indicated as “int_1”, “int_2” etc. (depending on the number of interactions), and in a different way, for the single coefficients of the variables involved in the interactions. Let’s start by the latter.\nThe variables involved in the interactions are listed in the “product key term” table (in this example they are negemot and age). Let’s call the variables involved in the interaction “interaction variables” (this is not a technical term, but just something we use here to facilitate the discussion).\n\n\n\n\n\nThe interpretation of each of these “interaction variables” is conditional to the other “interaction variables” being set to zero. They measure the amount of change in the estimate of y for a 1-unit increase in that “interaction variable” when the other “interaction variable” is set to zero.\nFor instance: “among cases measuring zero on negative emotions (negemot), a one-unit increase in age yields to a decrease of 0.024 points in the estimated support for the government” (decrease, because the coefficient is negative: -0.024). In the same way: “among 0-year old people, one unit increase in negative emotion yields to a 0.120 point higher estimate of the support for government”.\nWhen considering a dichotomous variable, like for instance gender (female = 0, male = 1), remember that one-unit increase means that you are switching from one category (female = 0) the the next one (male = 1). In this case the coefficient represent a difference in the average of y in the group coded 1 and coded 0. For instance: “among cases measuring zero on negative emotions (negemot), the average support for government is 0.873 points lower in males than females” (lower, because the coefficient is negative: -0.873).\n\n\n\n\n\nImportantly, these coefficients (and their statistical significance tests) are not meaningful when the “zero-condition” is meaningless, because outside the actual values of the variable. For instance, 0-year old people do not exist, and even if they existed, they are not included in our dataset, so the estimate is meaningless and the coefficient, p-value, etc. Should not be interpreted. You can circumvent the problem by mean-centering the variable(s). By doing so, zero becomes the average of the variable, and the interpretation becomes, for instance: “among people who are average in negative emotions, one unit increase in age yields to an increase (or decrease) in estimated support for the government of xxx points”.\nIt should be underlined that the coefficients don’t need to be meaningful. Even if they are not interpretable, the model works perfectly fine to estimate and test the interaction, which is what we are usually interested in."
  },
  {
    "objectID": "moderation.html#interaction-coefficients",
    "href": "moderation.html#interaction-coefficients",
    "title": "Moderation",
    "section": "Interaction coefficients",
    "text": "Interaction coefficients\nThe interaction coefficients (“int_1”, “int_2”, and so on) also represent conditional effects. First of all, we look at the information on the statistical significance of the interaction (p-values and confidence intervals), because they allow us to conclude that the interaction is, or is not, significant.\nIf it is not significant, for instance, the moderation analysis is concluded, and it does not make any sense to proceed with further analyses (e.g. By checking the r2-change to measure its impact on the model, and by probing the interaction to see how the interaction effect changes at different values of the moderator).\nIn the example, the interaction is significant.\n\n\n\n\n\nThe interaction coefficients (int_1 etc.) Do not quantify changes in the estimate of y (as the other coefficients) but in the conditional effects. They quantify how much the conditional effect of the independent (x) on the dependent variable (y) changes as the moderator w changes by one unit. To express how w interacts with x to mediate its effect on y, the conditional effect is a complex term composed both by w and x. This will become clearer when we will learn to read the equation of the model.\nHowever, it should be underlined that it is not strictly necessary to interpret these coefficients. Usually, what matters, in the end, is the statistical significance of the interaction. If the interaction is significant, you can interpret it with other more appropriate statistical instruments, like the r2-change, the pick-a-point table, and the johnson-neymann table."
  },
  {
    "objectID": "moderation.html#reading-the-regression-table",
    "href": "moderation.html#reading-the-regression-table",
    "title": "Moderation",
    "section": "Reading the regression table",
    "text": "Reading the regression table\nTo recap, when interpreting the regression coefficients in a moderation model, you can do the following steps.\n\nModel summary\nFirst, check the overall significance and goodness of fit of the model in the model summary table.\n\n\nVariables involved in the interaction\n\nStart by looking at the “product term key” table to identify the variables involved in the interaction, which need therefore to be interpreted relative to the other variables (likewise involved in the interaction) being set to zero.\nNext, check whether the zero is included in the variable to be set to zero, otherwise avoid any interpretation (or re-estimate the model by mean-centering the variables that create problems).\nNext, start reading the coefficients from the “zero-condition”, e.g.: *“among cases measuring zero on negative emotions, a one-unit increase in age, increases the estimate of the govact (support for the government) of 0.024 points”; or: “among 0-year old people, a one-unit increase in negative emotion yields to an increase in the estimated support for the government of 0.120 units”.\n\n\n\nInteraction terms\n\nRemember that the interaction coefficients (int_1, int_2, etc.) Do not quantify changes in the estimate of y (as the other coefficients) but in the conditional effects. They quantify how much the conditional effect of the independent (x) on the dependent variable (y) changes as the moderator w changes by one unit. For instance: ” as age (the moderator) increases by one unit, the conditional effect of negative emotion (x) on government support (y) is estimated to increase by 0.006 points”.\nPay particular attention to the statistical significance of these coefficients. If they are not significant, you can conclude that there is no evidence of an interaction. If they are statistically significant, you can proceed to consider the change in the r-squared due to the interaction, and the tables dedicated to probing the interaction and the johnson neymann significance regions."
  },
  {
    "objectID": "moderation.html#interaction-and-conditional-effect",
    "href": "moderation.html#interaction-and-conditional-effect",
    "title": "Moderation",
    "section": "Interaction and conditional effect",
    "text": "Interaction and conditional effect\nThe interaction term “int_1” is \\(b_3xw\\). It is equal 0.006 and is statistically significant (p = 0.00). It quantifies the change in the conditional effect of x on y for a 1-unit change in w. It does not estimate a change in y, but a change in the conditional effect of x on y. The effect depends on the mediator w.\n\n\n\n\n\nThe conditional effect is not expressed by a single number but by a function. It is a function describing the differentiated way x affects y depending on the values of the moderator. A function is a mathematical “mechanism” that transforms input numbers into an output. This mechanism takes the place of the single coefficient that would characterize x in a regression model without moderation. In such a simpler mode, the effect of x on y would be expressed by \\(b_1x\\), while in this case it is expressed by both \\(b_1x\\) and \\(b_3xw\\). Both have to be taken into account to understand the effect of x on y because it depends on (i.e., it is a function of) the moderator.\nThe function that describes the conditional effect can be identified by collecting the terms containing x, which results to be \\(b_1 + b_3w\\), and is represented by the symbol “theta” \\(\\theta\\) in hayek’s book. Thus, we can see the effect of x on y is not a single number, but the function \\((b_1 + b_3w)\\). The value of x is not multiplied by a single coefficient which is always the same, as in regression without interaction. Instead, the value of x is multiplied by the result of this function to determine its effect on y, where this effect depends on the value of w (i.e., it is moderated by w).\n\\[\\hat y = b_1x + b_2w + b_3xw\\] \\[\\hat y = (b_1 + b_3w)x + b_2w\\] \\[\\theta_{x \\to y} = b_1 + b_3w\\]\nIn this case \\(b_1\\) is the coefficient of x (negemot = 0.120) and \\(b_3\\) is the coefficient of the interaction term (int_1 = 0.006). By plugging these values into the function of the conditional effect, we have what follows:\n\\[\\theta_{x \\to y} = 0.120 + 0.006w\\]"
  },
  {
    "objectID": "moderation.html#interaction-and-conditional-effect-1",
    "href": "moderation.html#interaction-and-conditional-effect-1",
    "title": "Moderation",
    "section": "Interaction and conditional effect",
    "text": "Interaction and conditional effect\nThe function \\(\\theta_{x \\to y} = 0.120 + 0.006w\\) describes how x affects y, depending on the values of w.\nFor instance, let’s plug values of w from 1 to 6 into the function. We can see that a 1-unit increase of w yields to 0.006 unit increase of the conditional effect. That is to say, the coefficient of the interaction term quantifies how much the conditional effect of x on y changes at a 1-unit increase in w. It increases by 0.006 points (the coefficient of int_1).\n\ntheta <- function(w){0.120 + 0.006*w}\nw <- c(1,2,3,4,5,6)\ndata.frame(w = w, \"theta\" = theta(w))\n\n  w theta\n1 1 0.126\n2 2 0.132\n3 3 0.138\n4 4 0.144\n5 5 0.150\n6 6 0.156\n\n\nThe same procedure used here is applied by the pick-a-point technique to probe the interaction at different levels of w. After having found the conditional effect function, the pick-a-point technique plugs different values of the moderator into the function, to measure the changes of the conditional effect of x on y.\nFor instance, by plugging the values of the moderator age equal 30, 50, and 70, we find the “effects” listed in the table “conditional effect of the focal predictor at values of the moderator(s)”. (in the example, results are only aproximately the same reported in the output of process because numbers were rounded and are thus imprecise).\n\nw <- c(30,50,70)\ndata.frame(\"age\" = w, \"effect\" = theta(w))\n\n  age effect\n1  30   0.30\n2  50   0.42\n3  70   0.54\n\n\n\n\n\n\n\nprobing the interaction with a continuous moderator\n\n\n\n\nWhen the moderator (w) is dichotomous, the pick-a-point technique shows the conditional effect of \\(x\\) on \\(y\\) in the two groups defined by the categories of the variable (e.g., males and females, or those assigned to the experimental condition and the control group, etc.). In the example, the effect of skepticism (\\(x\\)) on the justification for not providing help (\\(y\\)) is higher in the frame=1 group (0.31) and lower in the frame=0 group (0.11).\n\n\n\n\n\nprobing the interaction with a dichotomous moderator\n\n\n\n\nInstead, when the antecedent variable x is dichotomous, the table shows differences in the estimate of y between the two conditions of the antecendent variable (\\(x=0\\) and \\(x=1\\)) for a 1-unit change in w.\nRecalling basic arithmetic terminology, a “difference” is the outcome of subtraction, and in subtraction, the first term is called “minuend”, and the second one (the number that is subtracted) is the “subtrahend” (i.e, minued - subtrahend = difference). In this table involving a dichotomous antecendent variable (\\(x\\)), the minuend is the group coded as 1 (frame(x) = 1), and the subtrahend is the reference group that is coded as 0 (frame(x) = 0). Thus, a negative effect means that the effect is higher in the reference group, and a positive effect means that the effect is higher in the other group.\n\n\n\n\n\nprobing the interaction with a dichotomous moderator\n\n\n\n\nFor instance, at relatively low levels of skepticism (skepticism = 1.592), the difference in justification for not providing help (\\(y\\)) is higher in the reference group frame = 0 (which is the group told the drought was due to natural causes) than in the other group frame = 1 (which is the group told the drought was due to climate change). However, the difference is not statistically significant. Instead, for higher values of skepticism, a one-point increase in skepticism yields to a higher level of justification for not providing aid to the afflicted populace."
  },
  {
    "objectID": "moderation.html#coefficients-of-x-and-w",
    "href": "moderation.html#coefficients-of-x-and-w",
    "title": "Moderation",
    "section": "Coefficients of x and w",
    "text": "Coefficients of x and w\nConcerning the coefficients of variables involved in interactions (listed in the table “product terms key”, in this case “negemot” and “age”), the equation makes clearer why the interpretation is relative to each of the variables alternately set equal to zero.\n\n\n\n\n\nConsidering the equation of the model \\(\\hat y = b_1x + b_2w + b_3xw\\) (ignoring the constants like the intercept and covariates), we can see what happens when we set \\(w\\) to 0, and \\(x\\) to 0.\nRecalling that the product of any number and zero is zero, when \\(w\\) is zero, the terms including the multiplier \\(w\\) such as \\(b_2w + b_3xw\\) become zero, and can be removed from the equation. By this way, we are left with only the variable x and its coefficient: \\(b_1x\\) (“negemot” = 0.120). So we can interpret the \\(b_1\\) coefficient by saying that when age (w) is equal to zero, a one-unit increase in negative emotion yields to an increase in the estimate of y of 0.120 points.\n\\[\\hat y = b_1x + b_2*(w=0) + b_3x*(w=0)\\] \\[\\hat y = b_1x + b_2*0 + b_3x*0\\] \\[\\hat y = b_1x\\] \\[\\hat y = 0.120*negemot\\]\nWe already said that this interpretation, although formally correct, does not make sense, because saying that w=0 is meaningless. This would be true also if cases measuring 0 on w would theoretically exist but are not in the range of the variable in the dataset. For instance, on page 240 of the book hayes wrote, in relation to a similar case:\n\n“disregard this estimate and its test of significance, for they are meaningless. Even if it was possible to score so low on the scale, in the data the lowest score is 1, so at best this would represent an interpolation of the results of the model beyond the range of the available data. Such interpolation is generally a bad idea.”\n\nWith r you can check the distribution of a variable (or of all the variables of a dataset) by using the command “summary”:\n\nsummary(glbwarm$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  17.00   36.00   51.00   49.54   63.00   87.00 \n\n\n\nsummary(glbwarm)\n\n     govact         posemot         negemot           nfc           tnews      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:2.000   1st Qu.:2.330   1st Qu.:4.20   1st Qu.:2.600  \n Median :4.800   Median :3.000   Median :3.670   Median :4.80   Median :3.600  \n Mean   :4.587   Mean   :3.132   Mean   :3.558   Mean   :4.77   Mean   :3.522  \n 3rd Qu.:5.600   3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:5.40   3rd Qu.:4.600  \n Max.   :7.000   Max.   :6.000   Max.   :6.000   Max.   :7.00   Max.   :7.000  \n    ideology          age            gender            educ      \n Min.   :1.000   Min.   :17.00   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:36.00   1st Qu.:0.0000   1st Qu.:3.000  \n Median :4.000   Median :51.00   Median :0.0000   Median :3.000  \n Mean   :4.083   Mean   :49.54   Mean   :0.4883   Mean   :3.119  \n 3rd Qu.:5.000   3rd Qu.:63.00   3rd Qu.:1.0000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :87.00   Max.   :1.0000   Max.   :4.000  \n    partyid     \n Min.   :1.000  \n 1st Qu.:1.000  \n Median :2.000  \n Mean   :1.883  \n 3rd Qu.:3.000  \n Max.   :3.000  \n\n\nIn the same way, we can read the coefficient of the variable age \\(w\\) relative to that of negative emotion set to zero \\(x\\). When \\(x=0\\) the terms including \\(x\\) become zero and can be removed from the equation, and we are left with only one variable age \\(w\\), and its coefficient \\(b_2\\), which can be intepreted as follows: among cases measuring zero in negative emotions, a one-unit change in age yields to a decrease of 0.024 in the estimate of y.\n\\[\\hat y = b_1x + b_2w + b_3xw\\]\n\\[\\hat y = b_1*(x=0) + b_2w + b_3*(x=0)*w\\] \\[\\hat y = b_2w \\] \\[\\hat y = -0.024*age\\]\nNotice that the meaning of “0” changes depending on the nature of the variable. When it is numeric (continuous or discrete), zero just means zero (numeric). When the variable is dichotomous, zero indicates the category coded with 0, which is the reference group. For instance, with a continuous w, you can say: among cases measuring 0 on w, a 1-unit increase in x (…). With a dichotomous w, you say, for instance: among people told the drought was due to natural causes (w=0) a 1-unit increase in x (…)."
  },
  {
    "objectID": "moderation.html#coefficients-of-variables-involved-in-the-interaction",
    "href": "moderation.html#coefficients-of-variables-involved-in-the-interaction",
    "title": "Moderation",
    "section": "Coefficients of variables involved in the interaction",
    "text": "Coefficients of variables involved in the interaction\nLet’s start by the coefficients not directly expressing interactions, but qualifying variables involved in the interaction. We know we can find them in the “product term key” table. They are \\(x_1\\) (\\(x_1 = -4.13\\)), \\(x_2\\) (\\(x_2 = -3.49\\)) and sexism (\\(w = -0.47\\)).\n\n\n\n\n\nWhen \\(w = 0\\) (ignoring the intercept and the error term, which do not influence the interpretation) the terms including the multiplier \\(w\\) become zero and can be removed from the equation. In this way, we are left with only the two variables \\(x_1\\) and \\(x_2\\).\n\\[\\hat y = b_1x_1 + b_2x_2 + b_3*(w=0) + b_4x_1*(w=0) + b_5x_2*(w=0)\\] \\[\\hat y = b_1x_1 + b_2x_2\\]\nNow, you could think that you need to set x1 to zero to interpret the coefficient of \\(x_2\\), and viceversa. While this is fundamentally true, there is no need to do so, because this is what was already done by default when fitting a model with a multi categorical variable. The statistical software represents a multicategorical variable with g groups (in this case 3) through g-1 dichotomous variables (in this case \\(x=1\\) and \\(x=2\\)).\n\n\n\n\n\nThe following table shows what happens: the cases assigned to the reference group \\(x=0\\) (no protest) are the ones marked with a zero on both \\(x_1\\) and \\(x_2\\). The cases assigned to the \\(x=1\\) group (individual protest frame) are the ones marked with a 1 on \\(x_1\\) and 0 on \\(x_2\\). Finally, the cases assigned to the \\(x=2\\) group (collective protest frame), are those marked with a 1 on \\(x_2\\) and 0 on \\(x_1\\). This is to say that when we are interpreting the coefficients or effects of \\(x_1\\) or \\(x_2\\), respectively, we do not have to care about \\(x_2\\) or \\(x_1\\), respectively, since they already are set to zero by default.\n\nlibrary(tidyverse)\nprotest %>% \n  mutate(x1 = case_when(protest==0 ~ 0,\n                        protest==1 ~ 1,\n                        protest==2 ~ 0),\n         x2 = case_when(protest==0 ~ 0,\n                        protest==1 ~ 0,\n                        protest==2 ~ 1)) %>%\n  select(protest, x1, x2)\n\n# A tibble: 129 × 3\n   protest           x1    x2\n   <dbl+lbl>      <dbl> <dbl>\n 1 2 [collective]     0     1\n 2 0 [no protest]     0     0\n 3 2 [collective]     0     1\n 4 2 [collective]     0     1\n 5 2 [collective]     0     1\n 6 1 [individual]     1     0\n 7 2 [collective]     0     1\n 8 0 [no protest]     0     0\n 9 0 [no protest]     0     0\n10 0 [no protest]     0     0\n# … with 119 more rows\n\n\nThus, going back to our interpretation problem: when \\(w = 0\\) (ignoring the intercept and the error term, which do not influence the interpretation) the terms including the multiplier \\(w\\) become zero and can be removed from the equation. In this way, we are left with only the two variables \\(x_1\\) and \\(x_2\\). We can interpret now the coefficient of each one without caring about the other, because it is set to zero by default.\n\\[\\hat y = b_1x_1 + b_2x_2 + b_3*(w=0) + b_4x_1*(w=0) + b_5x_2*(w=0)\\]\n\\[\\hat  y = b_1x_1 + b_2x_2\\]\nThus we can write this equation either as:\n\\[\\hat y = b_1x_1 + (b_2x_2*0)\\] \\[\\hat y = b_1x_1\\] \\[\\hat y = -4.13x_1\\]\nAnd:\n\\[\\hat y = (b_1x_1*0) + b_2x_2\\]\n\\[\\hat  y = b_2x_2\\] \\[\\hat  y = -3.49x_2\\]\nThe last complication in interpreting the coefficients of multicategorical variables is that they quantify differences between them and the reference category. From this perspective, they are like the coefficients of a dichotomous variable (indeed, we learned that a multicategorical variable is treated as several dichotomous variables). Keeping in mind that a negative coefficient means that the coefficient of the reference group is higher, and lower otherwise, \\(\\hat y = -4.13x_1\\) means that among cases measuring zero on the moderator w (sexism), the average appreciation of catherine (liking = y) in the individual protest condition (x1) is 4.13 points lower than that in the reference group (no protest). \\(x_2\\) can be interpreted similarly. In any case, these coefficients have no substantive meaning, because no cases are zero on the sexism scale.\n\nsummary(protest$sexism)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.870   4.500   5.120   5.117   5.620   7.000 \n\n\nThe variable “sexism” \\(w\\) is not multicategorical. However it is involved in the interaction with the multicategorical variable \\(x\\). Its coefficient (\\(b_3w = -0.47\\)) quantifies the difference in the estimate of \\(y\\) for one-unite change in the “sexism” measure, when x (both x1 and x2) is equal zero. As we know from how multicategorical variables are transformed, when both \\(x_1\\) and \\(x_2\\) are zero we are referring to the reference group. Therefore, this coefficient quantifies the estimate of y for a one-unit change in sexism among people in the reference group. In other terms, among people in the reference group (told catherine did not protest), a 1-unit increase in sexism decrease the estimate of y by 0.47 points.\n\\[\\hat y = b_1x_1 + b_2x_2 + b_3w + b_4x_1w + b_5x_2w\\]\n\\[\\hat y = (b_1x_1*0) + (b_2x_2*0) + b_3w + (b_4x_1w*0) + (b_5x_2w*0) + e\\] \\[\\hat y = b_3w\\] \\[\\hat y = -0.47w\\]"
  },
  {
    "objectID": "moderation.html#interaction-and-conditional-effect-2",
    "href": "moderation.html#interaction-and-conditional-effect-2",
    "title": "Moderation",
    "section": "Interaction and conditional effect",
    "text": "Interaction and conditional effect\nIn the regression table, int_1 and int_2 quantify, respectively, the difference in the conditional effect of x on y between groups x1 and the reference group x0, and between the group x2 and the reference group x0, as w changes by one unit. This interpretation is strictly related to that of conditional relative effects, as we are going to show.\n\n\n\n\n\nConsidering the equation of the model, we can see that “int_1” and “int_2” are, respectively, represented as \\(b_4x_1w\\) and \\(b_5x_2w\\).\n\\[y = i_y + b_1x_1 + b_2x_2 + b_3w + b_4x_1w + b_5x_2w + e\\]\nAs already said, we can ignore the intercept and the error term, and we already know that \\(b_3w\\) quantifies the changes in the average of y as w increases of one-unit, when both x1 and x2 are equal to zero, which means in the reference category.\n\\[\\hat y = (b_1x_1*0) + (b_2x_2*0) + b_3w + (b_4x_1w*0) + (b_5x_2w*0)\\]\nSince we are not interested in the reference group but in the x1 and x2 ones (\\(b_4x_1w\\) and \\(b_5x_2w\\)), we can also exlude this term (\\(b_3w\\)). We then said that, when interpreting x1 we can ignore x2, and vice versa, due to the way the regression model is built. Therefore, we can split the equation into two parts, one including x1 and the other including x2:\n\\[y = b_1x_1 + b_4x_1w\\] \\[y = b_2x_2 + b_5x_2w\\]\nBy collecting the x terms, we end up with the two relative conditional effects \\(\\theta_{x_1 \\to y}\\) and \\(\\theta_{x_2 \\to y}\\), composed of \\(b_1\\) and \\(b_4w\\) (where \\(b_4\\) is the “int_1” coefficient), and \\(b_2\\) and \\(b_5w\\) (where \\(b_4\\) is the “int_2” coefficient), respectively. So, we can see that the interaction terms int_1 and int_2 are a foundamental component of the relative conditional effect. As stated at the beginning of the discussion, they represent how the relative conditional effect of x on y changes as w changes by one unit. \\[y = (b_1 + b_4w)x_1\\] \\[\\theta_{x_1 \\to y} = b_1 + b_4w\\]\n\\[y = (b_2 + b_5w)x_2\\] \\[\\theta_{x_2 \\to y} = b_2 + b_5w\\]\nWe can now plug the coefficients into the above functions:\n\\[\\theta_{x_1 \\to y} = -4.13 + 0.90w\\] \\[\\theta_{x_2 \\to y} = -3.49 + 0.78w\\]"
  },
  {
    "objectID": "moderation.html#probing-the-interaction-1",
    "href": "moderation.html#probing-the-interaction-1",
    "title": "Moderation",
    "section": "Probing the interaction",
    "text": "Probing the interaction\nThe pick-a-point procedure plug specific w values into the functions defining the conditional relative effects in order to probe the interaction at different levels of the moderator:\n\\[\\theta_{x_1 \\to y} = -4.13 + 0.90w\\]\n\\[\\theta_{x_2 \\to y} = -3.49 + 0.78w\\]\nFor instance, for “relatively low” values of the moderator (w=4.25), the conditional relative effect equal \\(-4.13 + 0.90*4.25\\) (small differences from the table are due to the fact we are using rounded vales, otherwise it would be something like \\(-4.128758332 + 0.901226964*4.25\\) which rounded to the second decimal is exactly 0.30).\n\n\n\n\n\nThe interpretation takes into account the comparison with the reference group, but can be supported by reference to the means of y in the different groups, reported in the table estimated conditional means being compared. For instance: among those relatively low in the sexism scale (4.25), catherine is more liked by those told she did not protest (m = 5.70) than she protested using an individual frame (m = 5.40) or a collective frame (5.51). However these differences are not significant (p = 0.67).\nWhen working with a multicategorical variable, we have a general (“omnibus”) test of significance that says if there is any difference between the averages of y in the three groups, but without saying which groups are different (“test of equality of conditional means”). And then we have the usual table “conditional effects of the focal predictor at values of the moderator(s)”, where we can see if there is a difference between each of the considered categories (x1 and x2 in this case) and the reference group."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "At the end of this section you will understand the following concepts, know the R functions to calculate/create the following statistical measures/plots, and be able to interpret them:\n\nSample size and data set preview\nFrequency distributions of a variable\nHistograms, boxplots, bar plots\nMeasures of central tendency and measures of spread\nStandardization and z-scores"
  },
  {
    "objectID": "EDA.html#sample-size-and-data-preview",
    "href": "EDA.html#sample-size-and-data-preview",
    "title": "EDA",
    "section": "Sample size and data preview",
    "text": "Sample size and data preview\nWhen starting to work with a new data set, the first step is to describe its general characteristics and those of its variables. This includes:\n\ndescribing the number of cases in the data set (the sample size)\nexamining the types of variables\nand computing descriptive statistics such as the mean and standard deviation.\n\nA research report typically contains information on the data set and variables.\nWe already learned how to perform some of these operations by using the following functions:\n\ndim() to find the number of rows of the data set, which corresponds to the number of cases (sample size) in a common and well structured data set.\nstr() or glimpse() (in the package tidiverse) to look at the types of variables included in the data set\nhead() to see a preview of the data set\nView() to view the data set in a separate window (warning: not reccomended if the data set is huge, since it can freeze your computer)\n\nLet’s see an example, using the global warming study data set (glbwarm).\n\n# load the glbwarm data set\nglbwarm <- read.csv(\"data/glbwarm.csv\")\n\nIt’s a well-organized data set. Each line describes a case. The sample size can therefore be calculated by counting the number of rows.\n\ndim(glbwarm)\n\n[1] 815   7\n\n\nThe sample size is 815 (n = 815, where n stands for the number of cases included in the sample).\nWe can also see that there are 7 variables. We can now look at their type.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nglimpse(glbwarm)\n\nRows: 815\nColumns: 7\n$ govact   <dbl> 3.6, 5.0, 6.6, 1.0, 4.0, 7.0, 6.8, 5.6, 6.0, 2.6, 1.4, 5.6, 7…\n$ posemot  <dbl> 3.67, 2.00, 2.33, 5.00, 2.33, 1.00, 2.33, 4.00, 5.00, 5.00, 1…\n$ negemot  <dbl> 4.67, 2.33, 3.67, 5.00, 1.67, 6.00, 4.00, 5.33, 6.00, 2.00, 1…\n$ ideology <int> 6, 2, 1, 1, 4, 3, 4, 5, 4, 7, 6, 4, 2, 4, 5, 2, 6, 4, 2, 4, 4…\n$ age      <int> 61, 55, 85, 59, 22, 34, 47, 65, 50, 60, 71, 60, 71, 59, 32, 3…\n$ sex      <int> 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0…\n$ partyid  <int> 2, 1, 1, 1, 1, 2, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 3, 1, 3, 2…\n\n\nThe glimpse function also includes a preview of the data set, but a clearer preview is displayd with head():\n\nhead(glbwarm)\n\n  govact posemot negemot ideology age sex partyid\n1    3.6    3.67    4.67        6  61   0       2\n2    5.0    2.00    2.33        2  55   0       1\n3    6.6    2.33    3.67        1  85   1       1\n4    1.0    5.00    5.00        1  59   0       1\n5    4.0    2.33    1.67        4  22   1       1\n6    7.0    1.00    6.00        3  34   0       2"
  },
  {
    "objectID": "EDA.html#measure-of-central-tendency",
    "href": "EDA.html#measure-of-central-tendency",
    "title": "EDA",
    "section": "Measure of central tendency",
    "text": "Measure of central tendency\nDescriptive statistics are used to describe and summarize variables of a data set.\nTypically, there are two general types of statistics that are used to describe data: Measures of central tendency and Measures of spread.\nMeasures of central tendency include: - Mode: the most frequent score in our data set - Median: the middle score for a set of data that has been arranged in order of magnitude - Mean: the sum of all the values in the data set divided by the number of values in the data set.\nA clear description of the different measures can be found here.\n\nThe Mean\nIn this course we’ll focus on the mean (or average). This is the most popular and well known measure of central tendency, and it is used in linear regression modeling.\nThe mean itself is essentially a model of a data set. One of its important properties is that it minimises error in the prediction of any one value in the data set. In other terms, lacking better predictors, the mean is the best predictor of a value in the data set.\nIn R, you can calculate the mean of a variable by the function mean() (reminder: to access a variable in the data set, that is to say, a column of a data.frame, use the dollar sign):\n\nmean(glbwarm$age)\n\n[1] 49.5362\n\n\nThe mean age of people in the sample is 49.5362. To round off decimals, you can add the function round(). It takes two main arguments: a number (or a function that returns a number) to be rounded off, and an integer that tells R how many decimals you want to include.\n\nround(mean(glbwarm$age), 2)\n\n[1] 49.54\n\n\nThis is equivalent to creating an object mean_age representing the output of the function mean (i.e., the mean age), and then rounding it off.\n\nmean_age <- mean(glbwarm$age)\nround(mean_age, 2)\n\n[1] 49.54\n\n\n\n\nFrequency distributions\nThe frequency distribution of a variable is an overview of its values and the number of times they occur (i.e., their frequency).\nThe distributions can be continuous or discrete, depending on the type of variable. The distribution of a variable is summarized by the five-number summary, and graphically, by histograms and box plots (for continuous or discrete variables) and bar charts (for categorical variables).\n\nDiscrete variables have discrete distributions. Such a variable can only take specific values. Count data (non-negative integers), are an example: for instance, the number of friends on Facebook. In the glbwarm data set, the variable ideology is measured on a discrete scale ranging from 1 to 7.\nDiscrete variables can also represent categorical data. Categorical data expresses qualitative dimensions of a phenomenon. For instance, city names (Vienna, London, Verona), or ordered categorical values (low, medium, high). Categorical data can be represented by numbers, but they should not be conceived as mathematical objects, but as “names” or ordered categories. Categorical variables that can take on only two values are called dichotomous variables (e.g., sex, if you only differentiate between males and females). In the data set glbwarm, the variable “sex” is represented as a dichotomous variable.\nContinuous variables have continuous distributions. This kind of variable can assume one of an infinite (uncountable) number of different values. Continuous variables are represented by real numbers. Temperature or velocity are an example. In the data set glbwarm, the variable “govact” is represented as a continuous variable.\n\n\n\nFive-Number Summary\ndata set\nA numerical description of the distribution is provided by the function summary(), which gives the five-number summary.\n\nsummary(glbwarm$ideology)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.000   4.000   4.083   5.000   7.000 \n\n\nThe five-number summary includes the mean, the median, minimum and maximum values, the first and third quartiles. Quartiles are cut points dividing the range of values of the variable into four parts, or quarters, of more-or-less equal size (25%, 25%, 25%, 25%). The median is the value separating the lower 50% from the upper 50% of the distribution.\n\n\nBoxplot\nThe same information of a five-number summary is graphically displayed by the boxplot, also called box-and-whisker plot. The “box” represent the 1st and 3rd quartile, the central line is the median, the “whiskers” represent the maximumn and minimum.\nThe points above and below the whiskers are outliers, data points that differ significantly from other observations. Since they are so different they are difficult to predict. Moreover, they can bias the estimates of a linear regression models.\n\n# use the argument \"main\" to add a title\nboxplot(glbwarm$govact, main = \"Ideology\")\n\n\n\n\n\n\nHistogram\nAnother way to show the distribution of a variable is by means of an histogram, using the function hist().\nThe function can be used to show the frequency distribution of values:\n\n# use \"main\" tp change the title, and \"xlab\" to change the name of the x axis\nhist(glbwarm$govact, \n     main = \"GOVACT (GLBWARM data set)\", xlab = \"govact\")\n\n\n\n\nAs well as the probability distribution:\n\n# you can use the argument \"col\" to change the color\n# to see the available colors, run the following command in the console: colors() \nhist(glbwarm$govact, probability = TRUE,\n     main = \"GOVACT (GLBWARM data set)\", xlab = \"govact\",\n     col = \"slategrey\")\nlines(density(glbwarm$govact, adjust = 2))\n\n\n\n\nWhen dealing with continuous variables, may want to add a line to overlay a density curve on the histogram, to aproximate the shape of the distribution.\n\nhist(glbwarm$govact, probability = TRUE,\n     main = \"GOVACT (GLBWARM data set)\", xlab = \"govact\",\n     col = \"slategrey\")\n\n# this is to add a line of the \"density\"\n# the argument \"lwd\" sets the width of the line\nlines(density(glbwarm$govact), \n      col = \"steelblue1\", \n      lwd = 2)\n\n# this is to add a more smoothed (simpler and more aproximated) density curve \n# the argument \"lty\" sets the type of line\nlines(density(glbwarm$govact, adjust = 2), \n      col = \"lightblue\", \n      lwd = 3, \n      lty = \"dotted\")\n\n\n\n\n\n\nBar plot\nHistograms and box plot are used to represent the distribution of numerical variables, such as continuous and discrete variables. For categorical variables, a bar graph is used in place.\nFor instance, the histogram of the dichotomous categorical variable “sex”, is not appropriate. It only takes on the values 0 and 1, but they are not numbers (0 = male, 1 = female) and there is nothing in between them (e.g., 0, 0.1, 0.2, …, 1).\n\nhist(glbwarm$sex, main = \"SEX (categorical dichotomous variable)\", xlab = \"sex\")\n\n\n\n\nThe appropriate way to represent the frequency distribution of categorical variables is the bar chart (or bar plot). The corresponding R function is barplot().\nIt takes as argument a table with the frequency for each of the modalities of the category (i.e., a table with the number of males and females in the sample). The function table() returns the frequency of the variable.\n\ntable(glbwarm$sex)\n\n\n  0   1 \n417 398 \n\n\nThis function can be used as the main argument of the function barplot():\n\nbarplot(table(glbwarm$sex),\n        main = \"Observations by SEX\",\n        xlab = \"0 = Male 1 = Female\")"
  },
  {
    "objectID": "EDA.html#measures-of-spread",
    "href": "EDA.html#measures-of-spread",
    "title": "EDA",
    "section": "Measures of Spread",
    "text": "Measures of Spread\nMeasures of spread describe how similar or varied the set of observed values are for a particular variable. Measures of spread include the range, quartiles and the interquartile range, variance and standard deviation.\nFor example, the interquartile range is the difference between the third and first quartile and is represented by the size of the “box” in a boxplot.\n\nboxplot(glbwarm$age)\n\n\n\n\nEven more relevant are variance and standard deviation.\n\nVariance\nVariance (represented as \\(s^2\\) when computed on a sample, as in this case), is a measure of dispersion that quantifies how spread the values of a variable are from the mean (the average value of the variable), or in other terms, of how much they vary (“variance”).\nThe formula for variance is:\n\\[s^2 =  \\frac{\\sum_{i=1}^{n} (x_i-\\bar x)^2}{n-1}\\]\nCapital sigma \\(\\sum\\) is the summation symbol, \\((x_i-\\bar x)\\) is the deviation (i.e., the difference) of each value \\(i\\) of \\(X\\) from the mean of \\(X\\) (\\(\\bar x\\), the mean or average of the variables), and \\(n\\) is the sample size. In R:\n\nvar(glbwarm$age)\n\n[1] 266.6937\n\n\n\n\nStandard Deviation\nStandard deviation (represented as \\(s\\) when computed on a sample, as in this case) is a related, and more commonly used, measure of dispersion. A low standard deviation indicates that the values of a variable tend to be close to its average, while a high standard deviation indicates that the values are spread out over a wider range. It is the square root of the variance:\n\\[s = \\sqrt \\frac{\\sum (x_i-\\bar x)^2}{n-1}\\]\nIn R:\n\nsd(glbwarm$age)\n\n[1] 16.33076"
  },
  {
    "objectID": "EDA.html#correlation",
    "href": "EDA.html#correlation",
    "title": "EDA",
    "section": "Correlation",
    "text": "Correlation\ndata set\nCorrelation is a statistical relationship, whether causal or not, between two variables.\nTwo variables \\(X\\) and \\(Y\\) are said to be correlated when they show a systematic association, so that knowing the values of one variable enables the prediction of the values of the other.\nCorrelation analysis permits to identify this relationship and to quantify its strength."
  },
  {
    "objectID": "EDA.html#pearsons-correlation-coefficient-pearsons-r",
    "href": "EDA.html#pearsons-correlation-coefficient-pearsons-r",
    "title": "EDA",
    "section": "Pearsons’ correlation coefficient (Pearsons’ R)",
    "text": "Pearsons’ correlation coefficient (Pearsons’ R)\nThe Pearson’s product moment correlation, also known as Pearson’s r, is an important measure of correlation.\nIt is also the foundation of most of the linear regression methods discussed in this course. Indeed, it can be used to quantify the linear association (or relationship) between two quantitative variables, a quantitative and a dichotomous variable, as well as between two dichotomous variables.\nIn linear association, the direction and rate of change in one variable are constant with respect to changes in the other variable. For example, in a perfect linear association between two variables \\(X\\) and \\(Y\\), an increase of 1 point in \\(X\\) corresponds to an increase of 1 point in \\(Y\\).\nA linear relationship describes a straight-line relationship between two variables.\n\n\n\n\n\n\nPearsons’ R and covariance\nPearson’s r is based on the formalization of the idea of calculating the ratio between the degree to which \\(X\\) and \\(Y\\) vary together, and the degree to which \\(X\\) and \\(Y\\) vary separately, in order to find how much “co-variation” they share.\n\\[r = \\frac{degree-to-which-X-and-Y-vary-together}{degree-to-which-X-and-Y-vary-separately}\\]\nThe idea of covariation is mathematically expressed by covariance (\\(cov\\)), which is at the heart of virtually all statistical methods. The Pearson’s \\(r\\) is a standardized measure of covariance.\nCovariance is based on variance (\\(s^2\\)):\n\\[s^2 =  \\frac{\\sum_{i=1}^{n} (x_i-\\bar x)^2}{n-1}\\]\nMore precisely, covariance is a measure of the joint variability of two variables, and is calculated as the product of the variance of the variables:\n\\[ cov = \\frac{\\sum_{i=1}^{n} (x_i-\\bar x) (y_i-\\bar y)}{n-1}\\]\nThe covariance can be positive, negative, or equal to zero, and gives a measure of the linear correlation between variables.\nSo, why do we need Pearson’s r? Because covariance is a measure that depends on the unit of measurement of the variables, while it is preferable to have a standard measure, meaningful independently of each specific data set.\nStandardizazion of covariance is obtained by dividing the covariance \\(cov\\) by the product of the standard deviations of the variables \\(X\\) and \\(Y\\). This product represents the total amount of variation possible.\nThanks to the denominator, which represents the total amount of variation possible for the specific couple of variables, \\(r\\) is always constrained between 1 and -1 for any variable expressed in any unit of measure. The extent to which the covariation \\(cov_{XY}\\) accounts for all of the possible variation (\\(SD_XSD_Y\\)) is the extent to which \\(r\\) approximates \\(|1|\\) (negative -1, or positive +1).\n\\[ r = \\frac{cov_{XY}}{SD_XSD_Y}\\]\nIn R, the correlation coefficient is computed with the function cor. For example:\n\ncor(glbwarm$govact, glbwarm$negemot)\n\n[1] 0.5777458\n\n\n\n\nPearsons’ R interpretation\nPearson’s correlation coefficient can be interpreted as follows:\n\nThe sign (+/-) corresponds to the direction of the association between X and Y:\n\npositive linear association (+): the higher X, the higher Y;\nnegative linear association (-): the higher X, the lower Y.\n\n\\(r\\) is close to zero when the association is null or nonlinear\nThe closer \\(r\\) is to \\(|1|\\) (+1 or -1), the stronger the linear association.\n\nThere are different rule of thumbs for the interpretation of the Pearson’s correlation. For example, Mike Allen (2017), The SAGE Encyclopedia of Communication Research Methods (p. 269) suggests the following:\n\n0.00-0.19 Negligible correlation\n0.20-0.39 Weak correlation\n0.40-0.59 Fair correlation\n0.60-0.79 Moderate correlation\n0.80-1.00 Strong correlation\n\n\n\nScatterplots\nScatterplot is the common method for visually exploring and representing the correlation between two variables is scatterplot.\n“A scatter plot is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis” (Wikipedia).\nTo create a scatterplot in R it is possible to use the function plot(). It takes two argoments: x (the first variable) and y (the second variable). The argument pch changes the shape of the points: see more details here.\n\nplot(glbwarm$govact, glbwarm$posemot,\n     pch = 20,\n     xlab = \"govact\", \n     ylab = \"posemot\")\n\n\n\n\nThe scatter plot shows the direction of the association.\n\nX <- rnorm(100) # simulating data\nYp <- X + rnorm(100, sd=0.5)\nYn <- -Yp\n\n\npar(mfrow=c(1,2))\nplot(X, Yp, main = \"Positive Association\")\nplot(X, Yn, main = \"Negative Association\")"
  },
  {
    "objectID": "EDA.html#predictions",
    "href": "EDA.html#predictions",
    "title": "EDA",
    "section": "Predictions",
    "text": "Predictions\nWhen there is a correlation between \\(X\\) and \\(Y\\), using \\(X_j\\) to estimate \\(Y_j\\) produces estimates that are more accurate than if one were to merely estimate \\(Y_j = \\bar Y\\) (the average value of \\(Y\\)) for every case in the data.\n\nGiven a variable \\(X\\) and \\(Y\\) of a data set of size n, the subscripts “j” in \\(X_j\\) and \\(Y_j\\), indicates the j case. It can be any one of the n cases, but the same for both the \\(X\\) and \\(Y\\) variables. In other words, it indicates the measures of the case j on the variables \\(X\\) and \\(Y\\).\n\nPearson’s r provides an estimate as to how many standard deviations (Z) from the sample mean on \\(Y\\) a case is, given how many standard deviations from the sample mean the case is on \\(X\\). More formally:\n\\[\\hat Z_{Y_j} = r_{XY}Z_{X_j}\\]\nwhere \\(\\hat Z_{Y_j}\\) is the estimated value of \\(Z_{Y_j}\\).\n\nFor instance, a person who is one-half of a standard deviation above the mean (\\(Z_X = 0.5\\)) in negative emotions, is estimated to be \\(\\hat{Z} = 0.578(0.5) = 0.289\\) standard deviations from the mean in his or her support for government action.\nThe sign of \\(\\hat{Z_Y}\\) is positive, meaning that this person is estimated to be above the sample mean (i.e., more supportive than average).\nSimilarly, someone who is two standard deviations below the mean (\\(Z_X = −2\\)) in negative emotions is estimated to be \\(Z_Y = 0.578(−2) = −1.156\\) standard deviations from the mean in support for government action.\nIn this case, \\(\\hat{Z_Y}\\) is negative, meaning that such a person is estimated to be below the sample mean in support for government action (i.e., less supportive than average).\n\nEstimates of Y from X are expectations extracted from what is known about the association between X and Y.\nIn statistics, expectations are never perfectly met, but we have a numerical means of gauging how close are to reality.\nIn our case, that gauge is the size of Pearson’s r. The closer it is to +/- 1, the more consistent those expectations are with the reality of the data."
  },
  {
    "objectID": "EDA.html#notes-on-the-glbwarm-data-set-and-excercises",
    "href": "EDA.html#notes-on-the-glbwarm-data-set-and-excercises",
    "title": "EDA",
    "section": "Notes on the GLBWARM data set and excercises",
    "text": "Notes on the GLBWARM data set and excercises\nGLBWARM is a data set collected from 815 residents of the United States (417 female, 398 male) who expressed a willingness to participate in online surveys in exchange for various incentives. The sampling procedure was designed such that the respondents roughly represent the U.S. population.\nThe data set contains a variable constructed from how each participant responded to five questions about the extent to which he or she supports various policies or actions by the U.S. government to mitigate the threat of global climate change.\nExamples include:\n\n“How much do you support or oppose increasing government investment for developing alternative energy like biofuels, wind, or solar by 25%?”\n“How much do you support or oppose creating a ‘Cap and Trade’ policy that limits greenhouse gases said to cause global warming?”\n\nResponse options were scaled from “Strongly opposed” (coded 1) or “Strongly support” (7), with intermediate labels to denote intermediate levels of support. An index of support for government action to reduce climate change was constructed for each person by averaging responses to the five questions (GOVACT in the data file).\nThe data set also contains a variable quantifying participants’ negative emotional responses to the prospect of climate change. This variable was constructed using participants’ responses to a question that asked them to indicate how frequently they feel each of three emotions when thinking about global warming: “worried,” “alarmed,” and “concerned.” Response options included “not at all,” “slightly,” “a little bit,” “some,” “a fair amount,” and “a great deal.” These responses were numerically coded 1 to 6, respectively, and each participant’s responses were averaged across all three emotions to produce a measure of negative emotions about climate change (NEGEMOT in the data file). This variable is scaled such that higher scores reflect feeling stronger negative emotions.\n\nExcercise 1\nDisplay the correlation between the variables negemot and govact using a scatterplot.\nBased on the plot, can you tell if there is a correlation between the two variables?\nIn your opinion, is there a strong correlation between these variables?\n\n\nExcercise 2\nUse the function cor() to calculate the correlation between the variables negemot and govact in the glbwarm data set.\nBased on the Pearsons’s r coefficient, is there a strong correlation between these variables?\n\n\nExcercise 3\nConsider the variables tnews and age in the glbwarm data set:\n\nVisualize the association using a scatterplot\nCalculate the Pearsons’s r correlation\nWrite a comment to interpret the results"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "regression.html#learning-objectives",
    "href": "regression.html#learning-objectives",
    "title": "Regression analysis",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nUnderstand the concept of statistical model\nUnderstand the concept of linear regression model\nKnow how to fit a linear regression model with R\nKnow how to interpret the results of linear regression models\nUnderstand the concept of goodness of fit and know how to evaluate it\nUnderstand the assumptions of the linear regression model and know how to evaluate them"
  },
  {
    "objectID": "conditional.html#learning-objectives",
    "href": "conditional.html#learning-objectives",
    "title": "Conditional process",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nLearning the concept of conditional process with particular attention to conditional direct and indirect effects, and relative conditional direct and indirect effects.\nLearn to fit conditional process models using PROCESS\nLearn to interpret the output of conditional process models\nAdditionally, we will learn how to test regression assumptions"
  },
  {
    "objectID": "conditional.html#conceptual-diagram",
    "href": "conditional.html#conceptual-diagram",
    "title": "Conditional process",
    "section": "Conceptual diagram",
    "text": "Conceptual diagram\nThe diagram of the conditional process is also a combination of mediation and moderation diagrams. Basically, starting from a mediation model, a conditional process model is created when one or more relations are conditioned by a moderator variable."
  },
  {
    "objectID": "conditional.html#types-of-conditional-process",
    "href": "conditional.html#types-of-conditional-process",
    "title": "Conditional process",
    "section": "Types of conditional process",
    "text": "Types of conditional process\nThere are three principal cases of conditional processes:\n\nModeration only applies to the direct effect of a mediation model. In this case we have a conditional direct effect.\nModeration only applies to the indirect effect. In this case we have a conditional indirect effect.\nModeration applies to both the direct and indirect effect. In this case we have both conditional direct and indirect effects\n\nGiven that conditional direct and indirect effects are a combination of mediation and moderation effects, it may be helpful to briefly summarize them in order to better understand the former."
  },
  {
    "objectID": "conditional.html#recap-of-direct-and-indirect-effects",
    "href": "conditional.html#recap-of-direct-and-indirect-effects",
    "title": "Conditional process",
    "section": "Recap of direct and indirect effects",
    "text": "Recap of direct and indirect effects\nA mediation model divides the total effect ( c ) of X on Y into a direct effect ( c’ ) and **indirect effect ( a*b )**, such that \\(c = c' + a*b\\).\n\n\n\n\n\nIn greater detail:\n\nThe direct effect (c’) quantifies the change in the estimate of Y for a one-unit change in X, partialling out the effect of other variables in the model (i.e., the mediator M, and other possible covariates).\nThe indirect effect quantifies the effect of X on Y through M, independent of the direct effect of X on Y and possible covariates. It is denoted as ab and results from the product of the coefficients a and b (indirect effect = \\(a * b\\)).\nThe total effect is the effect of X on Y in a model that does not include M. It is denoted as c. It is the sum of the direct and indirect effect: \\(c = c' + ab\\)."
  },
  {
    "objectID": "conditional.html#recap-of-conditional-effects",
    "href": "conditional.html#recap-of-conditional-effects",
    "title": "Conditional process",
    "section": "Recap of conditional effects",
    "text": "Recap of conditional effects\nIn a moderation model, the effect of X on Y is conditional on the values of a moderator W. This means that the effect of X on Y is expressed as a function, which we called theta (\\(\\theta\\)).\nGiven a simple moderation model, the conditional effect of X on Y is expressed as the regression coefficient of X plus the coefficient of the interaction term X*W, multiplied by the value of W.\n\n\n\n\n\nIn more detail:\nIn moderation models, the effect of X on Y is no more expressed by a single coefficient, but by a function. A function is just a mathematical “mechanism” that transforms inputs into outputs (for instance, multiplication is a function whose inputs are two numbers, the multiplicand and the multiplier, which are converted into a number called the “product”).\nThe conditional effect is a function whose output quantifies how the effect of the independent variable (X) on the dependent variable (Y) changes as the moderator increases by one unit. We have denoted this function with the Greek letter “theta”(\\(\\theta_{X \\to Y}\\)).\nPerhaps the best way to understand the conditional effect is to look at the regression equation.\nConsider a regression model with an independent X variable and a dependent Y variable that also includes a W moderator. The relationship between X and Y is moderated. This regression model includes the coefficient of X, that of W and the one of the interaction between X and W (XW).\n\\[\\hat Y = b_1X + b_2W + b_3XW\\]\nIn a regression model without moderation there is no interaction between X and W. The effect of X on Y is expressed by a single coefficient, that of X (\\(b_1X\\)). Instead, in a moderation model, the effect of X is expressed by both \\(b_1X\\) and the interaction term \\(b_3XW\\). To find the effect of X on Y we have to collect the terms involving X and then factoring out X.\n\\[\\hat Y = (b_1 + b_3W)X + b_2W\\]\nWe can see that the effect of X on Y is no more a number, as in a regression model without interaction, but a function: \\((b_1 + b_3W)\\).\n\\[\\theta_{X \\to Y} = b_1 + b_3W\\]\nThe output of the function depends on the value of W. For instance, consider \\(b_1\\) the coefficient of X = NEGEMOT = 0.120, and \\(b_3\\) the coefficient of the interaction term W = 0.006. By plugging these values into the function of the conditional effect, we can see that the output varies depending on the values of W (this procedure is also known as the pick-a-point method, which is used to probe the interaction). The output of the function is the conditional effect of X on Y:\n\\[\\theta_{X \\to Y} = 0.120 + 0.006W \\]\n\ntheta <- function(W){0.120 + 0.006*W}\nW <- c(30,50,70)\ndata.frame(\"Age\" = W, \"Conditional_Effect\" = theta(W))\n\n  Age Conditional_Effect\n1  30               0.30\n2  50               0.42\n3  70               0.54"
  },
  {
    "objectID": "conditional.html#conditional-direct-effect",
    "href": "conditional.html#conditional-direct-effect",
    "title": "Conditional process",
    "section": "Conditional direct effect",
    "text": "Conditional direct effect\nThe conceptual and statistical diagram for a model with only a conditional direct effect is the following.\n\n\n\n\n\nThe model comprises four variables, X, Y, M, and the moderator W. Like any simple mediation model, it consists of two equations, one for each outcome variable (M and Y).\n\\[M = i_M + aX + e_M\\] \\[Y = i_Y + c'_1X +  c'_2W +  c'_3XW + bM + e_Y\\]\nThe outcome of the first equation is M, which is predicted by X. The outcome of the second equation is Y, which is predicted by both X and M. Additionally, the relationship between X and Y is moderated by W, so the second model also includes the variable W and the interaction term XW.\nFor clarity, we can ignore the intercept \\(i_M\\) and the error term \\(e_Y\\).\nOnly the second equation comprises a conditional effect, because only that equation has a moderator. More precisely, moderator W moderates the direct effect between X and Y. This is therefore a conditional direct effect, which is now expressed by a function instead of the single coefficient \\(c'\\). We can find the effect by grouping terms in the equation involving X and then factoring out X:\n\\[\\hat Y = c'_1X +  c'_2W +  c'_3XW + bM\\] \\[\\hat Y = (c'_1 + c'_3W)X +  c'_2W + + bM\\]\nThe conditional direct effect of X on Y is represented by the function between round brackets:\n\\[\\theta_{X \\to Y} = c'_1 + c'_3W\\]\nIt quantifies how the effect of X on Y changes independent of the mediator M (which is kept constant) for a one-unit change of W."
  },
  {
    "objectID": "conditional.html#conditional-indirect-effect",
    "href": "conditional.html#conditional-indirect-effect",
    "title": "Conditional process",
    "section": "Conditional indirect effect",
    "text": "Conditional indirect effect\nThis is the conceptual and statistical diagram for a model with only a conditional indirect effect:\n\n\n\n\n\nThe model comprises four variables, X, Y, M, and the moderator W. Like a simple mediation model, it consists of two equations, one for each outcome variable (M and Y).\nThe outcome of the first equation is M, which is predicted by X. The outcome of the second equation is Y, which is predicted by both X and M. Additionally, the relationship between M and Y is moderated by W, so the second model also includes the variable W and the interaction term MW.\n\\[M = i_M + aX + e_M\\] \\[Y = i_Y + c'_1X + b_1M + b_2W + b_3MW + e_Y\\]\nFor clarity, we can ignore the intercept \\(i_M\\) and the error term \\(e_Y\\).\nOnly the second equation comprises a moderating effect and, consequently, a conditional effect. Moderator W moderates the relation between M and Y, so we can find this effect by grouping the M terms together.\n\\[\\hat Y = c'_1X + b_1M + b_2W + b_3MW\\] \\[\\hat Y = c'_1X + (b_1 + b_3W)M + b_2W\\]\nThe conditional effect is represented by the function within parentheses:\n\\[\\theta_{M \\to Y} = b_1 + b_3W\\]\nThis conditional indirect effect quantifies how differences in X map onto differences in Y indirectly through M depending on the value of W.\nThe moderated relation contributes to define the indirect effect. In a simple mediation model, the indirect effect is defined as the product between the coefficients a and b \\(ab = a * b\\). In this conditional process model, b is no more a single coefficient but a function, i.e., the function defining the conditional effect. It follows that the conditional indirect effect is the product between a and the conditional effect \\(\\theta_{M \\to Y} = b_1 + b_3W\\), and its value is conditional on W:\n\\[a(b_1 + b_3W) = ab_1 + a_b3W\\]\n\n\n\n\n\nThe following is the conceptual and statistical diagram for a model with a conditional direct and indirect effect:\n\n\n\n\n\nThe model comprises five variables, X, Y, M, and the moderators W and Z. Like a simple mediation model, it consists of two equations, one for each outcome variable (M and Y).\nThe outcome of the first equation is M, which is predicted by X. Additionally, W moderates the relation between X and M, so the model includes the coefficient for W and the interaction between X and W.\n\\[M = i_M + a_1X + a_2W + a_3XW + e_M\\]\nThe outcome of the second equation is Y, which is predicted by both X and M. Additionally, the relationship between M and Y is moderated by Z, and so it is the relationship between X and Y. SO the model includes the coefficient for Z, and those for the interaction between X and Z (XZ), and M and Z (MZ):\n\\[Y = i_Y + c'_1X + c'_2Z + c'_3XZ + b_1M + b_2MZ + e_Y\\]\nFor clarity, we can ignore intercepts and error terms.\nAs we have interactions in both equations, they both include conditional effects. More precisely, the first equation includes a conditional indirect effect, and the second a conditional direct and indirect effect.\nStarting from the first equation, we can find the conditional indirect effects by grouping X terms and factoring out X:\n\\[\\hat M = a_1X + a_2W + a_3XW\\] \\[\\hat M = (a_1 + a_3W)X + a_2W\\] \\[\\theta_{X \\to M} = a_1 + a_3W\\]\nConsidering the second equation, we can find the conditional indirect effects by grouping the X terms and factoring out X, and the conditional indirect effects by grouping the M terms and factoring out M:\n\\[Y = c'_1X + c'_2Z + c'_3XZ + b_1M + b_2MZ\\] \\[Y = (c'_1 + c'_3Z)X + (b_1 + b_2Z)M +  + c'_2Z\\]\nThe two conditional effects are within parentheses. The conditional direct effect and and the conditional indirect effect is the former (\\(\\theta_{X \\to Y}\\)) and the latter (\\(\\theta_{M \\to Y}\\)), respectively:\n\\[\\theta_{X \\to Y} = c'_1 + c'_3Z\\] \\[\\theta_{M \\to Y} = b_1 + b_2Z\\]\nAs previously explained, given a mediation model \\(X \\to M \\to Y\\) with coefficient a (path from X to M) and b (path from M to Y), the indirect effect of X on Y is the product between a and b \\(ab = a * b\\). The conditional indirect effect is the product of the same paths where one or both are dependent on a moderator and are therefore represented by conditional effects. In our case, both the paths are conditioned on a moderator (respectively W and Z), so the indirect effect is the product of these conditional effects:\n\\[\\theta_{X \\to M} * \\theta_{M \\to Y}\\] \\[(a_1 + a_3W) * (b_1 + b_2Z)\\] \\[a_1b_1 + a_1b_2Z + a_3b_1W + a_3b_2WZ\\]"
  },
  {
    "objectID": "conditional.html#example",
    "href": "conditional.html#example",
    "title": "Conditional process",
    "section": "Example",
    "text": "Example\nIn the following example we’ll use the dataset TEAMS (available on Moodle) to fit a conditional indirect effect model.\n\nteams <- haven::read_sav(\"data/teams.sav\")\n\nThe data comes from a study on work teams. The goal was to examine the mechanism by which the dysfunctional behavior of members of a work team (DYSFUNC = X) can negatively affect the ability of a work team to perform well (PERFORM = Y). The researchers proposed a mediation model in which dysfunctional behavior (X) leads to a work environment filled with negative emotions (NEGTONE = M) that supervisors and other employees confront and attempt to manage, which then distracts from work and interferes with task performance (Y).\nHowever, according to the authors’ model, when team members are able to regulate their display of negative emotions (NEGEXP = W), essentially hiding how they are feeling from others, this allows the team to stay focused on the task at hand rather than having to shift focus toward managing the negative tone of the work environment and the feelings of others.\nThat is, the effect of negative affective tone of the work environment on team performance is hypothesized in their model as contingent on the ability of the team members to hide their feelings from the team, with a stronger negative effect of negative affective tone on performance in teams that express their negativity rather than conceal it."
  },
  {
    "objectID": "conditional.html#conditional-indirect-effect-1",
    "href": "conditional.html#conditional-indirect-effect-1",
    "title": "Conditional process",
    "section": "Conditional indirect effect",
    "text": "Conditional indirect effect\nLooking at the two regression tables, the model can be written as:\n\\[M = i_M + aX\\] \\[Y = i_Y + c'X + b_1M + b_2W + b_3MW + e_Y\\]\n\n\n\n\n\nIn a simple mediation model, the conditional indirect effect is the product between a (the path from X to M) and b (the path from M to Y). Here a corresponds to \\(a\\) and b to \\(b_1\\).\nWhereas the path \\(X \\to M\\) is not moderated and therefore a is a simple coefficient, the path \\(M \\to Y\\) is moderated by W and therefore \\(b_1\\) is a conditional effect expressed through a function, we can denote as \\(\\theta_{M \\to Y}\\).\n\n\n\n\n\nWe can find the conditional effect by grouping the M terms and factoring out M (we ignore the intercept and error term).\n\\[\\hat Y = c'X + b_1M + b_2W + b_3MW\\] \\[\\hat Y = c'X + (b_1 + b_3W)M + b_2W\\] \\[\\theta_{M \\to Y} = b_1 + b_3W\\]\n\nConditional effect\nThe conditional effect \\(\\theta_{M \\to Y} = b_1 + b_3W\\) represents the moderation effect. As in a simple moderation model, it is analyzed in the second part of the output. Here you will find the table Conditional effects of the focal predictor at values of the moderator(s) (resulting from the pick-a-point approach), and the table Johnson-Neyman significance regions.\nConsider the table Conditional effects of the focal predictor at values of the moderator(s). The function just identified for the conditional effect can be used to test the effect of M on Y at different levels of the moderator W. For example, at relatively low (-0.53), moderate (-0.06), and relatively high (0.60) level of the moderator, we obtain:\n\ncond_ind_effect <- function(W) { -0.44 + -0.52*W }\nmoderator_values <- c(-0.53, -0.06, 0.60)\nsapply(moderator_values, cond_ind_effect)\n\n[1] -0.1644 -0.4088 -0.7520\n\n\n\n\n\n\n\n\n\nConditional indirect effect\nWe can now find the conditional indirect effect by multiplying the conditional effect \\(\\theta_{M \\to Y}\\) by \\(a\\). Here \\(a\\) is the coefficient of DYSFUNC (first regression table, DYSFUNC = 0.62), \\(b_1\\) is the coefficient of NEGTONE (\\(M \\to Y\\)) (second regression table NEGTONE = -0.44), and \\(b_3\\) is the coefficient of the interaction term (second regression table, Int_1 = -0.52).\n\\[a\\theta_{M \\to Y}\\] \\[a(b_1 + b_3W)\\] \\[0.62(-0.44 + -0.52W)\\]\nThe conditional indirect effect is analyzed through a pick-a-point approach: various values of the moderator W are plugged into the function of the conditional indirect effect (\\(0.62(-0.44 + -0.52W)\\)). A significance test is subsequently conducted. The results are presented in the last section of the output. The table Conditional indirect effects of X on Y shows the changes in the Y estimates for a one-unit change in X through the mediator M at different levels of the moderator (Using Hayes’ own words: “the conditional indirect effect of X on Y through M conditioned on W quantifies the amount by which two cases with a given value of W that differ by one unit on X are estimated to differ on Y indirectly through X’s effect on M, which in turn influences Y.”).\n\n\n\n\n\nIn practice, the table can be read as follows: “at relatively low levels of emotional expressiveness (the moderator) there is no evidence of an effect of dysfunctional behavior on performance through negative emotional tone (-0.53, 95% CI [-0.37, 0.24]). Instead, at moderate (-0.06, 95% CI [-0.49, 0.04]) to relatively high (0.60, 95% CI [-0.81, -0.15]) levels of emotional expressivity, the effect is statistically significant, with higher levels of emotional expressiveness leading to worse team performance.\nThe Index of moderated mediation is displayed right below the pick-a-point table. It provides the results of a more general significance test of the relationship between the moderator and the indirect effect. When zero is not within the range of the confidence interval, it can be concluded that the indirect effect is associated with the moderator. Had the confidence interval included zero, we could not definitively claim that the indirect effect was related to the moderator.\nIn this case the “index” is negative, which leads to the conclusion that the indirect effect is negatively related to the moderator. That is, the mediation of the effect of dysfunctional team behavior on performance through negative affective tone of the work climate is moderated by the expressivity of the team.\nThe Index of moderated mediation is displayed if W is specified as a moderator of only one of the paths that define the indirect effect. Otherwise, the indirect effect will be a nonlinear function of W. In that case, PROCESS will not produce this index.\nIn the section of the output there is also the direct effect of X on Y. In this case it is not affected by the moderator and can be interpreted as in a simple mediation model."
  },
  {
    "objectID": "conditional.html#brief-recap-on-the-intepretation-of-multicategorical-variables",
    "href": "conditional.html#brief-recap-on-the-intepretation-of-multicategorical-variables",
    "title": "Conditional process",
    "section": "Brief recap on the intepretation of multicategorical variables",
    "text": "Brief recap on the intepretation of multicategorical variables\nThere is only one fundamental principle to be taken into account in the interpretation of multicategorical models. The g categories of a multicategorical variable are divided into g-1 dichotomous variables and a reference category. The newly created dichotomous variables represent differences between a category and the reference category.\nIn this case, the multicategorical variable PROTEST includes g=3 categories (X=0, X=1, X=2), which are divided into g-1 = 2 dichotomous variables (X1: X=1, and X2: X=2) and a reference category (X0: X=0). The newly created dichotomous variables X1 and X2 represent differences (in some estimates) between a category and the reference category (X1 = X1 - X0; X2 = X2 - X0).\n\nknitr::include_graphics(\"img/table_cond_proc_multicat.png\")\n\n\n\n\nFor example, consider the interaction term:\n\nIn a model with continuous X, the interaction terms (Int) would quantify changes in the conditional effect of X on Y for a one-unit change in X.\nIn a model with dichotomous X, one-unit change in X means switching from X=0 to X=1, and the interaction terms would quantify the difference between X1 and X0 in their conditional effect on Y.\nWe know that a multicategorical variable is treated as g-1 dichotomous ones, therfore the interpretation is similar to that of dichotomous variables. In this case, the interaction term Int_1 quantifies the difference between X1 and X0 in their conditional effect on Y, and Int_2 quantifies the difference between X2 and X0 in their conditional effect on Y. Therefore, they are relative conditional effects."
  },
  {
    "objectID": "conditional.html#check-the-assumptions",
    "href": "conditional.html#check-the-assumptions",
    "title": "Conditional process",
    "section": "Check the assumptions",
    "text": "Check the assumptions\nWe can test the assumptions of our models based on what we learned in the first part of the course. There, we learned how to fit multiple regression models in R using the lm function. We also learned how to test the assumptions by visually inspecting some plots.\nThe procedure includes the following steps:\n\nFit your mediation, moderation, or conditional process model using PROCESS.\nCheck the regression tables and fit the same models in R, using the lm function.\nUse R to check the assumptions.\nTake possible steps to address potential problems related to assumptions."
  },
  {
    "objectID": "conditional.html#check-regression-assumptions",
    "href": "conditional.html#check-regression-assumptions",
    "title": "Conditional process",
    "section": "Check regression assumptions",
    "text": "Check regression assumptions\nWe illustrate the method by a conditional process model that includes both mediating and moderating effects. Therefore, it is representative of any cases you may come across.\n\nSTEP 1: Fit the model with PROCESS\nAs a first step, let’s fit the model in PROCESS. We can omit jn and plot options which are not relevant in this case.\n\nprocess(y = \"perform\", x = \"dysfunc\", m = \"negtone\", w = \"negexp\",\n        model = 14, \n        decimals = 10.5,\n        seed=42517,\n        boot = 5000, # number of bootstraps \n        progress = 0,\n        data = teams)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 14     \n    Y : perform\n    X : dysfunc\n    M : negtone\n    W : negexp \n\nSample size: 60\n\nCustom seed: 42517\n\n\n*********************************************************************** \nOutcome Variable: negtone\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n     0.43841    0.19220    0.22683   13.79991    1.00000   58.00000    0.00046\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant    0.02568    0.06176    0.41585    0.67905   -0.09794    0.14930\ndysfunc     0.61975    0.16683    3.71482    0.00046    0.28580    0.95371\n\n*********************************************************************** \nOutcome Variable: perform\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n     0.55856    0.31199    0.20146    6.23504    4.00000   55.00000    0.00033\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant   -0.01188    0.05853   -0.20293    0.83994   -0.12918    0.10542\ndysfunc     0.36606    0.17782    2.05854    0.04429    0.00969    0.72243\nnegtone    -0.43575    0.13055   -3.33774    0.00152   -0.69738   -0.17412\nnegexp     -0.01918    0.11741   -0.16340    0.87080   -0.25448    0.21611\nInt_1      -0.51697    0.24092   -2.14576    0.03632   -0.99979   -0.03414\n\nProduct terms key:\nInt_1  :  negtone  x  negexp      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nM*W    0.05760    4.60429    1.00000   55.00000    0.03632\n----------\nFocal predictor: negtone (M)\n      Moderator: negexp (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n      negexp     effect         se          t          p       LLCI       ULCI\n    -0.53080   -0.16134    0.20875   -0.77290    0.44289   -0.57969    0.25701\n    -0.06000   -0.40473    0.13566   -2.98344    0.00424   -0.67660   -0.13286\n     0.60000   -0.74593    0.16259   -4.58792    0.00003   -1.07176   -0.42010\n\n*********************************************************************** \nBootstrapping in progress. Please wait.\n\n**************** DIRECT AND INDIRECT EFFECTS OF X ON Y ****************\n\nDirect effect of X on Y:\n      effect         se          t          p       LLCI       ULCI\n     0.36606    0.17782    2.05854    0.04429    0.00969    0.72243\n\nConditional indirect effects of X on Y:\n\nINDIRECT EFFECT:\n\ndysfunc    ->    negtone    ->    perform\n\n      negexp     Effect     BootSE   BootLLCI   BootULCI\n    -0.53080   -0.09999    0.14961   -0.37028    0.23944\n    -0.06000   -0.25083    0.11861   -0.49280   -0.03543\n     0.60000   -0.46229    0.17057   -0.80677   -0.14881\n\n     Index of moderated mediation:\n            Index     BootSE   BootLLCI   BootULCI\nnegexp   -0.32039    0.18907   -0.76118   -0.04209\n\n---\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000\n\nW values in conditional tables are the 16th, 50th, and 84th percentiles.\n\n\n\n\nSTEP 2: Check the regression tables and fit the models in R\nThe results consists of two regression tables, one for each dependent variable. The former is the model of “negtone” and the latter of “perform”. So we’re talking about two regression models.\n\nknitr::include_graphics(\"img/check_assumptions_1.png\")\n\n\n\n\nWe fit exactly the same models using R, plugging the variables shown in each table in the lm function. Note that the second model is moderated, thus includes a interaction term: Int_1. From the “Product term key” table you can see that the interaction is between the variables “negtone” and “negexp”: “negtone x negexp”. To create this interaction term into lm, you just put the variables into the function using the multiplication symbol \\(*\\): \\(negtone * negexp\\).\n\nm1 <- lm(negtone ~ dysfunc, data = teams)\nm2 <- lm(perform ~ dysfunc + negtone + negexp + negtone * negexp, data = teams)\n\nCompare the outputs to be sure that the models are exactly the same:\n\nsummary(m1)\n\n\nCall:\nlm(formula = negtone ~ dysfunc, data = teams)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.99459 -0.26346 -0.06103  0.23478  1.59585 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.02568    0.06176   0.416 0.679053    \ndysfunc      0.61975    0.16683   3.715 0.000459 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4763 on 58 degrees of freedom\nMultiple R-squared:  0.1922,    Adjusted R-squared:  0.1783 \nF-statistic:  13.8 on 1 and 58 DF,  p-value: 0.000459\n\n\n\nsummary(m2)\n\n\nCall:\nlm(formula = perform ~ dysfunc + negtone + negexp + negtone * \n    negexp, data = teams)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.03038 -0.31679 -0.00591  0.26235  0.98591 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    -0.01188    0.05853  -0.203  0.83994   \ndysfunc         0.36606    0.17782   2.059  0.04429 * \nnegtone        -0.43575    0.13055  -3.338  0.00152 **\nnegexp         -0.01918    0.11741  -0.163  0.87080   \nnegtone:negexp -0.51697    0.24092  -2.146  0.03632 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4488 on 55 degrees of freedom\nMultiple R-squared:  0.312, Adjusted R-squared:  0.2619 \nF-statistic: 6.235 on 4 and 55 DF,  p-value: 0.0003276\n\n\n\n\nSTEP 3: Use R to check the assumptions.\nWe start with the plot function to visually check if the assumptions are respected for both models.\n\npar(mfrow=c(2,2))\nplot(m1)\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(m2)\n\n\n\n\nWe can observe that the red line is roughly horizontal in the two plots to the left, which is good (linearity assumption).\nThe points are also more or less randomly distributed around the red line (homoskedasticity assumption), which is fine. In case of doubt, a formal test of homoskedasticity is the Breusch-Pagan Test. It is implemented in the function bptest of the library lmtest. As long as the test is not statistically significant, no obvious heteroskedastic problems are present.\n\nlibrary(lmtest)\n\n\nbptest(m1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  m1\nBP = 2.6223, df = 1, p-value = 0.1054\n\nbptest(m2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  m2\nBP = 4.491, df = 4, p-value = 0.3436\n\n\nThe graph in the upper right panel shows points approximately on a diagonal line, which is also good (normality hypothesis). In case of dubts, a formal test of normality is the Shapiro-Wilk’s test. It needs to be applied to the residuals of the model. Normality is presumed unless the test is statistically significant.\n\nshapiro.test(m1$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  m1$residuals\nW = 0.95575, p-value = 0.02927\n\nshapiro.test(m2$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  m2$residuals\nW = 0.98974, p-value = 0.896\n\n\nThe bottom right chart is mainly used to verify the presence of outliers. They are “influential observations” whose presence significantly affects the model results. They can be found off the dashed lines (undetected, which is fine).\nIndependence of observations is mainly a problem of time series data. In case of doubt, a formal test of this assumption is the Durbin-Watson test. It is implemented in the function dwtest of the lmtest package.The assumption is satisfied if the Durbin-Watson statistic (DW) is about equal to 2, and the test is statistically non-significant.\n\nlmtest::dwtest(m1)\n\n\n    Durbin-Watson test\n\ndata:  m1\nDW = 1.8549, p-value = 0.2854\nalternative hypothesis: true autocorrelation is greater than 0\n\nlmtest::dwtest(m2)\n\n\n    Durbin-Watson test\n\ndata:  m2\nDW = 1.9281, p-value = 0.3762\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nNo multicollinearity applies only to regression models with more than one independent variable (i.e. multiple regression models). It can be formally verified by calculating the Pearson correlation coefficient between the independent variables and the index VIF (“Variance Inflation Factor”). The correlation should include all the independent variables, including the interaction term, if present. The interaction term needs to be calculated manually (this is only the product between the variables). For the assumption to be met the correlation has not to be too high and the VIF scores have to be well below 10 (e.g., in a recent publication it can be read that “None of the correlations among the independent variables exceeded r=|0.58| while the largest variance inflation factor (VIF) was VIF=1.60, indicating that multicollinearity does not limit the regression model”).\n\n# just keep the independent variables \ndat <- teams[, c(\"dysfunc\", \"negtone\", \"negexp\")]\n# since we have an interaction term, we have to compute also this interaction\ndat$negtone_x_negexp <- dat$negtone * dat$negexp\n# Pearsons' correlation\ncor(dat)\n\n                      dysfunc   negtone       negexp negtone_x_negexp\ndysfunc           1.000000000 0.4384057 -0.006841573      -0.03631275\nnegtone           0.438405680 1.0000000  0.084626603       0.26881696\nnegexp           -0.006841573 0.0846266  1.000000000       0.40127021\nnegtone_x_negexp -0.036312754 0.2688170  0.401270214       1.00000000\n\n\n\ncar::vif(m2)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n       dysfunc        negtone         negexp negtone:negexp \n      1.279174       1.377851       1.193402       1.315798 \n\n\n\n\nSTEP 4: Take possible steps to address potential problems related to assumptions.\nIssues with linearity may be fixed through transformations (e.g., log-transforming the variables).\nOutliers have to be thoroughly inspected. If in doubt, you can perform robust regression in R and check whether the results are more or less the same as those of PROCESS. Robust regression attempts to dampen the influence of outlying cases in order to provide a better fit to the majority of the data. A good practical tutorial on robust regression is available here: https://www.youtube.com/watch?v=qte9ASvgElI.\nIn case of serious multicollinearity problems (based on the VIF index) you can omit one of the highly correlated variables.\nFinally, in case of hetersoskedasticity problems you can request PROCESS to use heteroskedasticity-consistent (HC) standard errors. The available options are: HC0, HC1, HC2, HC3, and HC4. You only need to add hc=4 (or a different number between 1 and 4) to the PROCESS function. Heteroskedasticity-consistent (HC) standard errors were also discussed in the first part of the course on regression models. Models with both standard errors and heteroskedasticity-consistent standard errors can be fitted for comparison even if hetersoskedasticity issues are not obvious.\nHayes and Cay wrote: >“(…) we argue that an HC estimator, preferably HC3 or HC4, should be routinely used in linear regression mod- els, if it is not used as the default method of standard error estimation, researchers would be well advised to at least double-check the results from the use of the OLSE estima- tor against the results obtained with an HC estimator, to make sure that conclusions are not compromised by heteroskedasticity”. Hayes, A. F., & Cai, L. (2007). Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722.\nTo treat normality problems, bootstrapping can be used to compute confidence intervals for regression coefficients. You only have to add modelbt = 1 to the PROCESS function. The coefficients with the bootstrap confidence intervals will appear at the bottom of the result. They can be interpreted as usual: if the range overlaps zero, there is not enough evidence to consider it statistically significant.\nThese confidence intervals are not the same as those reported in the regression tables at the beginning of the output. The latter is based on hypotheses of normality, and are symmetrical around the coefficient. The percentile bootstrap confidence intervals are rather “asymmetric”. They are not based on a normal theoretical distribution, but of the empirical distribution as given in the sample (which is almost never perfectly normal).\n\nprocess(y = \"perform\", x = \"dysfunc\", m = \"negtone\", w = \"negexp\",\n        model = 14, \n        jn = 1, \n        plot = 1,\n        decimals = 10.5,\n        seed = 42517,\n        hc = 4, # heteroskedasticity-consistent standard errors hc4\n        modelbt = 1, # bootstrap CI for regression coefficients \n        boot = 5000, \n        data = teams)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 14     \n    Y : perform\n    X : dysfunc\n    M : negtone\n    W : negexp \n\nSample size: 60\n\nCustom seed: 42517\n\n\n*********************************************************************** \nOutcome Variable: negtone\n\nModel Summary: \n           R       R-sq        MSE     F(HC4)        df1        df2          p\n     0.43841    0.19220    0.22683    5.29928    1.00000   58.00000    0.02494\n\nModel: \n              coeff    se(HC4)          t          p       LLCI       ULCI\nconstant    0.02568    0.06129    0.41899    0.67677   -0.09701    0.14838\ndysfunc     0.61975    0.26922    2.30202    0.02494    0.08084    1.15866\n\n*********************************************************************** \nOutcome Variable: perform\n\nModel Summary: \n           R       R-sq        MSE     F(HC4)        df1        df2          p\n     0.55856    0.31199    0.20146    3.49092    4.00000   55.00000    0.01304\n\nModel: \n              coeff    se(HC4)          t          p       LLCI       ULCI\nconstant   -0.01188    0.05975   -0.19879    0.84316   -0.13162    0.10787\ndysfunc     0.36606    0.23092    1.58522    0.11865   -0.09672    0.82883\nnegtone    -0.43575    0.15693   -2.77669    0.00749   -0.75025   -0.12125\nnegexp     -0.01918    0.11244   -0.17062    0.86515   -0.24453    0.20616\nInt_1      -0.51697    0.32049   -1.61303    0.11246   -1.15926    0.12532\n\nProduct terms key:\nInt_1  :  negtone  x  negexp      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng     F(HC4)        df1        df2          p\nM*W    0.05760    2.60186    1.00000   55.00000    0.11246\n----------\nFocal predictor: negtone (M)\n      Moderator: negexp (W)\n\nData for visualizing the conditional effect of the focal predictor:\n     negtone     negexp    perform\n    -0.45000   -0.53080    0.08360\n    -0.03500   -0.53080    0.01664\n     0.52240   -0.53080   -0.07329\n    -0.45000   -0.06000    0.18409\n    -0.03500   -0.06000    0.01613\n     0.52240   -0.06000   -0.20947\n    -0.45000    0.60000    0.32497\n    -0.03500    0.60000    0.01541\n     0.52240    0.60000   -0.40037\n\n*********************************************************************** \nBootstrapping progress:\n\n  |                                                                    \n  |                                                              |   0%\n  |                                                                    \n  |                                                              |   1%\n  |                                                                    \n  |>                                                             |   1%\n  |                                                                    \n  |>                                                             |   2%\n  |                                                                    \n  |>>                                                            |   2%\n  |                                                                    \n  |>>                                                            |   3%\n  |                                                                    \n  |>>                                                            |   4%\n  |                                                                    \n  |>>>                                                           |   4%\n  |                                                                    \n  |>>>                                                           |   5%\n  |                                                                    \n  |>>>                                                           |   6%\n  |                                                                    \n  |>>>>                                                          |   6%\n  |                                                                    \n  |>>>>                                                          |   7%\n  |                                                                    \n  |>>>>>                                                         |   7%\n  |                                                                    \n  |>>>>>                                                         |   8%\n  |                                                                    \n  |>>>>>                                                         |   9%\n  |                                                                    \n  |>>>>>>                                                        |   9%\n  |                                                                    \n  |>>>>>>                                                        |  10%\n  |                                                                    \n  |>>>>>>>                                                       |  10%\n  |                                                                    \n  |>>>>>>>                                                       |  11%\n  |                                                                    \n  |>>>>>>>                                                       |  12%\n  |                                                                    \n  |>>>>>>>>                                                      |  12%\n  |                                                                    \n  |>>>>>>>>                                                      |  13%\n  |                                                                    \n  |>>>>>>>>                                                      |  14%\n  |                                                                    \n  |>>>>>>>>>                                                     |  14%\n  |                                                                    \n  |>>>>>>>>>                                                     |  15%\n  |                                                                    \n  |>>>>>>>>>>                                                    |  15%\n  |                                                                    \n  |>>>>>>>>>>                                                    |  16%\n  |                                                                    \n  |>>>>>>>>>>                                                    |  17%\n  |                                                                    \n  |>>>>>>>>>>>                                                   |  17%\n  |                                                                    \n  |>>>>>>>>>>>                                                   |  18%\n  |                                                                    \n  |>>>>>>>>>>>                                                   |  19%\n  |                                                                    \n  |>>>>>>>>>>>>                                                  |  19%\n  |                                                                    \n  |>>>>>>>>>>>>                                                  |  20%\n  |                                                                    \n  |>>>>>>>>>>>>>                                                 |  20%\n  |                                                                    \n  |>>>>>>>>>>>>>                                                 |  21%\n  |                                                                    \n  |>>>>>>>>>>>>>                                                 |  22%\n  |                                                                    \n  |>>>>>>>>>>>>>>                                                |  22%\n  |                                                                    \n  |>>>>>>>>>>>>>>                                                |  23%\n  |                                                                    \n  |>>>>>>>>>>>>>>>                                               |  23%\n  |                                                                    \n  |>>>>>>>>>>>>>>>                                               |  24%\n  |                                                                    \n  |>>>>>>>>>>>>>>>                                               |  25%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>                                              |  25%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>                                              |  26%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>                                              |  27%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>                                             |  27%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>                                             |  28%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>                                            |  28%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>                                            |  29%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>                                            |  30%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>                                           |  30%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>                                           |  31%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>                                          |  31%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>                                          |  32%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>                                          |  33%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>                                         |  33%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>                                         |  34%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>                                         |  35%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>                                        |  35%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>                                        |  36%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>                                       |  36%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>                                       |  37%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>                                       |  38%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>                                      |  38%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>                                      |  39%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>                                      |  40%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>                                     |  40%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>                                     |  41%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>                                    |  41%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>                                    |  42%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>                                    |  43%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>                                   |  43%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>                                   |  44%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                  |  44%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                  |  45%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                  |  46%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                 |  46%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                 |  47%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                 |  48%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                |  48%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                                |  49%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                               |  49%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                               |  50%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                               |  51%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                              |  51%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                              |  52%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                             |  52%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                             |  53%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                             |  54%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                            |  54%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                            |  55%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                            |  56%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                           |  56%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                           |  57%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                          |  57%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                          |  58%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                          |  59%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                         |  59%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                         |  60%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                        |  60%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                        |  61%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                        |  62%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                       |  62%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                       |  63%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                       |  64%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                      |  64%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                      |  65%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                     |  65%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                     |  66%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                     |  67%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                    |  67%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                    |  68%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                    |  69%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                   |  69%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                   |  70%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                  |  70%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                  |  71%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                  |  72%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                 |  72%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                 |  73%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                |  73%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                |  74%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                |  75%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>               |  75%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>               |  76%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>               |  77%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>              |  77%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>              |  78%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>             |  78%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>             |  79%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>             |  80%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>            |  80%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>            |  81%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>           |  81%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>           |  82%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>           |  83%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>          |  83%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>          |  84%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>          |  85%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>         |  85%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>         |  86%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>        |  86%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>        |  87%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>        |  88%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>       |  88%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>       |  89%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>       |  90%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      |  90%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      |  91%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>     |  91%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>     |  92%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>     |  93%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    |  93%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    |  94%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   |  94%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   |  95%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>   |  96%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |  96%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |  97%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  |  98%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> |  98%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> |  99%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>|  99%\n  |                                                                    \n  |>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>| 100%\n\n**************** DIRECT AND INDIRECT EFFECTS OF X ON Y ****************\n\nDirect effect of X on Y:\n      effect    se(HC4)          t          p       LLCI       ULCI\n     0.36606    0.23092    1.58522    0.11865   -0.09672    0.82883\n\nConditional indirect effects of X on Y:\n\nINDIRECT EFFECT:\n\ndysfunc    ->    negtone    ->    perform\n\n      negexp     Effect     BootSE   BootLLCI   BootULCI\n    -0.53080   -0.09999    0.14961   -0.37028    0.23944\n    -0.06000   -0.25083    0.11861   -0.49280   -0.03543\n     0.60000   -0.46229    0.17057   -0.80677   -0.14881\n\n     Index of moderated mediation:\n            Index     BootSE   BootLLCI   BootULCI\nnegexp   -0.32039    0.18907   -0.76118   -0.04209\n\n---\n\n********** BOOTSTRAP RESULTS FOR REGRESSION MODEL PARAMETERS **********\n\nOutcome variable: negtone\n\n              Coeff   BootMean     BootSE   BootLLCI   BootULCI\nconstant    0.02568    0.02816    0.05982   -0.08350    0.14707\ndysfunc     0.61975    0.63196    0.22578    0.21552    1.09547\n----------\nOutcome variable: perform\n\n              Coeff   BootMean     BootSE   BootLLCI   BootULCI\nconstant   -0.01188   -0.01190    0.05835   -0.12698    0.10469\ndysfunc     0.36606    0.32286    0.19229   -0.09397    0.66792\nnegtone    -0.43575   -0.42239    0.13201   -0.65435   -0.12768\nnegexp     -0.01918   -0.02401    0.10235   -0.22884    0.17457\nInt_1      -0.51697   -0.53510    0.24224   -1.06997   -0.10944\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nNumber of bootstraps for percentile bootstrap confidence intervals: 5000\n\nW values in conditional tables are the 16th, 50th, and 84th percentiles.\n \nNOTE: A heteroscedasticity consistent standard error and covariance matrix estimator was used."
  },
  {
    "objectID": "conditional.html#custom-models",
    "href": "conditional.html#custom-models",
    "title": "Conditional process",
    "section": "Custom models",
    "text": "Custom models\nPROCESS implements several pre-programmed models to perform a wide range of common analyses. Advanced users can customize templates and even build new templates with more complex relationships. The details are given in the appendix of Hayes’ book."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a data analysis seminar focused on the application of linear regression analysis to explore questions about mediated and moderated effects.\nComputer applications will focus on R statistical language and the Rstudio environment (https://www.rstudio.com) and the PROCESS software by Andrew F. Hayes (http://processmacro.org).\nThe course is subdivided in 5 learning units.\nThe introductory part of the course will be dedicated to installing the data science tools R and Rstudio (https://www.rstudio.com) and to learning the basic principles necessary to work on statistical analysis with this software. Students will be required to acquire a basic familiarity with the Rstudio interface and R programming in a short period of time, in order to be able to use it for performing statistical analysis. At home study and exercise will be fundamental to reaching the objective.\nThe second part of the course will be dedicated to presenting the principles of linear regression analysis, and explaining how to fit, visualize, interpret, and evaluate regression models in R.\nWe’ll use linear regression models to answer questions of the WHETHER or IF variety. These are questions about association between variables. For example, is there a relationship between a variable Y, and another variable X, and possibly other ones? For example, we can use this kind of analysis to study whether polarization increases as the exposure to partisan news media increases.\nThe third unit will be dedicated to **mediation analysis**, and explaining how to fit, visualize, interpret, and evaluate mediation models using PROCESS in the R environment.\nMediation analysis answers questions of the **HOW** variety, and it is used to study the mechanism(s) by which one variable impacts another one.\nFor example, let’s say that we have established a relationship between the use of violent video games and the frequency of off-line assaults. The next step may be to use mediation analysis to analyze the potential mechanism by which violent video games impact offline aggressive behavior.\n**UNIT 4**: The fourth learning unit will be dedicated to **moderation analysis**, and explaining how to fit, visualize, interpret, and evaluate moderation models using PROCESS in the R environment.\nModeration analysis is used to answer questions of the **WHEN** variety, that is to say, to study the conditions or contingencies of an association.\nFor example, there is no reason to think that the association between violent video games and offline aggression is universal. Maybe, it is stronger for males than females, or the strength of the association is a function of personality traits. We use moderation analysis to answer these questions.\n**UNIT 5**: The fifth learning unit will be dedicated to **conditional process analysis**, and explaining how to fit, visualize, interpret, and evaluate conditional process models using PROCESS in the R environment.\nConditional process represents the conditions of mechanisms by which a variable impact on another one. It includes both mediation and moderation analysis.\nConditional process models allow to address the complexity of social science phenomena."
  },
  {
    "objectID": "introduction.html#learning-outcomes",
    "href": "introduction.html#learning-outcomes",
    "title": "Introduction",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nBy the end of this course, participants will be able to:\nRun and interpret the results of linear regression, moderation, mediation, and conditional process models.\nKnow how to test competing theories of mechanisms statistically through the comparison of indirect effects in models with multiple mediators.\nKnow how to visualize and probe interactions in regression models in order to interpret interaction effects in the appropriate ways.\nKnow how to estimate the contingencies of mechanisms through the computation and inference about conditional indirect effects.\nKnow how to R language and PROCESS to run, visualize, and understand linear regression, moderation, mediation, and conditional process models."
  },
  {
    "objectID": "introduction.html#assessment",
    "href": "introduction.html#assessment",
    "title": "Introduction",
    "section": "Assessment",
    "text": "Assessment\nThe assessment includes two online quizzes to assess the understanding of key concepts. The results of each questionnaire will constitute 50% of the final grade.\nThe evaluation also includes a final data analysis project to be conducted in small groups during the course. This constitutes a total of 50% of the final grade.\nAttendance is mandatory and participation positively evaluated."
  },
  {
    "objectID": "introduction.html#handbook",
    "href": "introduction.html#handbook",
    "title": "Introduction",
    "section": "Handbook",
    "text": "Handbook\nThe official handbook of the course is:\nAndrew F. Hayes. Introduction to Mediation, Moderation, and Conditional Process Analysis. A Regression-Based Approach. 2018. SECOND EDITION. THE GUILFORD PRESS, New York, London"
  },
  {
    "objectID": "Rintro.html",
    "href": "Rintro.html",
    "title": "Getting started with R",
    "section": "",
    "text": "Know the meaning of the following terms: script, object and some important R object (matrix, data.frame, and vector), statistical graph/chart, package, function\nKnow the main components of the RStudio interface and their functions\nKnow how to create a new RStudio project, and open an RStudio project you created\nKnow how to create new folders in the RStudio project\nKnow how to create, save, and open a script\nKnow how to execute a line of code\nKnow how to install and load a package\nKnow how to load and view a data set\n\n\n\nThe RStudio interface is structured into four quadrants. What follows is a general description of a standard RStudio layout, but the actual layout can be customized by the user.\n\nHere you can open and work on the scripts. A script is simply a text file containing a set of commands (code) used to perform analyses. Script files are authomatically saved with extension “.r”.\nThe top-right quadrant includes the window Environment. The user can find here data sets and other objects created and loaded in a RStudio work session. In R, objects are everything a user can manipulate. For example, a data set, a function to transform data, a more or less complex algorithm to obtain a result, are all “objects”.\nIn this quadrant, there are other tabs, such as Files, History, Connections, Packages, etc. The File tab can be particularly useful: here the user can navigate files and folders to find, for example, the scripts and the data sets to upload.\nThe bottom-right quadrant is where graphical outputs, mostly plots and other statistical charts, are shown. Statistical graphs or charts are representations of statistical data in graphical form. There are several types of graphs. For example, bar charts, histograms, timeplots, scatter plots, box plots, etc.\nIn this quadrant, there is also a tab for R packages. Packages (or libraries) are collections of functions for performing specific operations and analyses. Functions can be conceived as “machines” that generate output from an input. For instance, the “addition” is a function: it receives two or more numbers as input and generates an output that corresponds to their sum.\nHere you can also find the Help section. Help is great for learning more about the functions. There are two ways you can open the Help of a function. You can write in the search box the name of the function you want to learn about, or you can write a question mark in the console followed by the name of a function (for example, “?sum”).\nOn the bottom left is the R Console window, where the code gets executed and the output is produced. It is possible to type and run commands directly into the console. Contrary to what occurs when the code is written in a script, in this case the code will not be saved.\n\n\n\nWhen starting a new data analysis project, we can create a dedicated work space. This dedicated work space is called a project. A project includes all the folders and files (e.g., scripts and data sets) necessary to conduct the analyses for the project’s purposes, as well as the outputs of the analysis (charts, tables, etc.).\nTo create a new project with RStudio, follows these steps:\n\nClick on File (on the top left);\nClick on New Project;\nSelect New Directory, and New Project;\nChoose a folder for the project and give it a name. For our course, use ADA_MMCP (Advanced Data Analysis, Mediation Moderation Conditional Process Analysis).\n\n\nYou have now created a project along with its own folder. The name of the folder is identical to that of the project. Inside the folder, there is a file with extension .Rproj and the same name of the project (e.g., “ADA_MMCP.Rproj”). This is the file you have to open whenever you want to work on the project.\nProjects can be complex, so it is important to keep everything in order. To this aim, it is a good practice to create, in the main folder of the project, sub-folders dedicated to different types of files used in the project. To create a new folder, go to the Files tab, in the upper-right quadrant of the RStudio interface, click on New Folder, and give a name to your folder.\n\nCreate a folder for the data sets, called data. You will save here the data sets we are going to use during the course.\nOnce the project has been created, we can open a new script and save it. We said that a script is a file that contains code. Script files are authomatically saved with extension .r. You can open, update, and save your script whenever you like. Saving code is important to save time! Otherwise, you would have to write the same code whenever you work on the project. Create a script named introduction_to_r. You will use it to write and run the code we are going to use today.\nTo execute (or “run”) the code in the script, we can highlight the line of code we want to run, and then click the Run button. This button can be found on the top-right corner of the editor window. Alternatively, you can place the cursor on the row containing the code to be executed, and then clic Run. Instead of the Run botton, the code can be executed by using the Ctrl + Enter key combination (also Command + Enter on a Mac).\n> In your script “introduction_to_r”, write a simple addition and execute it.\n\n5 + 3\n\n[1] 8\n\n\nNow, save the script by clicking on the disk symbol you see in the top left corner of the editor. Close the script by clicking on the small “x” (upper right corner of the editor). Finally, in the “Files” window (upper left-hand quadrant), find the “introduction_to_r” script and reopen it in RStudio.\n\n\n\nPackages (also libraries) are kinds of “toolboxes” containing sets of functions used to perform different types of operations.\nR already includes several default functions and packages, but several additional packages are available. It is possible to install a package by using the menu (Tools/Install Packages…) or by using the function install.packages(“name-of-the-package-here”).\nA very useful package is tidyverse. Tidyverse actually includes several other packages, such as ggplot2 to create charts, and dplyr to manipulate data sets (“data wrangling”).\n\ninstall.packages(\"tidyverse\")\n\nTo use the functions contained in a package, you need to load the package into the work environment first. This can be done with by using the function library(). For instance, we can load tidyverse with the following code:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\n\nThere are a few important syntactic rules to keep in mind when writing code in R.\nR recognizes lower and upper case letters. For example, the lowercase command sum is something completly different from the command Sum or SUM. In this case, while the function sum exists, there is no function called Sum or SUM.\n\n# this works\nsum(4, 6)\n\n# this function does not exist (notice the error in the console)\nSum(4, 6)\n\n# this function does not exist (notice the error in the console)\nSUM(4, 6)\n\nOnly certain types of quotation marks are recognized by R. If you copy a command from the slides or other online sources, and paste them in R, you may encounter an error because of an incorrect character used for quotes.\n\n# this is wrong (notice the error in the console)\nglbwarm <- read.csv(file = \"data/glbwarm.csv\")\n\n# this is also wrong (notice the error in the console)\nglbwarm <- read.csv(file = `data/glbwarm.csv`)\n\n# this works\nglbwarm <- read.csv(file = \"data/glbwarm.csv\")\n\n# this also works\nglbwarm <- read.csv(file = 'data/glbwarm.csv')\n\n\n\n\nLoad data and create a data set object\nYou can load data into R in at least two different ways. First, you can navigate through your files/folders using the File window, and once you have found the data set, you can simply click on it and follow the RStudio semi-automated import process.\n\n\nOtherwise, you can upload a data set by using a function. Since a data set can be saved in different formats (e.g., “.csv”, “.rds”, “.xlsx”, “.sav”, etc.), there are different functions to upload it. For instance, the function read.csv can be used to import a .csv file (one of the most common extension).\nDownload from Moodle the data set “glbwarm.csv”, save it into your folder “data”, and load it in R by using the following code:\n\nglbwarm <- read.csv(file = \"data/glbwarm.csv\")\n\nThis function creates a object called glbwarm. You can see this object in the Environment window (top-right quadrant). We said that an object is something that can be manipulated in R. In this case, the object is a data set. You can manipulate it, for example, to perform analyses, create new data, etc.\nAlso note that, in R, we use an arrow <- to save the data set in an object. The arrow “transfers” the result of a function (its output) into an object. If you run a function without saving the output in an object through the arrow sign, the output is printed on the console.\n\nread.csv(file = \"data/glbwarm.csv\")\n\n    govact posemot negemot ideology age sex partyid\n1      3.6    3.67    4.67        6  61   0       2\n2      5.0    2.00    2.33        2  55   0       1\n3      6.6    2.33    3.67        1  85   1       1\n4      1.0    5.00    5.00        1  59   0       1\n5      4.0    2.33    1.67        4  22   1       1\n6      7.0    1.00    6.00        3  34   0       2\n7      6.8    2.33    4.00        4  47   1       1\n8      5.6    4.00    5.33        5  65   1       1\n9      6.0    5.00    6.00        4  50   1       2\n10     2.6    5.00    2.00        7  60   1       3\n11     1.4    1.00    1.00        6  71   1       2\n12     5.6    4.00    4.00        4  60   0       1\n13     7.0    1.00    5.00        2  71   1       1\n14     3.8    5.67    4.67        4  59   0       1\n15     3.4    3.00    2.00        5  32   1       1\n16     4.2    1.67    1.00        2  36   1       1\n17     1.0    6.00    1.00        6  69   1       2\n18     2.6    4.00    2.33        4  70   0       3\n19     5.2    3.33    4.00        2  41   0       1\n20     5.8    1.33    2.67        4  48   0       3\n21     5.4    3.67    5.67        4  38   0       2\n22     5.4    2.00    5.00        2  63   1       1\n23     6.4    4.33    2.33        6  71   1       3\n24     5.0    3.00    4.67        4  71   1       2\n25     5.4    3.67    4.67        4  60   1       1\n26     5.8    3.67    5.67        3  76   1       1\n27     3.6    4.00    3.33        4  58   1       1\n28     4.6    4.67    3.33        5  71   0       3\n29     5.2    2.33    3.00        4  63   0       1\n30     3.8    4.33    1.33        5  79   1       1\n31     4.2    2.33    4.33        4  68   0       1\n32     3.4    2.33    2.33        2  32   1       1\n33     2.2    2.67    5.00        7  76   1       3\n34     5.2    2.00    6.00        5  50   0       2\n35     5.2    4.00    3.67        4  72   0       2\n36     4.0    3.67    2.33        4  51   1       2\n37     7.0    3.67    5.33        4  60   1       1\n38     1.0    4.00    1.00        7  76   1       3\n39     5.4    1.33    3.33        3  34   0       1\n40     4.8    5.00    5.33        4  51   0       2\n41     6.0    4.00    3.00        3  49   0       1\n42     3.6    1.67    2.00        5  58   0       3\n43     5.2    4.00    5.00        2  45   0       1\n44     3.8    3.67    4.33        4  57   1       3\n45     4.8    3.00    3.00        5  46   0       3\n46     2.4    4.67    1.33        5  69   1       3\n47     6.0    5.00    1.67        5  70   0       3\n48     4.8    2.67    3.00        6  69   0       3\n49     4.6    1.00    3.00        4  58   1       1\n50     4.0    3.00    2.00        5  80   1       3\n51     3.4    1.67    2.67        6  64   1       3\n52     1.0    1.00    1.00        7  42   1       3\n53     3.6    3.33    1.67        6  71   1       3\n54     4.6    3.00    3.67        6  40   1       3\n55     4.0    4.00    3.33        1  43   0       1\n56     4.6    2.00    2.00        2  44   0       1\n57     5.0    4.00    3.00        4  40   0       1\n58     4.6    4.67    3.33        3  77   1       3\n59     3.4    1.00    5.33        5  73   1       2\n60     5.6    1.67    5.33        5  63   1       2\n61     6.4    2.00    4.00        3  47   0       1\n62     3.8    2.00    2.00        4  41   0       3\n63     5.8    3.33    4.00        2  64   1       1\n64     4.6    4.00    3.00        4  57   0       2\n65     4.4    4.00    3.00        4  41   0       3\n66     3.8    2.67    1.33        6  43   0       3\n67     1.0    2.67    1.67        6  52   1       3\n68     5.6    1.00    3.00        4  37   0       2\n69     3.6    1.00    1.00        2  68   0       1\n70     6.8    4.67    4.00        4  60   0       3\n71     7.0    5.00    4.67        1  40   1       1\n72     5.2    3.67    4.33        6  72   1       2\n73     5.8    2.67    4.00        2  57   1       1\n74     6.6    2.00    4.67        2  85   1       1\n75     5.2    2.33    3.33        3  52   0       1\n76     4.0    2.67    3.33        1  55   1       2\n77     5.4    1.00    4.33        4  42   0       3\n78     5.0    3.67    4.67        3  34   1       1\n79     2.2    1.00    1.00        4  40   0       2\n80     4.4    1.33    4.00        6  55   0       3\n81     3.6    3.00    3.00        5  73   1       2\n82     6.0    3.33    5.33        4  67   0       1\n83     2.8    3.67    3.33        3  65   0       1\n84     2.2    1.00    1.00        1  63   1       2\n85     5.8    5.00    3.33        1  50   0       1\n86     3.8    5.00    1.33        4  52   1       3\n87     3.0    2.67    2.00        4  72   1       3\n88     3.8    3.33    4.33        5  75   1       3\n89     2.8    2.33    1.00        4  54   1       2\n90     4.0    2.67    3.67        4  38   0       2\n91     4.2    5.00    2.67        4  61   0       2\n92     4.0    3.33    2.67        6  47   0       3\n93     2.2    2.67    2.00        4  45   1       2\n94     5.2    2.67    4.00        2  55   0       1\n95     4.0    3.67    3.67        6  72   0       3\n96     4.6    2.00    3.33        3  40   0       1\n97     5.6    2.33    4.33        4  52   0       3\n98     4.2    5.00    5.33        2  42   1       1\n99     5.2    4.33    5.33        2  73   0       1\n100    4.8    4.00    4.67        4  32   1       3\n101    5.0    1.33    3.33        4  41   0       2\n102    1.4    4.33    1.33        7  69   1       3\n103    6.2    1.00    5.33        2  44   0       1\n104    2.8    2.33    1.33        5  67   1       3\n105    5.2    2.00    3.00        3  53   0       1\n106    7.0    2.33    5.00        5  67   1       1\n107    3.2    3.33    2.33        6  60   1       2\n108    5.6    2.00    4.33        6  54   1       3\n109    3.0    3.00    1.00        6  61   1       3\n110    5.0    1.33    2.00        4  39   1       2\n111    4.8    1.33    2.00        6  70   1       3\n112    4.4    3.67    2.67        4  63   0       1\n113    4.6    3.00    5.67        2  64   1       1\n114    3.6    6.00    1.67        7  69   1       3\n115    5.0    3.33    4.67        4  84   1       1\n116    4.0    3.00    2.00        2  54   0       1\n117    4.2    4.33    3.67        4  41   0       1\n118    5.0    3.00    4.33        2  68   1       1\n119    6.2    2.00    4.67        3  57   0       1\n120    6.6    2.67    4.00        4  42   0       1\n121    5.0    1.67    5.67        4  41   0       3\n122    1.0    6.00    1.00        7  43   0       3\n123    6.4    1.67    3.67        2  42   0       1\n124    5.6    1.00    5.00        4  33   1       2\n125    4.8    4.33    3.00        4  71   0       2\n126    2.4    2.67    2.33        4  67   1       3\n127    4.2    3.00    3.33        6  69   0       3\n128    4.2    3.67    3.00        3  57   1       1\n129    2.8    4.67    1.00        5  70   1       3\n130    5.4    2.67    2.00        6  60   1       3\n131    3.4    1.00    2.33        6  76   1       3\n132    5.4    4.00    2.67        4  33   0       1\n133    3.6    3.67    2.00        5  78   1       3\n134    1.0    2.00    1.00        6  67   1       3\n135    5.2    4.00    3.00        6  53   0       3\n136    6.0    3.67    4.00        4  51   1       2\n137    4.8    2.67    3.33        4  40   0       1\n138    2.8    1.00    2.67        7  44   1       2\n139    3.8    2.67    1.00        4  52   0       2\n140    3.6    2.00    2.00        6  63   1       3\n141    1.4    1.67    2.67        5  76   1       3\n142    4.6    5.00    5.00        1  28   0       1\n143    5.6    2.33    5.67        6  67   1       3\n144    6.0    4.33    6.00        4  71   1       1\n145    4.0    1.00    1.00        4  41   1       2\n146    4.4    3.33    2.67        4  66   1       1\n147    4.0    4.00    4.00        4  39   0       2\n148    1.4    1.00    1.00        7  73   1       3\n149    4.8    1.00    5.00        5  51   0       1\n150    3.8    3.33    3.33        6  79   0       2\n151    6.0    5.00    2.67        4  67   0       1\n152    4.6    1.00    4.00        1  53   0       1\n153    1.0    1.00    1.00        7  59   1       3\n154    5.8    2.00    4.67        4  66   1       1\n155    4.6    4.33    1.67        3  75   1       1\n156    3.0    1.33    3.33        5  83   1       2\n157    5.0    3.00    4.00        3  74   1       1\n158    5.6    3.33    5.00        5  66   1       1\n159    6.2    1.33    5.67        2  42   0       1\n160    5.0    1.67    1.33        3  37   0       2\n161    6.6    4.67    2.67        5  55   1       1\n162    4.8    1.00    3.67        2  51   0       1\n163    4.8    5.67    4.33        5  69   1       1\n164    5.4    3.67    1.33        7  63   1       3\n165    5.6    4.67    3.33        4  73   1       1\n166    4.4    2.33    2.67        3  61   1       1\n167    4.2    2.33    3.00        6  65   1       3\n168    6.4    2.00    2.00        4  56   1       2\n169    4.6    4.00    5.00        4  49   1       1\n170    5.2    3.67    2.67        3  77   1       1\n171    6.6    1.67    5.33        2  44   0       1\n172    4.4    2.67    2.00        4  63   1       1\n173    1.0    4.00    1.00        7  54   1       3\n174    7.0    3.00    6.00        1  40   0       1\n175    1.6    5.00    1.67        4  72   1       3\n176    4.6    5.00    1.67        3  48   1       3\n177    3.6    2.67    4.00        4  42   1       3\n178    3.4    3.00    3.33        6  67   0       3\n179    3.8    4.33    2.00        4  26   0       1\n180    4.0    2.00    1.00        4  56   0       1\n181    4.8    4.33    4.33        4  54   1       1\n182    5.6    4.00    2.67        3  46   0       1\n183    4.4    4.00    4.00        5  29   0       1\n184    4.8    2.33    1.67        6  68   1       1\n185    4.0    3.00    3.00        4  41   0       1\n186    4.8    1.33    6.00        5  64   0       3\n187    4.0    2.33    1.67        4  50   1       3\n188    5.8    3.33    4.00        1  56   1       1\n189    1.0    1.00    1.00        6  70   1       3\n190    4.4    4.00    4.00        3  33   1       2\n191    4.4    1.00    2.00        5  57   1       3\n192    4.8    1.00    1.00        5  70   0       3\n193    6.2    3.33    3.67        4  83   1       2\n194    5.8    2.00    4.33        4  21   1       1\n195    5.0    1.00    4.00        4  57   0       1\n196    4.4    3.67    3.67        3  59   1       1\n197    5.8    2.33    5.00        6  72   1       3\n198    2.0    2.33    2.00        7  48   1       3\n199    4.0    3.00    3.00        6  29   1       2\n200    4.4    4.67    3.67        2  53   1       3\n201    4.4    3.67    3.33        4  29   1       1\n202    4.6    3.67    1.00        4  33   0       2\n203    5.2    1.00    5.00        2  29   0       1\n204    4.0    4.67    4.67        7  46   0       1\n205    4.8    4.33    4.67        2  57   0       1\n206    5.4    4.67    3.00        2  49   0       2\n207    2.2    1.67    2.00        4  27   1       2\n208    5.2    2.67    4.67        4  29   1       1\n209    1.6    1.00    1.00        7  64   0       3\n210    5.8    4.00    1.67        4  61   0       2\n211    5.6    3.67    3.33        4  25   0       1\n212    5.8    2.67    4.67        4  40   0       1\n213    2.4    3.33    3.00        7  62   1       3\n214    6.6    2.00    6.00        3  70   1       1\n215    4.0    2.67    5.00        7  50   0       2\n216    5.4    2.33    6.00        2  30   0       1\n217    3.6    3.00    3.33        4  27   1       2\n218    3.8    4.33    2.67        4  41   0       1\n219    1.0    2.00    2.00        6  60   1       3\n220    3.4    3.67    1.33        5  69   0       3\n221    5.8    4.00    5.00        2  37   0       1\n222    1.0    1.00    1.00        6  64   1       3\n223    2.6    5.00    2.33        4  66   0       3\n224    4.6    2.00    3.00        5  32   0       3\n225    3.8    2.33    5.00        4  67   0       1\n226    3.2    5.00    2.00        6  62   1       3\n227    3.6    3.33    4.33        6  72   1       3\n228    5.2    2.67    4.33        4  63   1       2\n229    1.0    1.67    1.00        6  68   1       3\n230    4.2    3.00    1.00        7  41   0       3\n231    5.8    2.33    5.00        4  31   0       1\n232    5.2    2.00    4.67        4  51   0       1\n233    6.0    2.67    5.00        4  30   0       3\n234    5.6    4.33    3.00        4  78   1       1\n235    4.0    3.33    2.67        2  62   0       1\n236    4.8    3.00    4.00        5  77   0       3\n237    5.2    2.00    3.33        6  46   0       3\n238    4.4    2.67    4.00        2  36   0       1\n239    4.2    4.00    3.67        4  34   1       1\n240    3.6    5.00    3.00        6  42   0       3\n241    5.4    1.67    5.67        2  25   0       1\n242    4.8    1.33    4.00        7  26   1       3\n243    6.8    4.00    3.33        4  59   1       2\n244    4.4    1.00    1.00        4  30   0       2\n245    4.6    1.00    2.67        4  60   0       1\n246    6.4    1.00    4.67        5  27   1       3\n247    2.6    2.67    5.00        4  51   1       1\n248    5.4    1.67    5.00        3  48   0       1\n249    6.2    5.00    5.33        3  49   1       1\n250    5.6    4.67    3.00        4  75   0       1\n251    4.8    2.67    6.00        4  46   1       2\n252    4.6    4.33    4.67        3  66   1       1\n253    5.0    2.00    5.33        7  64   1       3\n254    5.6    5.00    3.67        2  35   1       1\n255    3.4    1.00    6.00        4  65   0       2\n256    5.8    4.33    4.00        3  77   0       1\n257    5.6    3.00    3.33        7  71   1       3\n258    5.6    4.33    5.00        5  54   0       3\n259    2.0    4.67    2.67        6  29   1       3\n260    5.6    3.67    5.00        2  70   0       3\n261    6.4    4.33    6.00        4  65   0       1\n262    3.4    1.00    3.67        4  27   1       2\n263    5.0    3.67    5.00        4  72   0       2\n264    5.0    3.33    2.33        4  67   1       1\n265    4.0    1.00    1.00        4  67   0       1\n266    5.0    2.00    2.67        3  73   1       1\n267    3.8    3.67    3.00        4  36   0       2\n268    6.6    2.00    5.67        3  58   1       1\n269    4.0    3.00    3.00        4  34   0       1\n270    4.2    1.33    4.33        4  53   0       1\n271    5.2    6.00    4.00        2  25   1       1\n272    5.2    1.67    3.00        2  33   0       1\n273    5.2    5.00    5.00        4  64   0       1\n274    3.2    4.67    2.33        5  80   0       1\n275    4.8    4.00    3.33        4  41   0       1\n276    4.0    4.00    4.00        4  27   1       1\n277    3.4    5.00    1.00        4  52   0       1\n278    3.8    3.00    3.67        4  28   0       2\n279    5.8    3.00    6.00        4  67   1       3\n280    6.0    4.67    4.67        2  25   0       2\n281    5.0    1.33    1.67        4  61   0       1\n282    2.2    1.00    1.00        7  68   1       3\n283    4.2    2.00    1.33        4  25   1       2\n284    5.2    5.00    5.00        2  27   1       2\n285    4.0    4.33    4.00        3  23   1       1\n286    4.8    5.00    3.67        4  65   1       1\n287    4.8    1.00    1.00        5  66   1       1\n288    4.6    1.00    1.00        2  50   1       1\n289    5.0    3.00    1.33        6  64   1       3\n290    4.0    4.67    5.00        4  33   0       1\n291    2.2    6.00    2.67        4  59   1       1\n292    5.2    4.00    4.67        4  79   1       1\n293    4.4    2.67    2.67        5  72   1       3\n294    5.6    5.33    4.67        6  60   0       1\n295    6.0    2.67    6.00        4  62   0       3\n296    5.8    3.33    4.33        2  63   1       1\n297    3.8    3.67    3.00        4  30   1       2\n298    3.6    3.00    2.00        4  67   1       2\n299    4.2    5.33    3.67        4  45   0       1\n300    4.8    2.00    6.00        2  41   0       1\n301    2.2    4.33    2.00        6  61   1       3\n302    1.2    4.00    1.00        7  62   1       3\n303    3.2    2.33    1.67        5  66   0       2\n304    1.0    1.00    1.00        4  46   1       2\n305    1.0    1.00    1.00        6  73   1       3\n306    5.6    3.67    4.33        4  60   1       2\n307    6.8    5.00    5.67        2  60   0       1\n308    4.2    4.00    1.33        4  65   1       2\n309    5.6    1.67    5.00        7  63   1       3\n310    5.4    3.67    4.00        5  59   0       3\n311    7.0    2.00    5.67        1  82   1       1\n312    4.4    2.00    6.00        4  55   1       3\n313    7.0    5.33    6.00        4  55   1       3\n314    5.6    4.00    2.33        5  67   1       3\n315    4.0    4.00    4.00        4  29   1       2\n316    5.0    3.00    4.00        2  68   1       1\n317    7.0    5.33    5.33        1  57   0       1\n318    5.6    2.67    5.67        2  72   1       1\n319    3.6    3.00    5.00        4  29   0       1\n320    6.6    2.00    5.00        1  26   0       1\n321    5.6    2.00    5.33        1  50   1       1\n322    1.0    1.00    1.00        6  58   1       3\n323    6.4    4.00    6.00        4  64   1       2\n324    2.2    4.67    1.67        4  60   1       2\n325    5.8    3.67    5.00        2  67   0       1\n326    5.0    1.00    1.00        1  34   0       1\n327    4.4    2.33    3.67        3  59   0       1\n328    4.6    1.67    1.67        5  23   1       3\n329    5.4    3.33    4.67        3  21   0       1\n330    4.0    2.67    1.33        4  27   1       2\n331    2.2    6.00    1.00        7  65   1       3\n332    3.6    2.33    4.33        4  30   0       2\n333    5.0    6.00    1.00        7  42   0       3\n334    3.4    1.67    3.67        5  53   0       3\n335    1.4    4.67    1.33        7  40   1       3\n336    3.8    2.00    1.67        6  44   0       3\n337    5.4    3.67    5.33        4  27   1       3\n338    5.2    1.67    2.33        3  22   0       3\n339    5.2    5.00    3.67        5  55   0       3\n340    4.4    2.67    3.67        4  72   0       1\n341    4.6    4.33    3.67        2  24   0       1\n342    4.2    5.33    4.00        5  63   0       3\n343    4.8    4.00    3.00        5  63   0       1\n344    6.0    4.00    2.67        2  68   0       1\n345    6.0    5.00    6.00        4  35   0       2\n346    6.2    4.33    4.33        4  47   0       2\n347    4.6    2.67    1.33        4  41   0       1\n348    1.2    2.00    2.67        5  68   1       2\n349    5.0    5.00    2.33        4  68   1       3\n350    2.4    4.00    1.00        6  42   0       3\n351    4.0    2.67    1.33        5  73   1       3\n352    5.8    2.00    4.00        4  25   1       1\n353    3.6    3.33    2.00        5  61   1       3\n354    5.2    1.00    3.67        5  55   0       3\n355    3.6    2.67    1.00        4  34   1       2\n356    4.4    1.00    5.67        5  66   0       1\n357    4.8    2.00    5.00        2  65   0       2\n358    7.0    1.33    6.00        5  40   0       1\n359    5.2    2.33    4.67        3  30   1       1\n360    3.4    1.00    4.33        4  42   0       1\n361    4.2    4.33    2.00        7  53   1       3\n362    4.0    4.00    4.00        2  60   1       1\n363    4.8    1.00    1.33        4  20   0       2\n364    3.0    3.00    1.00        6  67   0       3\n365    5.8    5.00    4.67        3  68   0       1\n366    2.8    4.33    2.67        4  26   1       2\n367    6.2    2.33    5.33        1  48   1       1\n368    3.6    3.33    3.00        6  64   1       3\n369    2.8    4.00    1.33        4  62   1       1\n370    4.0    4.00    3.33        2  21   0       1\n371    4.8    5.00    5.33        4  73   1       1\n372    4.2    5.00    3.67        4  57   0       1\n373    5.8    4.33    4.00        3  71   1       1\n374    4.0    4.00    4.33        4  55   0       2\n375    5.8    3.00    3.67        1  26   0       1\n376    3.4    1.67    3.33        5  83   0       2\n377    4.0    5.33    2.00        2  47   0       1\n378    5.0    2.00    2.67        6  57   1       3\n379    4.6    2.00    3.33        4  35   0       1\n380    1.0    6.00    1.00        4  71   1       3\n381    5.2    2.67    1.33        6  29   0       3\n382    4.2    4.00    3.00        6  76   1       3\n383    5.6    2.67    4.00        2  55   0       2\n384    3.6    3.33    4.67        4  28   1       2\n385    5.2    3.33    5.33        4  58   0       2\n386    5.2    2.00    2.00        4  47   0       1\n387    4.6    3.67    3.33        2  30   1       1\n388    2.2    5.33    1.00        6  52   1       3\n389    7.0    1.00    3.67        3  31   1       1\n390    3.6    2.33    4.67        6  62   1       3\n391    6.4    3.33    6.00        7  72   1       3\n392    5.8    3.67    5.00        6  50   0       3\n393    4.4    2.33    2.67        4  29   0       1\n394    6.4    4.00    3.33        1  45   0       1\n395    4.6    4.33    4.00        4  65   0       2\n396    5.2    2.67    4.00        4  37   0       3\n397    5.6    4.00    3.67        5  30   0       3\n398    6.0    2.33    3.67        7  44   0       3\n399    4.8    4.67    3.33        4  65   1       3\n400    5.6    3.33    3.00        4  41   1       2\n401    4.0    4.00    4.00        4  38   1       1\n402    5.2    2.33    3.67        3  59   0       3\n403    5.2    3.00    3.33        3  19   1       1\n404    4.8    4.67    3.00        4  32   1       1\n405    4.2    2.00    2.67        4  55   0       1\n406    6.0    4.00    5.00        4  53   0       3\n407    2.0    1.00    1.00        4  35   0       3\n408    4.2    2.33    3.33        5  40   0       1\n409    6.2    2.00    4.67        3  65   1       1\n410    1.2    3.00    1.33        6  71   1       3\n411    6.0    4.67    5.33        5  69   1       3\n412    4.6    4.00    1.00        4  51   1       1\n413    5.6    3.00    4.67        5  39   0       3\n414    5.6    5.33    6.00        7  72   1       2\n415    5.0    4.00    3.33        7  26   1       1\n416    4.6    1.00    5.00        1  32   1       1\n417    4.0    1.00    2.00        4  25   0       2\n418    6.2    2.67    5.00        4  32   1       2\n419    4.0    3.00    3.33        1  23   1       1\n420    4.6    5.67    2.67        4  19   1       1\n421    3.4    1.00    4.33        4  22   0       1\n422    4.0    2.00    3.67        4  29   0       3\n423    4.6    3.67    4.00        3  62   0       3\n424    6.4    4.00    4.00        4  39   0       2\n425    5.8    4.67    4.67        7  49   0       2\n426    6.4    3.67    6.00        2  37   0       1\n427    4.2    1.00    2.67        2  43   1       1\n428    5.4    2.00    5.00        5  59   1       3\n429    5.0    5.67    5.67        2  32   1       1\n430    6.4    6.00    5.00        5  57   1       3\n431    1.0    1.00    1.00        7  39   0       1\n432    3.4    2.00    2.00        4  33   0       1\n433    6.2    5.33    5.00        4  51   1       1\n434    3.2    2.67    1.67        4  25   0       3\n435    4.4    3.33    3.33        4  26   0       1\n436    4.8    4.33    5.00        2  48   1       1\n437    5.2    3.67    4.67        4  27   1       2\n438    4.8    2.00    4.67        3  54   1       2\n439    4.0    4.67    3.67        4  30   0       2\n440    5.4    2.33    3.67        5  53   1       1\n441    4.4    5.00    3.67        2  36   0       1\n442    4.6    3.33    3.33        5  60   0       1\n443    3.0    4.00    5.00        3  50   0       1\n444    7.0    4.00    5.00        4  27   1       2\n445    4.6    1.33    1.33        3  22   0       2\n446    4.6    4.67    5.33        4  68   1       1\n447    4.2    6.00    4.00        4  52   0       2\n448    2.8    5.00    1.00        7  61   1       3\n449    4.0    3.00    4.00        3  38   0       1\n450    5.0    5.33    1.00        2  21   1       2\n451    4.0    5.00    5.00        3  36   0       3\n452    5.2    4.67    4.00        4  22   0       1\n453    1.2    1.00    1.00        6  66   0       3\n454    5.6    3.33    5.67        4  64   0       1\n455    2.0    4.00    1.00        5  68   1       3\n456    3.6    3.33    3.67        4  51   1       1\n457    2.8    2.00    3.00        4  31   0       2\n458    4.8    2.67    5.00        1  18   1       1\n459    4.6    5.00    5.00        4  34   0       1\n460    6.0    2.33    4.33        5  55   1       1\n461    4.6    2.67    5.33        6  43   0       1\n462    5.8    5.00    6.00        6  50   0       1\n463    7.0    2.67    6.00        1  64   1       1\n464    5.4    3.00    3.67        2  33   1       1\n465    4.8    3.33    2.00        5  38   0       2\n466    4.8    2.00    5.67        2  55   0       1\n467    2.8    1.00    1.00        4  30   0       1\n468    5.4    4.67    1.67        4  26   1       2\n469    4.6    3.33    3.67        3  26   0       1\n470    5.4    2.67    4.67        4  61   1       1\n471    4.0    1.00    1.00        7  30   0       3\n472    5.4    4.67    6.00        2  68   0       1\n473    5.4    5.00    3.33        4  87   1       1\n474    4.0    3.67    3.33        4  43   0       2\n475    5.0    5.33    3.67        4  32   0       1\n476    6.0    4.00    5.00        5  63   1       2\n477    3.2    1.00    1.00        7  57   0       3\n478    5.8    3.67    4.33        2  55   0       1\n479    4.4    2.00    4.67        4  69   0       1\n480    4.0    4.67    3.00        4  75   0       1\n481    5.4    5.00    3.33        3  54   1       1\n482    4.0    3.33    4.00        4  43   0       2\n483    5.2    3.00    2.33        3  26   1       1\n484    1.4    6.00    1.33        6  62   0       3\n485    5.8    4.00    4.00        2  33   1       1\n486    4.0    4.00    5.33        4  36   1       1\n487    6.8    3.67    6.00        7  52   1       3\n488    6.2    4.67    6.00        4  83   1       1\n489    3.6    2.00    2.67        2  55   0       1\n490    4.6    2.67    3.33        4  18   0       2\n491    5.8    4.00    4.00        2  30   1       2\n492    5.8    4.00    4.00        3  59   0       1\n493    7.0    2.33    3.00        1  51   1       1\n494    5.4    1.67    3.00        4  41   0       3\n495    1.2    1.67    1.00        6  60   1       2\n496    3.4    5.00    3.33        4  39   0       3\n497    4.4    2.00    2.33        4  64   1       2\n498    2.2    3.67    2.00        6  52   1       3\n499    3.0    5.00    1.00        5  55   1       3\n500    5.0    3.33    1.67        4  45   1       3\n501    5.4    4.00    3.33        3  45   0       1\n502    4.0    3.67    1.67        6  66   1       3\n503    7.0    4.00    6.00        2  73   1       1\n504    4.4    1.00    4.00        4  40   0       2\n505    4.2    4.33    2.00        2  29   0       1\n506    2.8    4.00    2.67        6  73   1       3\n507    4.2    2.33    3.67        5  54   0       3\n508    4.6    3.33    2.67        6  49   1       3\n509    3.8    4.00    4.00        5  54   1       3\n510    5.2    3.00    4.00        4  48   1       2\n511    6.2    4.33    5.33        4  63   0       2\n512    4.6    3.33    5.00        4  36   0       1\n513    4.0    3.00    3.00        4  25   0       1\n514    5.8    2.67    6.00        4  62   0       3\n515    5.0    3.67    3.33        4  65   1       3\n516    4.0    3.33    4.00        4  65   0       1\n517    4.2    2.67    3.33        4  42   0       3\n518    6.0    1.00    1.67        3  32   1       1\n519    4.4    1.00    2.67        4  58   0       1\n520    5.2    2.33    4.00        3  40   0       1\n521    3.6    4.00    3.67        4  71   0       3\n522    6.0    3.33    5.00        2  60   1       1\n523    5.6    5.67    4.00        4  28   1       1\n524    3.8    3.33    4.33        5  64   0       3\n525    4.0    6.00    6.00        3  19   0       1\n526    4.4    2.00    4.33        1  44   1       1\n527    4.0    2.00    3.67        4  18   1       1\n528    5.4    2.33    4.00        2  29   1       1\n529    5.6    4.33    5.33        4  59   0       2\n530    4.0    2.67    4.00        5  80   1       2\n531    5.0    4.00    4.00        3  27   0       1\n532    4.0    1.33    1.67        7  62   1       3\n533    5.4    3.00    5.00        2  19   1       1\n534    4.0    4.00    2.00        2  26   1       2\n535    2.2    4.67    1.00        7  50   1       3\n536    4.4    4.33    4.67        4  59   0       1\n537    4.2    3.00    3.00        3  47   1       1\n538    3.2    5.33    2.00        6  63   1       3\n539    2.2    4.00    1.67        5  67   1       3\n540    5.8    1.67    1.00        4  42   0       3\n541    5.2    2.33    3.33        6  71   0       3\n542    7.0    3.67    4.00        4  49   1       1\n543    5.2    3.00    3.33        2  55   1       1\n544    6.0    3.67    6.00        4  44   0       2\n545    4.4    4.67    4.33        4  20   0       1\n546    5.8    4.33    4.00        2  31   1       2\n547    6.2    3.33    5.67        4  44   1       1\n548    4.8    3.67    3.67        5  56   1       3\n549    5.2    3.67    4.33        3  24   1       1\n550    6.8    4.33    6.00        6  40   1       1\n551    4.8    2.33    4.00        3  35   0       1\n552    5.8    3.00    4.33        3  54   1       1\n553    2.8    3.67    3.67        5  38   0       2\n554    4.4    3.33    1.67        5  40   1       3\n555    5.6    1.00    1.00        7  53   1       3\n556    5.0    2.67    3.67        3  53   1       1\n557    4.8    4.00    5.33        6  64   0       1\n558    5.6    4.33    4.67        2  68   0       1\n559    1.2    6.00    1.00        6  34   1       3\n560    4.4    2.67    2.67        3  36   1       3\n561    1.8    1.67    1.00        7  62   1       3\n562    5.2    5.00    5.00        6  60   1       3\n563    3.6    3.00    2.67        7  79   0       3\n564    4.0    4.00    2.33        4  29   1       2\n565    5.6    3.67    1.67        2  32   0       1\n566    5.0    4.00    4.00        5  23   1       1\n567    4.0    3.00    3.00        3  53   1       3\n568    1.0    3.67    1.00        7  63   1       3\n569    5.2    1.00    5.67        4  40   0       3\n570    3.8    2.00    1.00        6  29   1       3\n571    6.2    2.00    5.33        3  61   0       1\n572    1.2    1.33    1.00        7  31   0       3\n573    4.8    1.33    4.67        6  43   0       3\n574    6.6    1.00    6.00        4  59   1       1\n575    6.8    2.67    2.67        1  74   1       1\n576    4.0    1.00    1.00        4  24   1       2\n577    3.6    5.67    3.00        4  54   0       1\n578    3.4    1.67    4.00        4  38   0       2\n579    3.4    3.00    3.00        4  27   0       2\n580    3.0    2.67    1.00        4  53   1       3\n581    4.0    2.67    2.00        6  39   0       1\n582    4.8    2.67    4.33        3  23   1       1\n583    3.4    2.00    1.00        4  41   1       1\n584    3.6    1.00    1.00        5  70   1       3\n585    4.6    3.67    4.00        4  74   0       2\n586    4.6    4.00    4.33        3  62   1       1\n587    4.8    3.33    5.00        5  33   1       3\n588    6.8    3.00    2.67        4  39   1       2\n589    4.6    1.00    1.67        4  20   1       2\n590    7.0    2.33    5.67        5  21   0       3\n591    6.4    3.67    5.33        2  23   0       1\n592    4.8    2.33    1.67        4  50   1       2\n593    5.0    4.00    4.00        2  27   1       2\n594    4.8    1.33    5.00        4  54   1       1\n595    6.0    4.33    4.00        4  31   1       2\n596    5.8    5.33    5.33        3  67   0       1\n597    2.8    6.00    5.67        2  62   0       3\n598    6.0    2.33    5.67        3  56   0       1\n599    2.2    4.67    2.00        6  53   1       3\n600    6.2    6.00    6.00        3  30   1       1\n601    6.0    2.00    5.00        2  60   1       1\n602    3.2    2.33    2.00        6  66   1       3\n603    1.8    3.33    6.00        4  55   0       3\n604    5.4    1.67    4.67        4  43   0       3\n605    6.4    4.67    5.33        2  45   0       1\n606    5.6    2.67    6.00        7  64   0       3\n607    2.6    1.00    6.00        4  32   0       3\n608    5.4    1.00    6.00        6  54   1       3\n609    1.2    5.00    1.00        6  49   0       3\n610    6.6    5.00    5.00        4  57   1       2\n611    5.0    1.00    1.33        2  44   0       1\n612    5.0    1.67    4.67        4  39   0       2\n613    6.4    2.67    5.33        5  41   0       3\n614    1.8    1.33    1.00        6  55   1       2\n615    4.0    5.00    1.33        6  62   1       3\n616    5.2    3.00    3.67        4  62   1       2\n617    2.4    4.00    2.00        6  66   1       3\n618    4.0    3.33    2.00        6  52   0       3\n619    4.4    4.00    2.33        5  63   1       3\n620    5.2    1.00    3.33        4  24   0       1\n621    2.8    4.33    2.33        7  69   1       3\n622    5.4    1.00    2.00        4  55   0       3\n623    5.0    3.00    4.00        3  35   0       1\n624    2.2    3.33    1.33        7  49   1       3\n625    4.0    4.67    4.67        2  42   0       1\n626    6.2    1.00    6.00        1  67   0       1\n627    5.2    2.33    5.33        4  55   0       1\n628    4.4    1.67    3.67        1  35   0       1\n629    4.8    4.00    5.67        2  63   1       1\n630    5.4    1.00    6.00        4  64   0       3\n631    5.2    4.00    4.67        6  61   1       1\n632    4.8    3.67    3.67        4  49   1       2\n633    7.0    6.00    5.67        2  32   1       1\n634    3.8    2.67    4.00        5  68   0       1\n635    4.8    4.67    5.67        4  50   1       2\n636    5.0    3.33    2.00        4  35   0       1\n637    4.2    2.00    1.67        6  73   0       3\n638    6.4    2.33    5.00        4  34   1       2\n639    4.8    3.33    4.00        7  47   1       3\n640    2.6    1.00    1.00        6  75   0       3\n641    4.8    4.00    4.00        6  69   0       3\n642    4.2    3.00    5.67        5  73   1       3\n643    7.0    6.00    5.33        6  33   0       3\n644    6.6    5.67    4.67        4  27   1       2\n645    4.8    1.00    6.00        1  40   0       1\n646    4.6    1.67    2.00        6  30   1       3\n647    5.0    2.33    4.33        3  27   0       2\n648    4.8    1.00    2.33        4  31   1       2\n649    4.6    1.67    1.67        4  23   1       2\n650    3.8    2.67    3.67        4  43   0       2\n651    5.6    1.00    5.00        2  21   0       1\n652    2.6    4.00    3.33        3  53   0       1\n653    3.0    4.00    2.00        4  29   0       3\n654    5.4    1.33    6.00        4  67   0       2\n655    4.2    1.33    2.00        4  30   1       1\n656    6.8    1.67    6.00        5  47   0       1\n657    5.4    3.33    2.33        4  31   1       2\n658    5.2    1.00    2.67        4  64   0       2\n659    4.0    2.33    3.00        5  65   1       3\n660    4.6    5.00    3.00        6  65   0       3\n661    4.4    4.33    6.00        4  47   1       2\n662    4.2    3.67    3.67        3  22   1       1\n663    5.2    4.67    4.33        6  64   0       3\n664    4.2    3.00    3.00        4  59   0       2\n665    4.0    3.33    4.00        4  18   0       2\n666    3.4    2.33    4.67        4  19   1       1\n667    3.4    4.00    5.67        5  44   0       3\n668    5.6    4.00    4.33        2  41   0       2\n669    5.2    3.67    2.33        4  28   0       2\n670    3.4    3.67    3.67        2  22   1       1\n671    4.4    2.33    5.00        5  62   1       3\n672    5.0    4.67    3.33        2  29   0       1\n673    4.0    2.33    5.00        4  52   0       2\n674    6.4    2.00    5.67        3  65   0       1\n675    6.6    1.00    4.00        6  54   0       2\n676    6.6    1.67    4.67        4  51   0       1\n677    5.2    3.67    3.67        4  19   0       2\n678    5.8    2.33    4.33        2  27   1       1\n679    2.4    2.00    6.00        4  42   0       1\n680    6.8    3.00    3.33        2  70   0       1\n681    5.0    5.33    4.00        4  73   1       2\n682    4.0    4.67    4.67        4  42   0       2\n683    3.8    1.67    2.67        7  42   0       3\n684    4.6    4.33    3.33        5  23   0       3\n685    3.6    5.67    1.67        4  19   0       1\n686    5.0    2.67    5.33        4  22   1       2\n687    5.0    6.00    6.00        3  30   0       2\n688    6.8    6.00    6.00        4  51   1       2\n689    1.6    1.00    1.00        7  34   1       3\n690    5.0    4.00    4.00        3  63   0       2\n691    4.0    2.00    4.00        4  29   1       2\n692    6.4    2.00    4.67        4  46   0       3\n693    5.0    2.00    4.33        4  43   0       2\n694    3.6    3.00    1.33        4  56   0       2\n695    2.4    5.00    2.00        7  48   1       3\n696    4.0    3.00    3.00        4  64   0       1\n697    7.0    6.00    6.00        1  22   0       1\n698    4.4    3.33    4.33        5  44   1       3\n699    4.6    3.33    3.33        5  25   0       3\n700    3.4    2.00    2.00        4  24   1       2\n701    4.2    3.00    6.00        2  55   0       2\n702    4.6    3.00    3.33        2  41   0       1\n703    5.8    2.33    3.67        2  65   0       1\n704    6.2    4.67    4.33        5  36   0       3\n705    5.4    3.00    6.00        2  56   1       1\n706    5.4    5.67    4.00        6  36   0       3\n707    4.8    1.33    5.67        4  36   0       2\n708    3.4    5.67    5.00        4  29   0       2\n709    5.2    3.67    1.67        5  44   0       1\n710    3.4    2.00    3.33        3  40   0       1\n711    4.2    3.33    2.67        4  51   0       1\n712    2.2    3.33    3.67        4  30   1       2\n713    6.2    1.33    2.00        4  43   0       3\n714    4.6    4.67    4.00        4  50   1       1\n715    5.6    4.33    3.00        3  40   0       1\n716    6.6    4.00    5.33        3  56   1       1\n717    6.4    1.67    1.67        5  65   0       3\n718    5.6    2.33    5.33        5  62   1       3\n719    7.0    3.00    5.00        1  22   0       1\n720    3.6    6.00    6.00        4  43   1       2\n721    6.2    4.67    6.00        3  34   1       2\n722    4.0    3.00    3.67        4  29   1       2\n723    5.2    2.67    4.00        4  44   0       3\n724    4.0    4.00    2.33        4  32   0       3\n725    2.2    3.00    1.00        6  59   1       3\n726    6.4    3.67    5.67        1  66   1       1\n727    4.2    4.00    5.00        4  66   0       1\n728    2.6    1.00    2.00        5  44   0       3\n729    2.0    4.33    2.00        6  72   1       3\n730    5.0    4.00    4.00        4  68   1       2\n731    6.0    3.33    4.00        4  33   1       1\n732    6.0    4.33    4.67        4  17   1       2\n733    5.2    3.33    5.67        2  32   0       1\n734    6.6    4.00    4.33        3  63   1       2\n735    5.0    1.00    2.00        4  54   0       3\n736    5.8    3.33    5.33        3  59   1       1\n737    5.6    3.33    5.33        6  65   1       2\n738    5.6    2.00    3.67        5  59   1       2\n739    6.0    5.33    5.67        4  42   0       1\n740    4.4    3.33    3.67        3  48   1       1\n741    5.6    2.33    6.00        4  45   1       1\n742    5.2    5.67    4.67        5  43   0       3\n743    5.0    1.33    4.67        4  60   0       1\n744    4.2    3.00    3.00        2  71   0       1\n745    4.4    5.00    5.33        5  41   1       1\n746    5.2    1.00    5.00        2  38   0       1\n747    5.2    3.00    6.00        3  29   1       1\n748    5.2    5.00    3.00        4  36   0       3\n749    4.6    1.00    2.67        1  63   0       1\n750    6.4    3.67    3.00        4  55   1       1\n751    5.4    3.00    3.00        4  41   0       1\n752    2.6    1.00    1.00        6  67   0       3\n753    4.2    4.00    4.00        4  49   0       2\n754    3.6    2.00    5.33        6  42   0       1\n755    4.4    2.33    2.67        4  40   0       2\n756    6.2    2.00    5.33        4  31   0       1\n757    5.8    2.67    5.00        3  44   0       2\n758    5.6    3.67    3.67        3  55   1       2\n759    1.8    1.00    1.00        7  63   1       3\n760    5.8    3.33    6.00        4  34   0       2\n761    4.0    1.67    1.33        5  46   1       3\n762    5.0    4.33    3.67        4  19   1       3\n763    5.4    2.00    4.67        4  57   0       2\n764    5.4    3.67    4.00        2  49   0       1\n765    4.0    3.67    4.33        3  26   0       1\n766    5.4    4.00    5.00        5  62   1       3\n767    4.0    5.33    4.33        1  43   0       2\n768    4.0    2.33    2.67        4  42   0       2\n769    5.0    2.33    5.00        2  21   0       1\n770    2.2    5.00    1.00        4  54   0       2\n771    3.4    3.67    2.00        5  19   0       3\n772    6.0    1.33    2.33        2  35   0       1\n773    5.8    1.00    5.00        4  54   1       1\n774    7.0    1.00    2.00        6  27   0       2\n775    2.4    3.67    1.67        6  38   0       3\n776    4.6    3.67    3.67        4  51   1       3\n777    5.4    2.00    5.33        4  28   0       1\n778    5.2    4.00    3.33        4  41   0       1\n779    4.6    3.33    5.33        2  27   0       2\n780    4.0    4.00    4.00        4  33   1       2\n781    4.6    5.00    5.67        6  33   0       1\n782    3.2    2.67    4.00        5  29   0       3\n783    5.0    5.00    4.00        1  79   0       1\n784    6.8    3.33    4.67        2  67   0       1\n785    4.2    4.00    4.00        4  49   0       2\n786    5.6    5.00    2.33        5  31   1       3\n787    5.0    3.00    2.33        4  39   0       1\n788    5.0    1.33    6.00        4  31   0       2\n789    2.0    1.00    1.00        7  42   0       3\n790    1.8    2.67    1.67        6  40   0       3\n791    5.8    1.67    6.00        3  44   0       1\n792    4.4    3.33    1.00        5  18   1       2\n793    5.8    2.33    5.00        4  40   0       1\n794    5.4    5.00    5.67        4  38   0       2\n795    4.8    2.00    5.33        6  31   0       3\n796    5.8    2.00    4.67        3  36   0       1\n797    4.0    1.00    1.00        6  52   0       3\n798    7.0    1.67    5.33        3  50   0       1\n799    4.2    2.00    4.00        1  36   0       3\n800    5.2    3.33    5.00        3  27   1       2\n801    4.8    1.00    4.00        2  29   0       1\n802    5.2    4.67    4.67        2  42   0       1\n803    5.2    2.67    1.67        4  58   1       3\n804    6.8    3.33    4.67        4  30   1       2\n805    7.0    3.00    6.00        4  48   0       2\n806    5.8    3.67    3.67        2  40   1       1\n807    5.6    3.67    5.33        3  43   0       1\n808    4.6    3.00    2.00        6  52   1       3\n809    5.8    1.67    6.00        5  18   0       3\n810    5.8    1.33    5.33        3  58   1       2\n811    3.2    4.00    5.00        4  52   1       2\n812    3.4    1.00    1.00        7  67   0       3\n813    1.6    3.67    1.67        7  72   1       3\n814    5.4    2.67    3.33        6  36   0       2\n815    5.4    5.33    6.00        4  82   1       1\n\n\nThis way you may take a look at the output (but this is not the right way to do that), but you cannot manipulate it. It is only when you create an object containing the output information of the function (using the arrow sign and a name for the object), that you can continue working on it.\nTo take a look at the data set, you can use the function head() to print the first few rows in the console.\n\nhead(glbwarm)\n\n  govact posemot negemot ideology age sex partyid\n1    3.6    3.67    4.67        6  61   0       2\n2    5.0    2.00    2.33        2  55   0       1\n3    6.6    2.33    3.67        1  85   1       1\n4    1.0    5.00    5.00        1  59   0       1\n5    4.0    2.33    1.67        4  22   1       1\n6    7.0    1.00    6.00        3  34   0       2\n\n\nYou can also view the data set in a separate window by using the function View().\n\nView(glbwarm)\n\nAnother way to explore a data set is through the function str(). This function allows the user to see the type of variables in the data set.\n\nstr(glbwarm)\n\n'data.frame':   815 obs. of  7 variables:\n $ govact  : num  3.6 5 6.6 1 4 7 6.8 5.6 6 2.6 ...\n $ posemot : num  3.67 2 2.33 5 2.33 1 2.33 4 5 5 ...\n $ negemot : num  4.67 2.33 3.67 5 1.67 6 4 5.33 6 2 ...\n $ ideology: int  6 2 1 1 4 3 4 5 4 7 ...\n $ age     : int  61 55 85 59 22 34 47 65 50 60 ...\n $ sex     : int  0 0 1 0 1 0 1 1 1 1 ...\n $ partyid : int  2 1 1 1 1 2 1 1 2 3 ...\n\n\nA similar function, included in the tidyverse package, is glimpse()\n\n# load the package (load it just once in each R session) \nlibrary(tidyverse)\nglimpse(glbwarm)\n\nRows: 815\nColumns: 7\n$ govact   <dbl> 3.6, 5.0, 6.6, 1.0, 4.0, 7.0, 6.8, 5.6, 6.0, 2.6, 1.4, 5.6, 7…\n$ posemot  <dbl> 3.67, 2.00, 2.33, 5.00, 2.33, 1.00, 2.33, 4.00, 5.00, 5.00, 1…\n$ negemot  <dbl> 4.67, 2.33, 3.67, 5.00, 1.67, 6.00, 4.00, 5.33, 6.00, 2.00, 1…\n$ ideology <int> 6, 2, 1, 1, 4, 3, 4, 5, 4, 7, 6, 4, 2, 4, 5, 2, 6, 4, 2, 4, 4…\n$ age      <int> 61, 55, 85, 59, 22, 34, 47, 65, 50, 60, 71, 60, 71, 59, 32, 3…\n$ sex      <int> 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0…\n$ partyid  <int> 2, 1, 1, 1, 1, 2, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 3, 1, 3, 2…\n\n\nIn this case, we can see that the first three variables are numerical (num), and the second three integers (int). Numeric (also double) is a data type that contains real numbers (). Integers is a data type that contains integer numbers.\nThere are also other types of data, such as character (char) for character values, logical for logical values (TRUE and FALSE), and factors to describe items that can have a finite number of values (gender: male, female, other; education, etc.).\nSometimes, variables represets dates and times. Just to make an example of data set inclusing several type of data, we create a data set named data_set_example by running the following code.\n\ndata_set_example <-\n  data.frame(\n    name = as.character(c(\"Karl\", \"Emma\", \"Sebastian\", \"Sarah\")),\n    gender = as.factor(c(\"M\", \"F\", \"Other\", \"M\")),\n    age = as.integer(c(30, 23, 12, 45)),\n    austrian = as.logical(c(TRUE, FALSE, FALSE, TRUE)),\n    salary = factor(\n      c(\"high\", \"medium\", \"low\", \"medium\"),\n      levels = c(\"low\", \"medium\", \"high\")\n    ),\n    partisanship = as.numeric(c(-1.33, 4.56, -3.12, 2.54)),\n    date = as.Date(c(\n      \"2020-01-01\", \"2020-03-01\", \"2020-01-02\", \"2020-01-03\"\n    ))\n  ) \n\n\nglimpse(data_set_example)\n\nRows: 4\nColumns: 7\n$ name         <chr> \"Karl\", \"Emma\", \"Sebastian\", \"Sarah\"\n$ gender       <fct> M, F, Other, M\n$ age          <int> 30, 23, 12, 45\n$ austrian     <lgl> TRUE, FALSE, FALSE, TRUE\n$ salary       <fct> high, medium, low, medium\n$ partisanship <dbl> -1.33, 4.56, -3.12, 2.54\n$ date         <date> 2020-01-01, 2020-03-01, 2020-01-02, 2020-01-03\n\n\nData analysis operations basically consist of manipulations of objects through functions. An object is an entity composed of a name and a value (such as a number) or more complex information (such as a data set). The arrow <- sign is used to create objects, or to assign/update a value, if the object already exists.\nLet’s make an example. Let’s create an object with name object_2 and value equal to 2.\n\nobject_2 <- 2\n\nNow, let’s write the name of the object in the script and run the command (put the cursor on the line of the name, and click run in the top right corner of the editor window, or Control + Enter). The value assigned to the object will be printed on the console.\n\nobject_2\n\n[1] 2\n\n\nObject names can be freely chosen. However, avoid using names that are already used for other objects and functions. For example, avoid giving the name “read.csv” to a data set, as this would overwrite the “read.csv” function. Similarly, if you use “object_2” to save the output of another function, or to represent another value, you will delete the previous value (2):\n\nobject_2 <- 44\nobject_2\n\n[1] 44\n\nobject_2 <- 23 * 34\nobject_2\n\n[1] 782\n\n\nSometimes you may want to delete an object from the environment. You can do that with the function rm() (notice the object disappearing from the environment, in the Environment window).\n\nrm(object_2)\n\nSince the object is equivalent to its value, an object with a numerical value can be used, for example, to perform arithmetical operations.\n\nobject_2 <- 2\nobject_2 * 10\n\n[1] 20\n\n\n\n\n\nThere are different types of object. We take into consideration some of the most important ones: Vectors, Matrices, Data Frames, Lists, Functions.\nVectors are sequences of values:\n\nvector_of_numbers <- c(1,2,3,4,5,6,7,8,9,10) \n\nvector_of_numbers\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nA numeric vector can be used as a term for mathematical operations.\n\nvector_of_numbers * 2\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\nvector_of_numbers + 3\n\n [1]  4  5  6  7  8  9 10 11 12 13\n\n\nVectors may consist of digits but also other symbols. For instance, a vector of letters is a character vector:\n\ncharacter_vector <- c(\"a\", \"b\", \"c\")\ncharacter_vector\n\n[1] \"a\" \"b\" \"c\"\n\n\nAnother type of R object is the matrix (or array). A matrix is an object made up of a series of numeric vectors. In other words, it is a table where rows and columns contain numeric values.\n\na_matrix <- matrix(data = 1:50, nrow = 10, ncol = 5)\na_matrix\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    1   11   21   31   41\n [2,]    2   12   22   32   42\n [3,]    3   13   23   33   43\n [4,]    4   14   24   34   44\n [5,]    5   15   25   35   45\n [6,]    6   16   26   36   46\n [7,]    7   17   27   37   47\n [8,]    8   18   28   38   48\n [9,]    9   19   29   39   49\n[10,]   10   20   30   40   50\n\n\nAs matrixes are mathematical objects, they can contain only numbers.\nA data.frame is a very important object for data analysis and in this course we will work almost exclusively with data.frames.\nData frames are similar to matrices, but they can contain several types of data: numbers, characters (for instance, the texts of an answer), factors (such as gender, or ordered categories, such as “low”, “medium”, “high”), and dates. We have already encountered a couple of examples of data frames:\n\nhead(glbwarm)\n\n  govact posemot negemot ideology age sex partyid\n1    3.6    3.67    4.67        6  61   0       2\n2    5.0    2.00    2.33        2  55   0       1\n3    6.6    2.33    3.67        1  85   1       1\n4    1.0    5.00    5.00        1  59   0       1\n5    4.0    2.33    1.67        4  22   1       1\n6    7.0    1.00    6.00        3  34   0       2\n\nhead(data_set_example)\n\n       name gender age austrian salary partisanship       date\n1      Karl      M  30     TRUE   high        -1.33 2020-01-01\n2      Emma      F  23    FALSE medium         4.56 2020-03-01\n3 Sebastian  Other  12    FALSE    low        -3.12 2020-01-02\n4     Sarah      M  45     TRUE medium         2.54 2020-01-03\n\n\nData frames are important because they represent data sets in R. For instance, if you import a csv or an Excel file in R, the corresponding R object is a data.frame. You can see the type of data by using the function\n\nclass(glbwarm)\n\n[1] \"data.frame\"\n\nclass(data_set_example)\n\n[1] \"data.frame\"\n\nclass(object_2)\n\n[1] \"numeric\"\n\nclass(a_matrix)\n\n[1] \"matrix\" \"array\" \n\n\nThe columns of a data set/data frame represent variables, and rows represent cases. For example, the global warming data set (glbwarm) comprises 815 rows/cases and 7 columns/variables. This data set results from measuring 815 persons on 7 dimensions (e.g., support for government, ideology, age, etc.).\n\nhead(glbwarm)\n\n  govact posemot negemot ideology age sex partyid\n1    3.6    3.67    4.67        6  61   0       2\n2    5.0    2.00    2.33        2  55   0       1\n3    6.6    2.33    3.67        1  85   1       1\n4    1.0    5.00    5.00        1  59   0       1\n5    4.0    2.33    1.67        4  22   1       1\n6    7.0    1.00    6.00        3  34   0       2\n\n\nThe function dim() can be used to find the number of rows and columns of a data set. The first and second figure in the output is the number of rows and column, respectively.\n\ndim(glbwarm)\n\n[1] 815   7\n\n\nThe functions str() or glimpse() permits to see the names of the columns/variables. Another way to see them is using the function names().\n\nnames(glbwarm)\n\n[1] \"govact\"   \"posemot\"  \"negemot\"  \"ideology\" \"age\"      \"sex\"      \"partyid\" \n\n\nWe can access the columns of a data.frame by using the following sintax: name of the data frame, dollar sign, name of the column. For isntance, to access the column “age” of the data.frame “glbwarm”:\n\nglbwarm$age\n\n  [1] 61 55 85 59 22 34 47 65 50 60 71 60 71 59 32 36 69 70 41 48 38 63 71 71 60\n [26] 76 58 71 63 79 68 32 76 50 72 51 60 76 34 51 49 58 45 57 46 69 70 69 58 80\n [51] 64 42 71 40 43 44 40 77 73 63 47 41 64 57 41 43 52 37 68 60 40 72 57 85 52\n [76] 55 42 34 40 55 73 67 65 63 50 52 72 75 54 38 61 47 45 55 72 40 52 42 73 32\n[101] 41 69 44 67 53 67 60 54 61 39 70 63 64 69 84 54 41 68 57 42 41 43 42 33 71\n[126] 67 69 57 70 60 76 33 78 67 53 51 40 44 52 63 76 28 67 71 41 66 39 73 51 79\n[151] 67 53 59 66 75 83 74 66 42 37 55 51 69 63 73 61 65 56 49 77 44 63 54 40 72\n[176] 48 42 67 26 56 54 46 29 68 41 64 50 56 70 33 57 70 83 21 57 59 72 48 29 53\n[201] 29 33 29 46 57 49 27 29 64 61 25 40 62 70 50 30 27 41 60 69 37 64 66 32 67\n[226] 62 72 63 68 41 31 51 30 78 62 77 46 36 34 42 25 26 59 30 60 27 51 48 49 75\n[251] 46 66 64 35 65 77 71 54 29 70 65 27 72 67 67 73 36 58 34 53 25 33 64 80 41\n[276] 27 52 28 67 25 61 68 25 27 23 65 66 50 64 33 59 79 72 60 62 63 30 67 45 41\n[301] 61 62 66 46 73 60 60 65 63 59 82 55 55 67 29 68 57 72 29 26 50 58 64 60 67\n[326] 34 59 23 21 27 65 30 42 53 40 44 27 22 55 72 24 63 63 68 35 47 41 68 68 42\n[351] 73 25 61 55 34 66 65 40 30 42 53 60 20 67 68 26 48 64 62 21 73 57 71 55 26\n[376] 83 47 57 35 71 29 76 55 28 58 47 30 52 31 62 72 50 29 45 65 37 30 44 65 41\n[401] 38 59 19 32 55 53 35 40 65 71 69 51 39 72 26 32 25 32 23 19 22 29 62 39 49\n[426] 37 43 59 32 57 39 33 51 25 26 48 27 54 30 53 36 60 50 27 22 68 52 61 38 21\n[451] 36 22 66 64 68 51 31 18 34 55 43 50 64 33 38 55 30 26 26 61 30 68 87 43 32\n[476] 63 57 55 69 75 54 43 26 62 33 36 52 83 55 18 30 59 51 41 60 39 64 52 55 45\n[501] 45 66 73 40 29 73 54 49 54 48 63 36 25 62 65 65 42 32 58 40 71 60 28 64 19\n[526] 44 18 29 59 80 27 62 19 26 50 59 47 63 67 42 71 49 55 44 20 31 44 56 24 40\n[551] 35 54 38 40 53 53 64 68 34 36 62 60 79 29 32 23 53 63 40 29 61 31 43 59 74\n[576] 24 54 38 27 53 39 23 41 70 74 62 33 39 20 21 23 50 27 54 31 67 62 56 53 30\n[601] 60 66 55 43 45 64 32 54 49 57 44 39 41 55 62 62 66 52 63 24 69 55 35 49 42\n[626] 67 55 35 63 64 61 49 32 68 50 35 73 34 47 75 69 73 33 27 40 30 27 31 23 43\n[651] 21 53 29 67 30 47 31 64 65 65 47 22 64 59 18 19 44 41 28 22 62 29 52 65 54\n[676] 51 19 27 42 70 73 42 42 23 19 22 30 51 34 63 29 46 43 56 48 64 22 44 25 24\n[701] 55 41 65 36 56 36 36 29 44 40 51 30 43 50 40 56 65 62 22 43 34 29 44 32 59\n[726] 66 66 44 72 68 33 17 32 63 54 59 65 59 42 48 45 43 60 71 41 38 29 36 63 55\n[751] 41 67 49 42 40 31 44 55 63 34 46 19 57 49 26 62 43 42 21 54 19 35 54 27 38\n[776] 51 28 41 27 33 33 29 79 67 49 31 39 31 42 40 44 18 40 38 31 36 52 50 36 27\n[801] 29 42 58 30 48 40 43 52 18 58 52 67 72 36 82\n\nhead(glbwarm$age, 10)\n\n [1] 61 55 85 59 22 34 47 65 50 60\n\n\nYou may have noticed that this is just a vector. Indeed, data.frames are object composed by a series of vectors.\nLists are another common type of object. Lists can be used to contain different types of objects. For example, a list may include a matrix, a number, and a data.frame. We will not work with this type of object in this course.\n\nlist(a_matrix, object_2, data_set_example)\n\n[[1]]\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    1   11   21   31   41\n [2,]    2   12   22   32   42\n [3,]    3   13   23   33   43\n [4,]    4   14   24   34   44\n [5,]    5   15   25   35   45\n [6,]    6   16   26   36   46\n [7,]    7   17   27   37   47\n [8,]    8   18   28   38   48\n [9,]    9   19   29   39   49\n[10,]   10   20   30   40   50\n\n[[2]]\n[1] 2\n\n[[3]]\n       name gender age austrian salary partisanship       date\n1      Karl      M  30     TRUE   high        -1.33 2020-01-01\n2      Emma      F  23    FALSE medium         4.56 2020-03-01\n3 Sebastian  Other  12    FALSE    low        -3.12 2020-01-02\n4     Sarah      M  45     TRUE medium         2.54 2020-01-03\n\n\nAn object can also represent a function. We said that functions can be conceived as “mechanisms” that generate output from an input. Functions can transform other objects based on specific rules. For instance, addition is a function that receives two or more numbers as input and generates an output that corresponds to the sum of these numbers.\n`\n\nsum(1, 4)\n\n[1] 5\n\n\nA function has a name (the name of the function, like the name of any other object, e.g., the function sum) and one or more arguments. Arguments are written in parentheses, while the function name is left out of parentheses. Since functions transform objects, among the arguments there is always an object or a value, for instance a numerical value, which is the content the function is applied to. Usually, there are also several other arguments, either mandatory or optional, which modify the behavior of the function.\nIn this course we’ll use statistical functions to perform data analysis like linear regression, moderation, mediation, and conditional process analysis. For example, the name of the linear regression function in R is lm.\n\nlinear_model <- lm(glbwarm$govact ~ glbwarm$ideology)\nsummary(linear_model)\n\n\nCall:\nlm(formula = glbwarm$govact ~ glbwarm$ideology)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7477 -0.7595  0.0287  0.8051  3.3109 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       6.12419    0.12481   49.07   <2e-16 ***\nglbwarm$ideology -0.37645    0.02867  -13.13   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 813 degrees of freedom\nMultiple R-squared:  0.175, Adjusted R-squared:  0.174 \nF-statistic: 172.4 on 1 and 813 DF,  p-value: < 2.2e-16\n\n\n\n\n\nReview the content of this lecture\nFamiliarize with R and RStudio\n\n\n\nDetailed information on the R language (e.g., data structures, types of objects, etc.) can be found in the R Manual). A detailed overview of R programming language is beyond the objectives of this course, and not necessary to perform the analyses we’ll discuss. However, you can use this manual as a reference if you have doubts or would like to learn more.\nSeveral resources about R are freely available online. YouTube features several good introductory videos to R. An important resource for programmers (both professionals and beginners) is https://stackoverflow.com.\n\n\n\n\nOpen the RStudio project you created today\nOpen the script you created today\nWrite and run a few mathematical operations in the script file (e.g., 3+2, 5/3, etc.)\nCreate a new object containing the output of the function sum (e.g., my_sum <- sum(4, 8)) and use this object to perform another mathematical operation of your choice\nSave the updated script and close it\nCreate and save a new script\nCopy all the data sets you find on Moodle in your “data” folder\nLoad one of the data set\nTake a look at the data set using the functions View, str, and glimpse (remember to load the package tidyverse before using the glimpse function)\nAccess one of the column of the data set (name_of_dataframe + dollar sign “$” + name of the column) using the dollar sign"
  },
  {
    "objectID": "moderation.html#visualize-the-simple-mediation-model-with-ggplot2",
    "href": "moderation.html#visualize-the-simple-mediation-model-with-ggplot2",
    "title": "Moderation",
    "section": "Visualize the simple mediation model with ggplot2",
    "text": "Visualize the simple mediation model with ggplot2\n\nDichotomous X, Continuous Y, and Continuous W\nFit the model.\n\nprocess(y = \"justify\", x = \"frame\", w = \"skeptic\",\n        model = 1, \n        jn = 1, \n        plot = 1,\n        decimals = 10.2,\n        data = disaster)\n\n\n********************* PROCESS for R Version 4.0.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : justify\n    X : frame  \n    W : skeptic\n\nSample size: 211\n\n\n*********************************************************************** \nOutcome Variable: justify\n\nModel Summary: \n           R       R-sq        MSE          F        df1        df2          p\n        0.50       0.25       0.66      22.54       3.00     207.00       0.00\n\nModel: \n              coeff         se          t          p       LLCI       ULCI\nconstant       2.45       0.15      16.45       0.00       2.16       2.75\nframe         -0.56       0.22      -2.58       0.01      -0.99      -0.13\nskeptic        0.11       0.04       2.76       0.01       0.03       0.18\nInt_1          0.20       0.06       3.64       0.00       0.09       0.31\n\nProduct terms key:\nInt_1  :  frame  x  skeptic      \n\nTest(s) of highest order unconditional interaction(s):\n       R2-chng          F        df1        df2          p\nX*W       0.05      13.25       1.00     207.00       0.00\n----------\nFocal predictor: frame (X)\n      Moderator: skeptic (W)\n\nConditional effects of the focal predictor at values of the moderator(s):\n     skeptic     effect         se          t          p       LLCI       ULCI\n        1.59      -0.24       0.15      -1.62       0.11      -0.54       0.05\n        2.80       0.00       0.12       0.01       0.99      -0.23       0.23\n        5.20       0.48       0.15       3.21       0.00       0.19       0.78\n\nModerator value(s) defining Johnson-Neyman significance region(s):\n       Value    % below    % above\n        1.17       6.64      93.36\n        3.93      67.77      32.23\n\nConditional effect of focal predictor at values of the moderator:\n     skeptic     effect         se          t          p       LLCI       ULCI\n        1.00      -0.36       0.17      -2.09       0.04      -0.70      -0.02\n        1.17      -0.33       0.17      -1.97       0.05      -0.65       0.00\n        1.42      -0.28       0.16      -1.77       0.08      -0.58       0.03\n        1.84      -0.19       0.14      -1.36       0.17      -0.47       0.09\n        2.26      -0.11       0.13      -0.84       0.40      -0.36       0.15\n        2.68      -0.02       0.12      -0.19       0.85      -0.26       0.21\n        3.11       0.06       0.11       0.55       0.58      -0.16       0.29\n        3.53       0.15       0.11       1.31       0.19      -0.07       0.37\n        3.93       0.23       0.12       1.97       0.05       0.00       0.46\n        3.95       0.23       0.12       1.99       0.05       0.00       0.46\n        4.37       0.32       0.12       2.54       0.01       0.07       0.56\n        4.79       0.40       0.14       2.94       0.00       0.13       0.67\n        5.21       0.49       0.15       3.22       0.00       0.19       0.78\n        5.63       0.57       0.17       3.41       0.00       0.24       0.90\n        6.05       0.66       0.19       3.54       0.00       0.29       1.02\n        6.47       0.74       0.20       3.62       0.00       0.34       1.14\n        6.89       0.82       0.22       3.68       0.00       0.38       1.27\n        7.32       0.91       0.24       3.72       0.00       0.43       1.39\n        7.74       0.99       0.27       3.74       0.00       0.47       1.52\n        8.16       1.08       0.29       3.76       0.00       0.51       1.64\n        8.58       1.16       0.31       3.77       0.00       0.56       1.77\n        9.00       1.25       0.33       3.78       0.00       0.60       1.90\n\nData for visualizing the conditional effect of the focal predictor:\n       frame    skeptic    justify\n        0.00       1.59       2.62\n        1.00       1.59       2.38\n        0.00       2.80       2.75\n        1.00       2.80       2.75\n        0.00       5.20       3.00\n        1.00       5.20       3.48\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\nW values in conditional tables are the 16th, 50th, and 84th percentiles.\n\n\nSelect the table “Data for visualizing the conditional effect of the focal predictor” (as when you want to copy and paste something) and copy it (CTRL+C on Windows, COMMAND+C on Mac). Next, run the following code to create a data frame “dataviz” with the data. When you run the command with a different data set, you need to change the names of the variables.\n\ndataviz <- read_clip_tbl() %>%\n  separate(names(.), \n           c(\"Frame\", \"Skeptic\", \"Justify\"), \n           sep=\"     \") %>% # 5 spaces\n  mutate_all(~ as.numeric(.)) %>%\n  # recode the levels of the dichotomous variable using meaningful labels\n  # (this is useful because these labels will be used in the legend of the plot)\n  mutate(Frame = case_when(Frame == 0 ~ \"Natural Causes\",\n                           Frame == 1 ~ \"Climate Change\")) %>%\n  # Convert the grouping variable to factor\n  mutate(Frame = as.factor(Frame))\n\n\n\n\nCreate the plot with ggplot2.\n\nggplot(dataviz) +\n  geom_line(aes(x = Skeptic, y = Justify, \n                group = Frame, linetype = Frame),\n            size = 1) +\n  xlab(\"Climate Change Skepticism (W)\") +\n  ylab(\"Strength of Justifications for Withholding Aid (Y)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\") # top, bottom, left, right\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nContinuous X, Continuous Y, and Dichotomous W\n\nprocess(y = \"justify\", x = \"skeptic\", w = \"frame\",\n        model = 1, plot = 1,\n        decimals = 10.2,\n        data = disaster)\n\n\n# delete the previous dataset to avoid confusion\nrm(dataviz) \n\n\ndataviz <- read_clip_tbl() %>%\n  separate(names(.), \n           c(\"Skeptic\", \"Frame\", \"Justify\"), \n           sep=\"     \") %>% # 5 spaces\n  mutate_all(~ as.numeric(.)) %>%\n  # recode the levels of the dichotomous variable using meaningful lables\n  mutate(Frame = case_when(Frame == 0 ~ \"Natural Causes\",\n                           Frame == 1 ~ \"Climate Change\")) %>%\n  mutate(Frame = as.factor(Frame))\n\n\n\n\nThe plot is the same of before, just change the labels where necessary.\n\nggplot(dataviz) +\n  geom_line(aes(x = Skeptic, y = Justify, \n                group = Frame, linetype = Frame),\n            size = 1) +\n  xlab(\"Climate Change Skepticism (X)\") +\n  ylab(\"Strength of Justifications for Withholding Aid (Y)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\") # top, bottom, left, right\n\n\n\n\n\n\nContinuous X, Continuous Y, and Continuous W\n\nprocess(y = \"govact\", x = \"negemot\", w = \"age\",\n        cov = c(\"posemot\", \"ideology\", \"sex\"),\n        model = 1, \n        jn = 1, \n        plot = 1,\n        decimals = 10.3,\n        data = glbwarm)\n\n\n# delete the previous dataset to avoid confusion\nrm(dataviz) \n\n\ndataviz <- read_clip_tbl() %>%\n  separate(names(.), \n           c(\"Negemot\", \"Age\", \"Govact\"), \n           sep=\"     \") %>% # 5 spaces\n  mutate_all(~ as.numeric(.)) %>%\n  mutate(Age = as.factor(Age))\n\n\n\n\n\nggplot(dataviz) +\n  geom_line(aes(x = Negemot, y = Govact, \n                group = Age, linetype = Age,\n                color = Age), # to use a different color by group\n            size = 1) +\n  # just to choose a specific color:\n  # scale_color_manual(values = c(\"steelblue\", \"grey\", \"skyblue\")) +\n  xlab(\"Negative Emotions (W)\") +\n  ylab(\"Support for Government Action (Y)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\") # top, bottom, left, right"
  },
  {
    "objectID": "moderation.html#visualize-additive-multiple-moderation-models",
    "href": "moderation.html#visualize-additive-multiple-moderation-models",
    "title": "Moderation",
    "section": "Visualize Additive Multiple Moderation Models",
    "text": "Visualize Additive Multiple Moderation Models\n\nContinuous X, Continuous Y, Dichotomous W, and Continuous Z\n\nprocess(y = \"govact\", x = \"negemot\", w = \"sex\", z = \"age\",\n        cov = c(\"posemot\", \"ideology\"),\n        model = 2, \n        plot = 1,\n        zmodval = c(30,50,70),\n        decimals = 10.3,\n        data = glbwarm)\n\n\n# delete the previous dataset to avoid confusion\nrm(dataviz) \n\n\ndataviz <- read_clip_tbl() %>%\n  separate(names(.), \n           c(\"Negemot\", \"sex\", \"Age\", \"Govact\"), \n           sep=\"     \") %>% # 5 spaces\n  mutate_all(~ as.numeric(.)) %>%\n  mutate(sex = case_when(sex == 0 ~ \"Female\",\n                           sex == 1 ~ \"Male\")) %>%\n  mutate(Age = as.factor(Age),\n         sex = as.factor(sex))\n\n\n\n\n\nggplot(dataviz) +\n  geom_line(aes(x = Negemot, y = Govact, \n                group = sex, linetype = sex),\n            size = 1) +\n  # facet_wrap slit the plot in several ones based on a grouping variable (i.e., Age)\n  facet_wrap(~Age, nrow = 3,\n            # Change the labels to make them clearer\n            labeller = as_labeller(c(\"30\" = \"Age 30\", \n                                     \"50\" = \"Age 50\", \n                                     \"70\" = \"Age 70\"))) + \n  xlab(\"Negative Emotions (W)\") +\n  ylab(\"Support for Government Action (Y)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\") # top, bottom, left, right"
  },
  {
    "objectID": "moderation.html#visualize-moderated-moderation-models",
    "href": "moderation.html#visualize-moderated-moderation-models",
    "title": "Moderation",
    "section": "Visualize Moderated Moderation Models",
    "text": "Visualize Moderated Moderation Models\n\nContinuous X, Continuous Y, Dichotomous W, and Continuous Z\n\nprocess(y = \"govact\", x = \"negemot\", w = \"sex\", z = \"age\",\n        cov = c(\"posemot\", \"ideology\"),\n        model = 3, \n        jn = 1, \n        plot = 1,\n        zmodval = c(30,50,70),\n        decimals = 10.3,\n        data = glbwarm)\n\n\n# delete the previous dataset to avoid confusion\nrm(dataviz) \n\n\ndataviz <- read_clip_tbl() %>%\n  separate(names(.), \n           c(\"Negemot\", \"sex\", \"Age\", \"Govact\"), \n           sep=\"     \") %>% # 5 spaces\n  mutate_all(~ as.numeric(.)) %>%\n  mutate(sex = case_when(sex == 0 ~ \"Female\",\n                           sex == 1 ~ \"Male\")) %>%\n  mutate(Age = as.factor(Age),\n         sex = as.factor(sex))\n\n\n\n\n\nggplot(dataviz) +\n  geom_line(aes(x = Negemot, y = Govact, \n                group = sex, linetype = sex),\n            size = 1) +\n  facet_wrap(~Age, nrow=3,\n            labeller = as_labeller(c(\"30\" = \"Age 30\", \n                                     \"50\" = \"Age 50\", \n                                     \"70\" = \"Age 70\"))) + \n  xlab(\"Negative Emotions (W)\") +\n  ylab(\"Support for Government Action (Y)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\") # top, bottom, left, right"
  }
]